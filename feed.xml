<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://fanpu.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://fanpu.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-06-13T10:06:18+00:00</updated><id>https://fanpu.io/feed.xml</id><title type="html">blank</title><subtitle>Homepage
</subtitle><entry><title type="html">CMU 15-712 Advanced Operating Systems and Distributed Systems Course Review</title><link href="https://fanpu.io/blog/2023/advanced-operating-systems-course-review/" rel="alternate" type="text/html" title="CMU 15-712 Advanced Operating Systems and Distributed Systems Course Review" /><published>2023-06-09T00:00:00+00:00</published><updated>2023-06-09T00:00:00+00:00</updated><id>https://fanpu.io/blog/2023/advanced-operating-systems-course-review</id><content type="html" xml:base="https://fanpu.io/blog/2023/advanced-operating-systems-course-review/"><![CDATA[<p>This semester (Spring 2023), I took <a href="https://www.cs.cmu.edu/~15712/">15-712 Advanced Operating Systems and
Distributed Systems</a> under professor <a href="http://www.cs.cmu.edu/~gibbons/">Phil
Gibbons</a> and his TA and also PhD advisee <a href="http://nicebowlofsoup.com/">Val
Choung</a>. This was the first time I took
a class under Phil (he usually teaches either this class or <a href="http://www.cs.cmu.edu/afs/cs/academic/class/15745-s19/www/">15-745 Optimizing
Compilers for Modern
Architectures</a>),
and interestingly enough this was the third time Val was my TA, the 
previous two times being 15-330 Introduction to Computer Security
in Fall 2019 and <a href="https://www.cs.cmu.edu/~410/">15-410 Operating System Design and Implementation</a>
in Spring 2020.</p>

<h1 id="overall-impression">Overall Impression</h1>
<p>This class exceeded my expectations significantly.
I found it especially meaningful and apt since this was my last systems class before I
graduate, and the topics and discussions from class helped to unify all the
systems concepts that I had learnt from previous classes into a nice package
informed by common underlying principles: from distributed systems, to
networking, databases, filesystems, operating systems, and even machine learning
systems.</p>

<h1 id="why-i-took-the-class">Why I Took The Class</h1>
<p>I had to take a systems class this semester to fulfill my graduation
requirements for the MSCS program. I initially did include this class in my
shortlist of systems classes to take, but then thought it was just going to be a
paper reading class (not that I had been in one of such classes before, but it
just did not sound very interesting and felt like something I could do by myself
asynchronously after I graduate) and therefore was quite hesitant to take it.</p>

<p>As such, during registration week I settled on <a href="https://www.cs.cmu.edu/~418/">15-618 Parallel Computer Architecture and
Programming</a>,
since it included topics on GPU programming that aligned with my current
interests in machine learning. However, I did not feel like the class was
sufficiently challenging for me after the first lecture, as it was a bit too
slow-paced and simple for my liking as I already had exposure to most of the
topics from other system classes that I had taken. I decided to switch
to 15-712, and I knew immediately that it was the right class for me after the
first lecture.</p>

<p>In a sense, this class was a hidden gem and I was really glad that I ended up
taking it.</p>

<h1 id="the-first-lecture">The First Lecture</h1>
<p>The first lecture went through 2 Wisdom Papers, which no one was expected
to have read yet as it was the first class. You can refer
to the <a href="https://www.cs.cmu.edu/~15712/lectures/01-intro.pdf">slides here</a> if you are curious.</p>

<p>The first paper, <a href="https://www.cs.cmu.edu/~15712/papers/mythicalmanmonth00fred.pdf">Mythical Man-Month: Essays on Software
Engineering</a>
is a book by Turing-award winner Fred Brooks. It is about many of his observations
and principles on software engineering based on his own vast experiences. What
really brought it home to me was that a couple of them were also things that I
had some suspicions about previously, but never really thought it was universally
applicable, and thought they were simply artifacts of the way I approached things.</p>

<p>For instance, one of the principles is “Plan to Throw One Away”, meaning that
one should first build a worthwhile system in a short amount of time, and then
re-build a better second version with the benefit of hindsight. This is because
one would end up having to re-build the system anyway after being confronted
with change and feedback, and also due to the following observation on program
maintenance:</p>

<blockquote>
  <p>“Program maintenance is an entropy-increasing
process, and even its most skillful execution only
delays the subsidence of the system into unfixable
obsolescence”</p>
</blockquote>

<p>This had many parallels with my own experiences. For instance, my group ended up
having 4 major re-writes of our kernel during 15-410, and I also did a complete
re-write of my CloudFS filesystem for my 18-746 project. Similarly, many of my
internship projects were also re-writes and improvements on design of existing
systems that had accumulated too much technical debt. It does seem
a lot more reasonable to plan for this eventual change to begin with.</p>

<p>The paper also contained a lot of other great advice, such as the
importance of conceptual integrity to separate architecture
from implementation, structuring a team in a “surgical” fashion to drive software
development where the best programmer leads the most critical development
work like a surgeon and directs others on the other aspects, and of
course the famous Brook’s law:</p>

<blockquote>
  <p>“Adding manpower to a late
software project makes it later”</p>
</blockquote>

<p>The second paper, <a href="https://www.cs.cmu.edu/~15712/papers/hamming86.pdf">You and Your
Research</a> by Richard Hamming
(of Hamming code fame), talks about how to become a great scientist. 
The following two slides gives a good sense of the spirit of the paper:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/adv_os/slide_1.avif" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      How to be a Great Scientist (1)
    </figcaption>
</figure>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/adv_os/slide_2.avif" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      How to be a Great Scientist (2)
    </figcaption>
</figure>

<p>I mention the first lecture and the two papers that were discussed here not
simply because they were interesting, but because they helped to set the tone
and expectations for the rest of the semester going forward. The message is clear: this is going to be a practical and useful class that will help you on your journey to becoming great systems designers and researchers.</p>

<h1 id="course-structure">Course Structure</h1>
<p>There are three main components to the class: paper summaries,
projects, and exams.</p>

<h2 id="1-paper-summaries">1. Paper Summaries</h2>
<p>Before each lecture, the class is assigned a required reading and an optional reading. A paper summary of the required reading must be submitted before
the class, which will discuss both readings.</p>

<p>The paper summary will contain 3 things:</p>
<ol>
  <li>The 3 most important things in the paper,</li>
  <li>1 most glaring deficiency of the paper (even highly celebrated papers have faults!),</li>
  <li>A conclusion on how you will use lessons from this paper to inform you on how
you will build systems in the future.</li>
</ol>

<p>It took me on average 2-4 hours to read each paper and around 15 minutes for the summary.</p>

<p>The lectures for this class are front-loaded, meaning that during the first
two-thirds of the semester, you will meet 3 times a week for 80 minutes each,
while there will be no lectures at all during the final third of the semester,
and so “on average” throughout the semester you will meet twice a week. This is so that
students have enough knowledge and content to begin working on their course
projects early on in the semester.</p>

<p>There will be 3 short breaks in each lecture, where all students will get into
breakout groups and share and discuss among themselves one of the prompts
for the paper based on their paper summaries. Afterwards, all groups
are invited to share what they thought.</p>

<p>Reading and writing the paper summaries are the only “homework” you will get in
this class.</p>

<h2 id="2-course-project">2. Course Project</h2>
<p>There is also a semester-long course project with a significant systems
component in groups of three. This will begin in earnest after a third of the
semester, and all the project groups met with Phil and the TA Val once every two
weeks.  The deliverables include a project proposal, an interim report, a final
presentation, and a final report. The course project will be the
largest constituent of your final grade.</p>

<h2 id="3-midterms">3. Midterms</h2>
<p>Finally, there are two midterm exams, which are taken during class time.
The first is taken in the middle of the semester, and the second is taken
after all lectures have concluded.</p>

<p>Each midterm will cover content from a shortlisted selection of 10
of the required readings. There will be 9 questions on the midterm,
which covers 9 of the 10 papers, and you are only required to answer 7 of
the problems.</p>

<p>The course staff will also provide two past year exams to practice on, though
some of the readings may have changed since.</p>

<p>It admittedly does seem quite daunting to have to study and be familiar with 10
papers spanning very different topics. I did not have time to actually re-read
all 10 papers to prepare for the midterm, and so the way I prepared was to go
through all the lecture slides again, re-read the most important sections of the
paper, and skim through the rest. Afterward, I attempted the past exams to fill
in any gaps that I may have missed. This strategy allowed me to do fairly well
on the exam.</p>

<h1 id="course-content">Course Content</h1>

<p>The class us on a whirlwind tour through many <a href="https://www.sigops.org/awards/hof/">SIGOPS
Hall of Fame papers</a>, which the award
description states was “instituted in 2005 to recognize the most influential
Operating Systems papers that were published at least ten years in the past”.
Reading through the papers helped to consolidate a lot of the knowledge that I
learned in previous systems classes, and it was cool to see how decades ago many
of these ideas that were once unappreciated or heavily criticized now form the
bedrock of many of the systems that we use today.</p>

<p>In addition to the Hall of Fame papers, there were also several relatively
recent papers that the course staff thought were conceptually interesting
and promising.</p>

<p>The following sections will go through each of the modules and
the required papers that you will read (refer to the <a href="https://www.cs.cmu.edu/~15712/syllabus.html">course website</a>
if you are also interested in the optional papers), and a short description
of what the paper is about so you can get a pretty good sense of what is
covered. A cool thing to note is that the scope of all the papers will
touch almost all the systems classes offered at CMU.</p>

<h2 id="part-1-concurrency-ordering-races">Part 1: Concurrency, Ordering, Races</h2>
<ul>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/birrell84.pdf">Implementing Remote Procedure Calls (Birrell’84), SigOps HoF paper</a> - introduced the now-standard design of RPC calls with interfaces and caller/callee stubs for distributed communication.</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/lamport78.pdf">Time, Clocks, and the Ordering of Events in a Distributed System (Lamport’78),	SigOps HoF paper</a> - introduced Lamport clocks, the foundation for ordering distributed systems today</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/chandy85.pdf">Distributed Snapshots: Determining Global States of Distributed Systems (Chandy’85), SigOps HoF paper</a> - how to snapshot a consistent global state in a distributed system</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/li19.pdf">Efficient Scalable Thread-Safety-Violation Detection (Li’19), SOSP’19 best paper</a> - using active testing to discover and reproduce concurrency bugs</li>
</ul>

<h2 id="part-2-file-systems-and-disks">Part 2: File Systems and Disks</h2>
<ul>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/mckusick84.pdf">A Fast File System for UNIX (McKusick’84), SigOps HoF paper</a> - addresses many of the problems
of the original Unix filesystem by introducing the Fast File System (FFS),
and many of its ideas are now staple in modern filesystems like the Linux <code class="language-plaintext highlighter-rouge">ext*</code> filesystems</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/howard88.pdf">Scale and Performance in a Distributed File System (Howard’88), SigOps HoF paper</a> - introduces the Andrew File System (AFS) that significantly improved on NFS in terms of scalability. AFS was developed at CMU and is still widely used today.</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/rosenblum92.pdf">The Design and Implementation of a Log-Structured File System (Rosenblum’92), SigOps HoF paper</a> - 
introduced log-structured filesystems that support high write throughput which addresses the problem of disk traffic being dominated by slow writes
on traditional filesystems</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/patterson88.pdf">A Case for Redundant Arrays of Inexpensive Disks (RAID) (Patterson’88), SigOps HoF paper</a> - introduced RAID, solved problem of how to get cheap fault tolerance</li>
</ul>

<h2 id="part-3-transactions-and-databases">Part 3: Transactions and Databases</h2>
<ul>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/kung81.pdf">On Optimistic Methods for Concurrency Control (Kung’81), SigOps HoF paper</a> - introduced optimistic concurrency control, now standard in many database environments
with low contention and high throughput requirements</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/franklin97.pdf">Concurrency Control and Recovery (Franklin’97)</a> - survey paper on concurrency
control and recovering from crashes in database systems (write-ahead logging, ARIES)</li>
</ul>

<h2 id="part-4-fault-tolerance">Part 4: Fault Tolerance</h2>
<ul>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/paxos-simple.pdf">Paxos (Lamport’01), SigOps HoF paper</a> - simplified version of his original theatrical <a href="https://www.cs.cmu.edu/~15712/papers/part-time-parliament.pdf">The Part-Time Parliament</a> paper that was mostly ignored, introduced how to get replicated logs
among unreliable (but not Byzantine) nodes</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/castro99.pdf">Practical Byzantine Fault Tolerance (Castro’99)</a> - distributed consensus but in the
presence of Byzantine faults (i.e a fraction of the nodes can collude and behave maliciously)</li>
</ul>

<h2 id="part-5-os-kernels-and-virtual-machines">Part 5: OS Kernels and Virtual Machines</h2>
<ul>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/liedtke95.pdf">Microkernels (Liedtke’95), SigOps HoF paper</a> - the first demonstration of an
efficient microkernel designed with a very extreme minimality principle,
where as much OS functionality is moved outside of the microkernel
as possible, including even its memory manager, pagers, device drivers, software TLBs, etc.</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/klein09.pdf">seL4: Formal Verification of an OS Kernel (Klein’09), SigOps HoF paper</a> - first paper to perform a formal, machine-checked verification of a microkernel using the Isabelle/HOL theorem prover</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/waldspurger02.pdf">Memory Resource Management in VMware ESX Server (Waldspurger’02), SigOps HoF paper</a> - introduced many great ideas for hypervisor memory management, like memory ballooning, idle tax, and transparent page sharing that are now commonplace in modern virtual machines</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/clements15.pdf">The Scalable Commutativity Rule: Designing Scalable Software for Multicore Processors (Clements’15), SOSP’13 best paper</a> - addresses
the problem of deciding whether there exists a scalable implementation (with respect to number of processors) of a program based on a restricted form of commutativity of the interfaces that it uses
called SIM commutativity (<strong>S</strong>tate-dependent, <strong>I</strong>nterface-based, <strong>M</strong>onotonic)</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/baumann09.pdf">The Multikernel: A new OS architecture for scalable multicore systems (Baumann’09), SigOps HoF paper</a> - solves the problem 
of modern operating systems not being able to take advantage of the current trend of increasing core counts due to the inherent limitations of the shared-memory kernel design,
by instead re-framing the OS as a distributed system split among different cores with event-driven execution, replicated state, and a hardware-neutral structure</li>
</ul>

<h2 id="part-6-big-data-systems">Part 6: Big Data Systems</h2>
<ul>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/decandia07.pdf">Dynamo: Amazon’s Highly Available Key-value Store (DeCandia’07), SigOps HoF paper</a> - the secret behind how
Amazon can support very high availability with eventual consistency due to
extreme business requirements (i.e users should never fail to add an item to
their shopping carts), by incorporating established techniques like consistent
hashing, sloppy quorums, gossip-based membership protocols, etc</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/corbett12.pdf">Spanner: Google’s Globally-Distributed Database (Corbett’12), SigOps HoF paper</a> - built using lessons
from <a href="https://cloud.google.com/bigtable">Google Bigtable</a>, Spanner powers Google as their globally replicated transactional database system 
with 5 nines of availability with several novel ideas like TrueTime to quantify uncertainty in wall clock time and distributed transactions</li>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/qiao21.pdf">Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning (Qiao’21), OSDI’21 best paper</a> -
introduces a new notion of goodput that combines throughput and statistical efficiency to evaluate the performance of schedulers for training deep learning systems, 
with a practical implementation that optimizes both cluster-wide and per-job parameters called Pollux</li>
</ul>

<h2 id="part-7-emerging-platforms">Part 7: Emerging Platforms</h2>
<ul>
  <li><a href="https://www.cs.cmu.edu/~15712/papers/kim21.pdf">LineFS: Efficient SmartNIC Offload of a Distributed File System with Pipeline Parallelism (Kim’21), SOSP’21 best paper</a> - optimizing the performance of distributed file systems (DFS) by decomposing DFS operations into pipelined stages and offloading networked stages to a SmartNIC asynchronously</li>
</ul>

<h1 id="workload">Workload</h1>
<p>The class has a moderate workload for a systems class. Expect to spend 10-12 hours 
a week on the readings and paper summaries while lectures are ongoing, probably
a couple more hours once the projects get into motion midway through the
semester, and for it to consume a significant portion of your existence in the
last two weeks before the final presentations.</p>

<p>It is a far less demanding and stressful class than the legendary <a href="https://www.cs.cmu.edu/~410/">15-410/605
Operating System Design and Implementation</a> class, 
so don’t let the “advanced” in the course title scare you off from taking this class.
After all, most people taking this class are Ph.D. students who have their
own research to work on and can’t exactly spend all their time on courses,
unlike undergraduates.</p>

<h1 id="takeaways-from-the-class">Takeaways From The Class</h1>
<p>Here are my thoughts on the key takeaways from the class.</p>

<h2 id="class-discussion">Class Discussion</h2>
<p>As a seminar-based class, one of the most surprising things for me was how fun
and valuable the class discussions were.  It was especially enlightening to
hear the comments of Ph.D. students who are working in systems and other fields
in computer science, who often had very different critiques and opinions of
the papers than what I had come up with, which often led me to wonder how they
got their perspectives and what their background is like. 
This was particularly true when someone mentioned glaring deficiencies and
problems with the paper that I had completely not even thought of.</p>

<p>However, one thing that made me sad was that attendance in class
started to fall after the halfway point of the semester. This
included quite a few of the students who used to give very insightful
and interesting responses and so the diversity of perspectives of the
discussions as a whole suffered.</p>

<p>While attendance is not strictly enforced, actively participating in the
discussions and being engaged in lectures is one of the most valuable
takeaways from this class, and positively impacts not just you but also
your classmates, and so I would strongly encourage anyone interested in the
class to attend all the lectures that you can.</p>

<h2 id="tribal-knowledge">Tribal Knowledge</h2>
<p>Another aspect of the class that I really appreciated was how Phil taught us a
lot of the spirit and tribal knowledge of doing CS research during his lively
lectures. These were often presented as off-hand remarks while presenting the
context or background of a paper, and provided insight into the zeitgeist of the
time, the motivations and challenges that the paper authors faced, and what the
authors went on to do in the future based on the impact (or lack of impact at
that time) of their work.</p>

<p>As someone who has not done a long-term research project with a faculty member
but am thinking about possibly doing a Ph.D. in the future, all of these were
very valuable wisdom which are not things that you can pick up easily yourself
from reading past papers or books. In fact, it almost felt as if I had my own
advisor at times.</p>

<h2 id="witness-the-evolution-of-systems-research">Witness the Evolution of Systems Research</h2>
<p>As you read through the papers, you almost feel as if you are being put into the
driver’s seat and can see how systems research has matured and evolved over the
past few decades. Seminal papers of the past tackled the most
general problems, although many of them lacked implementations or proper benchmarks
that would surely be grounds to be red-flagged and rejected from any systems
conference today. Many of the more recent papers strive to anticipate and build
for future changes in the computation landscape, have solid replicable
implementations and evaluations, and are a lot more careful about anticipating
and providing rebuttals for criticisms.</p>

<h2 id="personal-attention-for-projects">Personal Attention for Projects</h2>
<p>I also really appreciated the personal attention that Phil and Val gave to us
by meeting with us every other week for our course projects. 
This is especially so if you consider that many advisors already have trouble
meeting their own Ph.D. students for an hour a week, whereas in this case the
course staff dedicated half an hour every two weeks for every single group in
the class (there were around 10), which I thought was some real dedication.
I will admit that I did not live up to my end of the bargain by spending
as much time on the project as I would have wanted to (compared to when I took 15-410).
One could always give excuses for anything so you don’t have to listen to mine,
but if I had to reflect on it, it was due to a combination of high
workloads from other classes, the fact that this was not the highest priority for
the members in our group (my project partners were both quite busy with
their own research and I was busy with other classes), and some unexpected
obstacles in our project that forced us back to the drawing boards a few times
(our project interim report was drastically different from our initial
proposal).</p>

<h2 id="fantastic-course-staff">Fantastic Course Staff</h2>
<p>Phil is a really good lecturer. He is very clear, the class pacing is great, and
the lecture slides are polished. He is very approachable and respectful
towards students, and puts in great effort to give a good and satisfying answer
to every question.</p>

<p>Feedback for projects is prompt (there was no feedback for the paper summaries),
and the midterms were graded fairly quickly.</p>

<p>Overall it is clear that the class is pedagogically mature and has benefited
from many rounds of feedback during past iterations.  It is rich in content,
is accessible and yet challenging to students from a wide range of backgrounds,
and will prepare one well for building systems in the future, be it in 
academia or industry.</p>

<h1 id="our-course-project-and-reflections">Our Course Project, and Reflections</h1>
<p>Our course project was on the automated optimal scheduling of data in dynamic neural
networks over heterogeneous GPUs for inference tasks in a pipeline-parallelism
fashion. This means that when a model is too large to fit on a single GPU
but instead has to be distributed across multiple GPUs, we aim to solve the problem
of finding the optimal way to perform this split in the presence of dynamism
in the network. In our case we focused on input dynamism, meaning that the
sizes of the inputs can vary, which can result in different execution times in different
segments of the network. We built a system called <code class="language-plaintext highlighter-rouge">DynPartition</code>, 
a reinforcement-learning based scheduler that uses Deep-Q Learning to learn
the optimal way of performing this split.</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/adv_os/presentation.avif" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Giving our final project presentation in the Panther Hollow conference room at CIC
    </figcaption>
</figure>

<p>We had some positive empirical results on our benchmarks, but will require
additional future work to verify the generality of these results. Overall, I
thought it was a great experience working with PhD students and to learn from
their working styles and approach to solving problems. It was also really cool
to see the breadth and depth of projects presented by the other teams during the
final presentation, which was structured like a conference.</p>

<h1 id="is-this-class-suitable-for-me">Is This Class Suitable For Me?</h1>
<p>I cannot recommend this course enough to anyone who has sufficient
background and have an interest in building systems, or systems research.</p>

<p>You should be sufficiently prepared for the class if you have taken
<a href="https://www.cs.cmu.edu/~410/">15-410 Operating System Design and Implementation</a>,
or any other equivalent rigorous operating systems design class in your undergraduate college.
Most papers draw heavily on low-level concepts from operating systems and assume
that the reader is familiar with them, and therefore familiarity with these ideas
is critical to understanding the papers.</p>

<p>I don’t feel any of the other classes are as critical, as any new concepts can
be picked up relatively easily. For instance, a good grasp of considerations
involved in operating systems design means that it’s not too hard to also
understand the challenges involved in filesystems or virtual machine design.
Having taken other classes would definitely still help to make the papers more
approachable though. For instance, the Pollux paper was not very approachable for
people who did not have prior exposure to machine learning systems, which led to the
course staff deciding not to include that as one of the papers tested for the
second midterm.</p>

<p>When I took the class, all the students were either Masters or Ph.D. students.
Strong undergraduates with sufficient background would also definitely do well
in the class.</p>

<h1 id="acknowledgments">Acknowledgments</h1>
<p>I would like to express my gratitude to <a href="https://adbforlife.github.io/">Albert
Gao</a> and <a href="https://abigalekim.github.io/">Abigale Kim</a> for helping to proofread this article. Both of them
took the class with me this semester.</p>]]></content><author><name>fanpu</name></author><category term="courses" /><category term="cmu" /><category term="systems" /><summary type="html"><![CDATA[15-712 Advanced Operating Systems and Distributed Systems was an excellent seminar-based graduate course that took us on a whirlwind tour through many of the most seminal SigOps Hall of Fame papers across several systems domains. It will prepare you to be a great systems designer and researcher. In this post, I will share my experience in the class, the course structure and content, what I thought were the key takeaways, and who this class would be suitable for.]]></summary></entry><entry><title type="html">Score-Based Diffusion Models</title><link href="https://fanpu.io/blog/2023/score-based-diffusion-models/" rel="alternate" type="text/html" title="Score-Based Diffusion Models" /><published>2023-06-07T00:00:00+00:00</published><updated>2023-06-07T00:00:00+00:00</updated><id>https://fanpu.io/blog/2023/score-based-diffusion-models</id><content type="html" xml:base="https://fanpu.io/blog/2023/score-based-diffusion-models/"><![CDATA[<p>\(\newcommand{\pdata}{p_{\text{data}}(\bx)}
    \newcommand{\st}{\mathbf{s}_\mathbf{\theta}}
    \newcommand{\xt}{\tilde{\bx}}
    \newcommand{\stx}{\mathbf{s}_\mathbf{\theta}(\bx)}
    \newcommand{\sdx}{\mathbf{s}_\text{data}(\bx)}
    \newcommand{\stxt}{\mathbf{s}_\mathbf{\theta}(\xt, \sigma)}
    \newcommand{\pv}{p_{\bv}}
    \newcommand{\score}{\nabla_\bx \log \pdata}
    \newcommand{\bov}{\bar{\beta}}\)
<em>Joint work with <a href="https://www.linkedin.com/in/owen-wang/">Owen Wang</a>.</em></p>

<p>There has recently been a flurry of work in score-based
diffusion models as part of the broader area of generative models.
This is due to the recent success of such score-based methods,
which has achieved results comparable to the state-of-the-art
of generative adversarial networks (GANs).</p>

<p>Past techniques in generative modeling have either relied on the
approximation of the partition function of the probability density, or the
combination of an implicit network representation of the probability density
and adversarial training.
The former suffers from having to either constrain the model
to make the partition function tractable, or otherwise relies on
approximations with surrogate losses that may be inaccurate,
and the latter suffers from training instability and mode collapse.</p>

<p>Score-based diffusion models try to address the cons of both approaches, and
instead, use score-matching to learn a model of the gradient of the log of the
probability density function.  This allows it to avoid computing the partition
function completely.</p>

<p>One of the first such approaches that rely on using score-matching to
perform generative modeling does so by generating new samples via Langevin
dynamics <a href="https://arxiv.org/abs/1907.05600">(Song &amp; Ermon, 2019)</a>.
A key observation
is that naively applying score-matching is that the model of score function will
be inaccurate in areas of low density with respect to the data distribution,
which results in improper Langevin dynamics in low-density
areas. The solution that was proposed is the injection of noise into the data,
which provides
additional training signal and increases the dimensionality
of the data.</p>

<p>The next major step introduced in 
<a href="https://arxiv.org/abs/2011.13456">(Song et al., 2021)</a>
is to perturb the data using a diffusion process
which is a form of a stochastic differential equation (SDEs).
The SDE is then reversed using annealed Langevin dynamics
in order to recover the generative process, where the reversal
process makes use of score matching.</p>

<p>Other recent refinements that have been proposed include
re-casting the objective as a Schrödinger bridge problem,
which is an entropy-regularized optimal transport problem.
The advantage of this approach is that it allows for fewer
diffusion steps to be taken during the generative process.</p>

<h1 id="survey-of-results">Survey of Results</h1>

<p>We will be primarily focusing on the paper
<a href="https://arxiv.org/abs/1907.05600">Generative Modeling by Estimating Gradients of the Data Distribution (Song &amp; Ermon, 2019)</a>.</p>

<p>In this section, we provide the necessary background, provide
derivations for important results,
and explain the key ideas of score matching for diffusion
models as proposed in the papers.</p>

<h2 id="motivation-for-score-matching">Motivation for Score Matching</h2>

<h3 id="limitations-of-likelihood-based-approaches">Limitations of Likelihood-Based Approaches</h3>
<p>Score matching is motivated by the limitations of likelihood-based
methods. In likelihood-based methods, we use a parameterized model
\(f_\theta(\bx) \in \mathbb{R}\) and attempt it to recover the parameters \(\theta\) that best
explains the observed data. For instance, in energy-based models,
the probability mass function \(p_\theta(\bx)\) would be given as
\begin{align}
    p_\theta(\bx) = \frac{\exp(-f_\theta(\bx))}{Z_\theta},
\end{align}
where \(Z_\theta\) is the normalizing constant that causes the
distribution to integrate to 1, i.e
\begin{align}
    Z_\theta = \int \exp(-f_\theta(\bx)) \, d \bx.
\end{align}
The goal then is to maximize the log likelihood of the observed data \(\{\bx_i\}_i^N\),
given by
\begin{align}
    \max_\theta \sum_{i=1}^N \log p_\theta (\bx_i).
\end{align}</p>

<p>It is often computationally intractable to compute the partition
function \(Z_\theta\) unless there
are restrictions on what the model can be, since there are usually
at least an exponential number of possible configurations.
Examples of models where the partition function can be
efficiently computed include
causal convolutions in autoregressive models, and
invertible networks in normalizing flow models
However, such architecture restrictions are very
undesirable as they limit the expressiveness of the models.</p>

<p>A likelihood-based approach that tries to avoid computing
the partition function is variational inference.
In variational inference, we use the Evidence Lower
Bound (ELBO) as a surrogate objective,
where the approximation error is the smallest
Kullback-Leibler divergence between the true distribution
and a distribution that can be parameterized by our model.</p>

<h3 id="limitations-of-adversarial-based-approaches">Limitations of Adversarial-Based Approaches</h3>
<p>Adversarial-based approaches, like generative adversarial networks (GANs), have
been shown to suffer from both instability in training and mode collapse.</p>

<p>Training GANs can be viewed as finding a Nash equilibrium
for a two-player non-cooperative game between
the discriminator and the generator. Finding a Nash
equilibrium is PPAD-complete which is computationally
intractable, and therefore methods like gradient-based
optimization techniques are used instead. However,
the highly non-convex and high-dimensional optimization
landscape means that small perturbations in the parameters of
either player can change the cost function of the other
player, which results in non-convergence.</p>

<p>Another problem with training GANs is that when either the
generator or discriminator becomes significantly better
than the other, then the learning signal for the other
player becomes very weak. For generators,
this is when the discriminator is always able to tell
it apart. For discriminators, this is when the generator
performs so well it can hardly do better than random guessing.</p>

<p>Finally, a common failure mode of GANs is mode collapse,
where the generator only learns to produce a set of very similar
outputs from a single mode instead of from all the modes.
This is due to the non-convexity of the optimization landscape.</p>

<h2 id="score-matching">Score Matching</h2>

<p>Score matching is a non-likelihood-based method to perform sampling
on an unknown data distribution, and seeks to address
many of the limitations of likelihood-based methods and
adversarial methods. This is achieved
by learning the score of the probability density function,
formally defined below:</p>

<div class="definition">
    <div class="theorem-title">Definition
        
        
            (Score Function)
        
    </div>
    <div class="theorem-contents">
        
    The score function of a distribution \( \pdata \) is given by
    \begin{align*}
        f(\bx) = \nabla_\bx \log \pdata.
    \end{align*}
  
    </div>
</div>

<p>In practice, we try to learn the score function using a neural
network \(\stx\) parameterized by \(\theta\).</p>

<p>The objective of score matching is to minimize the Fisher
Divergence between the score function and the score network:</p>

\[\begin{align} \label{eq:score-matching-target-fisher-div}
    \argmin_\theta \frac{1}{2} \E_{\pdata} \left[
        \| \stx - \nabla_\bx \log \pdata \|_2^2
        \right].
\end{align}\]

<p>However, the main problem here is that we do not know
\(\nabla_\bx \log \pdata\), since it depends on knowing
what \(\pdata\) is.</p>

<p><a href="http://jmlr.org/papers/v6/hyvarinen05a.html">(Hyvärinen, 2005)</a>
showed that Equation \ref{eq:score-matching-target-fisher-div}
is equivalent to Equation \ref{eq:score-matching-target} below:</p>

\[\begin{align} \label{eq:score-matching-target}
    \argmin_\theta \frac{1}{2} \E_{\pdata} \left[
        \tr \left( \nabla_\bx \stx \right) +
        \frac{1}{2} \| \stx \|_2^2
        \right].
\end{align}\]

<p>We can now compute this using Monte Carlo methods by sampling from \(\pdata\),
since it only depends on knowing \(\stx\).</p>

<h2 id="sliced-score-matching">Sliced Score Matching</h2>

<p>It is computationally difficult to compute the trace term 
\(\tr \left( \nabla_\bx \stx \right)\)
in Equation \ref{eq:score-matching-target} when \(\bx\) is high-dimensional.
This motivates another alternative cheaper approach for score matching,
called sliced score matching <a href="http://arxiv.org/abs/1905.07088">(Song et al., 2019)</a>.</p>

<p>In sliced score matching, we sample random vectors from some distribution \(\pv\) (such as the
multivariate standard Gaussian) in order to optimize an analog of the Fisher Divergence:</p>

\[\begin{align}
    L(\btheta, \pv) = \frac{1}{2} \E_{\pv} \E_{\pdata} \left[ (\bv^T \stx - \bv^T \sdx)^2 \right]
\end{align}\]

<p>We observe that</p>

\[\begin{align}
    L(\btheta; \pv) &amp;= \frac{1}{2} \E_{\pv} \E_{\pdata} \left[ (\bv^T \stx - \bv^T \sdx)^2 \right]\\
    &amp;=\frac{1}{2} \E_{\pv} \E_{\pdata} \left[ (\bv^T \stx )^2 + (\bv^T \sdx)^2 - 2(\bv^T \stx )(\bv^T \sdx) \right]\\
    &amp;= \E_{\pv} \E_{\pdata} \left[ \frac{1}{2}(\bv^T \stx )^2 - (\bv^T \stx )(\bv^T \sdx) \right] + C\\
\end{align}\]

<p>where the \(\sdx\) term is absorbed into \(C\) as it doesn’t depend on \(\theta\).
Now note</p>

\[\begin{align}
 &amp; -\E_{\pv} \E_{\pdata}\left[(\bv^T \stx )(\bv^T \sdx) \right] \\
=&amp; -\E_{\pv} \int \left[(\bv^T \stx )(\bv^T \sdx)  \pdata d\bx\right]\\
=&amp; -\E_{\pv}  \left[\int(\bv^T \stx )(\bv^T\nabla_{\bx}\log \pdata)\pdata d\bx\right] \\
=&amp; -\E_{\pv}  \left[\int(\bv^T \stx )(\bv^T\nabla_{\bx}\pdata)d\bx\right] \\
=&amp; -\E_{\pv}  \left[\int(\bv^T \stx )(\bv^T\nabla_{\bx}\pdata)d\bx\right] \\
=&amp; -\E_{\pv}  \left[\sum_{i}\int(\bv^T \stx )(v_i\frac{\partial \pdata}{\partial x_i})d\bx\right] \\
=&amp; \E_{\pv}  \left[\int \bv^T\stx\bv \cdot \pdata d\bx\right] \\
=&amp; \E_{\pv}\E_{\pdata}\left[\bv^T\stx\bv \right]
\end{align}\]

<p>where line 16 is obtained by applying multivariate integration by
parts. This finally yields the equivalent objective:</p>

\[\begin{align}
    J(\btheta; \pv) &amp;= \E_{\pv} \E_{\pdata} \left[ \bv^T \nabla_\bx \stx \bv + \frac{1}{2} \| \stx \|_2^2 \right]
\end{align}\]

<p>which no longer has a dependence on the unknown \(\nabla_{bx}\sdx\). This leads to the unbiased estimator:</p>

\[\begin{align}
\hat J_{N,M}(\btheta; \pv) &amp;=\frac{1}{N}\frac{1}{M}\sum_{i= 1}^N\sum_{j=1}^M \left[\bv_{ij}^T\nabla_{\bx}\mathbf{s}_\mathbf{\btheta}(\bx_i)\bv_{ij} + \frac{1}{2} \|\mathbf{s}_\mathbf{\btheta}(\bx_i)\|_2^2\right]
\end{align}\]

<p>where for each data point \(\bx_i\) we draw \(M\) projection vectors from \(\pv\).</p>

<p><a href="http://arxiv.org/abs/1905.07088">(Song et al., 2019)</a> showed that under some
regularity conditions, sliced score matching is an asymptotically consistent
estimator:</p>

\[\begin{align}
   \hat \btheta_{N,M} \overset{p}{\to} \btheta^* \text { as } \N \to \infty
\end{align}\]

<p>where</p>

\[\begin{align}
    \btheta^* &amp;= \underset{\btheta}{\text{argmin  }} J(\btheta; \pv), \\
    \hat \btheta_{N,M} &amp;= \underset{\btheta}{\text{argmin  }} \hat J_{N,M}(\btheta; \pv).
\end{align}\]

<p>Sliced score matching is computationally more efficient, since it now only involves Hessian-vector
products, and continues to work well in high dimensions.</p>

<h2 id="sampling-with-langevin-dynamics">Sampling with Langevin Dynamics</h2>

<p>Once we have trained a score network, we can sample from the data distribution
via Langevin dynamics. Langevin dynamics is a Markov Chain Monte Carlo
method of sampling from a stationary distribution, where we can efficiently
take gradients with respect to the probability of our samples \(\bx\).
We satisfy this criteria since we have the trained score network.</p>

<p>In Langevin dynamics, we start from some initial point \(\bx_0 \sim \bpi(\bx)\) sampled from some
prior distribution \(\bpi\), and then iteratively obtain updated points based on the
following recurrence:
\begin{align}
    \xt_t = \xt_{t-1} + \frac{\epsilon}{2} \nabla_\bx \log p(\xt_{t-1}) + \sqrt{\epsilon} \bz_t,
\end{align}
where \(\bz_t \sim \mathcal{N}(0, I)\). The addition of the Gaussian noise is required, or otherwise
the process simply converges to the nearest mode instead of converging to a stationary distribution.</p>

<p>It can be shown that as \(\epsilon \to 0\) and \(T \to \infty\), we have that the distribution
of the process \(\xt_T\) converges to \(\pdata\) 
<a href="https://dl.acm.org/doi/10.5555/3104482.3104568">(Welling &amp; Teh, 2011)</a>.</p>

<h2 id="challenges-of-langevin-dynamics">Challenges of Langevin Dynamics</h2>
<p>Langevin dynamics does not perform well with multi-modal distributions with poor conductance,
since it will tend to stay in a single mode, which causes long mixing times.
This is particularly a problem when the modes have disjoint supports, since there is very weak
gradient information in the region where there is no support.</p>

<h2 id="challenges-of-score-matching-for-generative-modeling">Challenges of Score Matching for Generative Modeling</h2>

<h3 id="the-manifold-hypothesis">The Manifold Hypothesis</h3>
<p>The manifold hypothesis postulates that real-world data often lies in a low-dimensional manifold
embedded in a high-dimensional space. This has been empirically observed in many datasets.</p>

<p>This poses problems for score matching.
The first problem that the manifold hypothesis poses is that the score 
\(\score\) becomes undefined if \(\bx\) actually just lies in a low-dimensional manifold.
The second problem is that the estimator in Equation \ref{eq:score-matching-target} is only consistent when
the support of \(\pdata\) is that of the whole space.</p>

<p>In order to increase the dimension of the data to match that of the ambient space,
<a href="http://jmlr.org/papers/v6/hyvarinen05a.html">(Hyvärinen, 2005)</a>
proposed injecting small amounts of Gaussian
noise into the data, such that now the data distribution has full support.
As long as the perturbation is sufficiently small (\(\mathcal{N}(0, 0.0001)\) was used in their paper),
it is almost indistinguishable to humans.</p>

<h3 id="low-data-density-regions">Low Data Density Regions</h3>
<p>The other problem with score matching is that it may not be able to learn
the score function in areas of low data density. This is due to the lack
of samples drawn from these regions, resulting in the Monte Carlo estimation
to have high variance.</p>

<h2 id="noise-conditional-score-networks-ncsn">Noise Conditional Score Networks (NCSN)</h2>

<p>The challenges mentioned in the previous sections are addressed
by Noise Conditional Score Networks (NCSN).</p>

<p>In NCSN, we define a geometric sequence of \(L\) noise levels
\({\left\{ \sigma_i \right\}}_{i=1}^L\), with the property
that \(\frac{\sigma_1}{\sigma_2} = \frac{\sigma_{L-1}}{\sigma_L}  &gt; 1\).
Each of these noise levels correspond to Gaussian noise that
will be added to perturb the data distribution, i.e
\(q_{\sigma_i} \sim \pdata + \mathcal{N}(0, \sigma_i)\).</p>

<p>We augment the score network to also take the noise level \(\sigma\) into
account, which is called the NCSN \(\stxt\).
The goal of NCSN is then to estimate the score conditioned on the
noise level. Once we have a trained NCSN, we use a similar
apporach as simulated annealing in Langevin sampling,
where we begin with a large noise level in order
to cross the different modes easily, before gradually
annealing down the noise to achieve convergence.</p>

<p>The denoising score matching objective for each noise level \(\sigma_i\) is given 
as</p>

\[\begin{align}
    \ell(\theta; \sigma) \triangleq \frac{1}{2} \E_{\pdata} \E_{\xt \sim \mathcal{N}(\bx, \sigma^2 I)} \left[ \left\| \stxt + \frac{\xt - \bx}{\sigma^2} \right\|_2^2 \right],
\end{align}\]

<p>and the unified objective for denoising across all levels is given as</p>

\[\begin{align}
    \mathcal{L}\left(\theta; \left\{ \sigma_i\right\}_{i=1}^L \right)
     \triangleq \frac{1}{L} \sum_{i=1}^L \lambda(\sigma_i) \ell(\theta; \sigma_i).
\end{align}\]

<h2 id="score-based-generative-modeling-through-stochastic-differential-equations-song-et-al-2021">Score-Based Generative Modeling through Stochastic Differential Equations <a href="https://arxiv.org/abs/2011.13456">(Song et al., 2021)</a></h2>

<p>We can extend the idea of having a finite number of noise scales
to having an infinite continuous number of such noise scales by modeling the
process as a diffusion process, which can be formalized as a 
stochastic differential equation (SDE). Such an SDE is given in the 
following form:</p>

\[\begin{align}
    d\bx = \boldf(\bx, t) \, dt + g(t) \, d\bw.
\end{align}\]

<p>Here, \(\boldf\) represents the drift coefficient,
which models the deterministic part of the SDE, and determines the rate
at which the process \(d\bx\) is expected to change over time on average.
\(g(t)\) is called the diffusion coefficient, which represents the random
part of the SDE, and determines the magnitude of the noising process
over time. Finally, \(\bw\) is Brownian motion. Thus \(g(t) \, d \bw\)
represents the noising process.</p>

<p>We want our diffusion process to be such that \(\bx(0) \sim p_0\) is 
the original data distribution, and 
\(\bx(T) \sim p_T\) is the Gaussian noise distribution that is independent
of \(p_0\). 
Then since every SDE has a corresponding reverse SDE, we can start
from the final noise distribution and run the reverse-time SDE in order
to recover a sample from \(p_0\), given by the following process:</p>

\[\begin{align}
d \bx = [\boldf (\bx, t) - g(t)^2 \nabla_{\bx} \log_{p_t} (\bx) ] \, dt + g(t) \,d \overline{w},
\end{align}\]

<p>where \(\overline{w}\) is Brownian motion that flows backwards in time from \(T\) to \(0\), and \(dt\) is an infinitesimal negative timestep.</p>

<p>The objective function for score matching for the SDE is then given by</p>

\[\begin{align}
    \argmin_{\theta} \E_t 
    \left[ 
    \lambda (t) \E_{\bx(0)} \E_{\bx (t) \mid \bx(0)} \left[ 
    \| \bs_\theta (\bx(t), t) - \nabla_{\bx(t)} \log p_{0t}(\bx (t) \mid \bx(0)) \|_2^2
    \right]
    \right].
\end{align}\]

<h3 id="score-based-generative-modeling-techniques">Score-based Generative Modeling Techniques</h3>

<p><a href="https://arxiv.org/abs/2011.13456">(Song et al., 2021)</a> covers two score-based generative models that uses SDEs to 
perform generative modeling.
The first is called score matching with Langevin dynamics (SMLD), which performs score estimation
at different noise scales and then performs sampling using Langevin dynamics with decreasing
noise scales. 
The second is denoising diffusion probabilistic modeling (DDPM)</p>

<p><a href="https://arxiv.org/abs/2006.11239">(Ho et al., 2020)</a>,
which uses a parameterized Markov chain that is trained with a re-weighted
variant of the evidence lower bound (ELBO), which is an instance of variational
inference. The Markov chain is trained to reverse the noise diffusion process,
which then allows sampling from the chain using standard Markov Chain Monte Carlo techniques.</p>

<p><a href="https://arxiv.org/abs/2011.13456">(Song et al., 2021)</a> shows that SMLD and DDPM actually corresponds to 
discretizations of the Variance Exploding (VE) and Variance Preserving (VP) SDEs, which
is the focus of the next two section. We believe expanding on this will be illuminating as
it highlights the connections between
SDEs and the discretized approaches that are used in practice.</p>

<h3 id="smld-as-discretization-of-variance-exploding-ve-sde">SMLD As Discretization of Variance Exploding (VE) SDE</h3>
<p>Recall that we use a geometric sequence of \(L\) noise levels 
\({\left\{ \sigma_i \right\}}_{i=1}^L\).
that is added to the data distribution</p>

<p>We can recursively define the distribution for each noise level \(i\) by incrementally adding noise:</p>

\[\begin{align}
    \bx_i = \bx_{i-1} + \sqrt{\sigma_i^2 - \sigma_{i-1}^2} \bz_{i-1}, \qquad \qquad i = 1, \dots, L,
\end{align}\]

<p>where \(\bz_{i-1} \sim \mathcal{N}(\mathbf{0}, \bI)\), and \(\sigma_0 = 0\) so \(\bx_0 \sim \pdata\).</p>

<p>If we view the noise levels as gradually changing in time, then the continuous time limit
of the process is given by the following SDE:
\begin{align}
    \bx(t + \Delta t) = \bx(t) + \sqrt{\sigma^2 (t + \Delta t ) - \sigma^2 (t)} \bz(t) \approx \bx(t) + 
    \sqrt{\frac{d [\sigma^2 (t)]}{dt} \Delta t } \bz (t),
\end{align}
where the approximation holds when \(\Delta t \ll 1\). If we take \(\Delta t \to 0\),
we recover the VE SDE:
\begin{align}
   d \bx = \sqrt{\frac{d [\sigma^2 (t)]}{dt} } d \bw,
\end{align}
which causes the variance of \(d \bx(t)\) to go to infinity as \(t \to \infty\) due to its geometric growth,
hence its name.</p>

<h3 id="ddpm-as-discretization-of-variance-preserving-vp-sde">DDPM As Discretization of Variance Preserving (VP) SDE</h3>
<p>Similarly, the Markov chain of the perturbation kernel of DDPM is given by
\begin{align}
    \bx_i = \sqrt{1 - \beta_i} \bx_{i-1} + \sqrt{\beta_i} \bz_{i-1}, \qquad i = 1, \cdots, L,
\end{align}
where \(\left\{ \beta_i \right\}_{i=1}^L\) are the noise scales,
and if we take \(L \to \infty\) with scaled noise scales \(\overline{\beta_i} = N \beta_i\), we get
\begin{align}
    \bx_i = \sqrt{1 - \frac{\bov_i}{N} } \bx_{i-1} + \sqrt{ \frac{\bov_i}{N} } \bz_{i-1}, \qquad i = 1, \cdots, L.
\end{align}
Now taking limits with \(L \to \infty\), we get
\begin{align}
    \bx(t + \Delta t) \approx \bx(t) - \frac{1}{2} \beta(t) \Delta t \bx(t) + \sqrt{\beta(t) \Delta t} \bz(t),
\end{align}
where the approximation comes from the second degree Taylor expansion of \(\sqrt{1 - \beta(t + \Delta t) \Delta t}\). Then taking the limit of \(\Delta t \to 0\), we obtain the VP SDE
\begin{align}
    d \bx = - \frac{1}{2} \beta(t) \bx dt + \sqrt{\beta(t)} d \bw.
\end{align}
This process thus has bounded variance since \(\beta_i\) is bounded.</p>

<h1 id="experiments">Experiments</h1>
<p>We conduct the following preliminary series of experiments, based on released work by <a href="https://arxiv.org/abs/1907.05600">(Song &amp; Ermon, 2019)</a>.</p>

<h2 id="investigating-the-manifold-hypothesis">Investigating the manifold hypothesis</h2>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/score-based-diffusion-models/sample_dist.avif" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption"><i>Figure 1.</i>
      
        Comparison between true data density and sampling
    
    </figcaption>
</figure>

<p>In this experiment, we have plotted the true data density of a toy distribution
along with samples drawn in three ways. The i.i.d samples are drawn directly
from the underlying distribution and we can see that more samples are drawn in
the area of high data density. However, applying Langevin dynamics without
annealing, we see that there is an almost equal number of points in the top left
and bottom right corners. This is evidence that the sampling method doesn’t
conform to the true distribution. Finally, by injecting and decreasing the
amount of noise through the annealing process, we can recover a representative
sample of the distribution.</p>

<h2 id="importance-of-annealing-when-sampling-via-langevin-dynamics">Importance of annealing when sampling via Langevin Dynamics</h2>

<p>To better visualize the effects of annealing when sampling via Langevin Dynamics, we generated images from a model trained on the CelebA dataset. We first tried applying Langevin Dynamics with a fixed noise and then used annealing to gradually decrease the noise.</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/score-based-diffusion-models/annealing_ablation.avif" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption"><i>Figure 2.</i>
      
        Langevin Dynamics with no annealing (top) and annealing (bottom)
    
    </figcaption>
</figure>

<p>Figure 2 shows that the results with annealing are significantly clearer and more varied, matching the performance of GANs in 2019.</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/score-based-diffusion-models/left_right.avif" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption"><i>Figure 3.</i>
      
        Closer comparison of no annealing (left) and annealing (right)
    
    </figcaption>
</figure>

<p>We notice that the image generated without annealing manages to produce the structure of a human face but fails to capture finer details such as the hair, and the surrounding backdrop. There is also little variation in color between different samples. This is in agreement with our theory that without annealing, Langevin dynamics cannot properly explore regions of lower data density.</p>

<h2 id="effect-of-noise-parameters-for-annealed-langevin-dynamics">Effect of noise parameters for annealed Langevin Dynamics</h2>
<p>We also investigated the effect of changing the lowest noise standard deviation \(\sigma\) while keeping the number of different noises injected fixed at \(10\). The 10 noise values are determined by an interpolation in log scale.</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/score-based-diffusion-models/vary_sigma.avif" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption"><i>Figure 4.</i>
      
        Left to right: \( \sigma_{\text{end}} = \{0.1, 0.01, 0.001\} \)
    
    </figcaption>
</figure>

<p>Our experiment shows that the effect of starting, ending, and the interval between noise values has a significant effect on the convergence of annealed Langevin sampling.</p>

<h1 id="discussion-and-future-work">Discussion and Future Work</h1>
<p>Having completed a survey of score-based diffusion models, and having
run some experiments on them, we now turn our attention to discussing the
pros and cons of this approach.</p>

<p>As mentioned previously in this paper, the main draw of 
score-based diffusion models is that it has shown to be capable of generating impressive
high-quality samples that is on-par with the state-of-the-art with GANs.
We hence focus on its limitations and how they might be overcome, drawing 
from work in <a href="https://arxiv.org/abs/2209.02646">(Cao et al., 2022)</a>.</p>

<h2 id="computation-cost">Computation Cost</h2>
<p>A common refrain of score-based diffusion model is the high computational
complexity in both training and sampling. This is because it requires 
thousands of small diffusion steps in order to ensure that the forward
and reverse SDEs hold in their approximations
<a href="https://arxiv.org/abs/2202.09671">(Zheng et al., 2022)</a>.
If the diffusion steps are too large,
then the Gaussian noise assumption may not hold, resulting in poor score
estimates.
This makes it significantly more expensive than other generative methods like
GANs and VAEs.  To this end, there are some directions being explored to improve
its computation cost.</p>

<p>The first technique seeks to reduce the number of sampling steps required by a
method known as knowledge distillation <a href="http://arxiv.org/abs/1710.07535">(Lopes et al., 2017)</a>.
In knowledge distillation, knowledge is transferred from a larger and more
complex model (called the teacher), to one that is smaller and simpler (called
the student).  This technique has found success in other domains such as image
classification, and has also been shown to result in improvements in diffusion
models 
<a href="https://arxiv.org/abs/2202.00512">(Salimans &amp; Ho, 2022)</a>. It would be interesting to see how far we
can take this optimization.</p>

<p>Another technique known as truncated diffusion probabilistic modeling (TDPM)
<a href="https://arxiv.org/abs/2202.09671">(Zheng et al., 2022)</a>.
In this approach, instead of considering the diffusion process until it becomes pure noise,
the process is stopped once it reaches a hidden noisy-data distribution that 
can be learnt by an auto-encoder by adversarial training. Then in order to produce
samples, a sample is first drawn from the learnt noisy-data distribution,
before being passed through the reverse-SDE diffusion steps.</p>

<p>It also suffers from poor explainability and interpretability, but this is a
common problem across other generative models.</p>

<p><a href="https://arxiv.org/abs/2011.13456">(Song et al., 2021)</a> also notes that it is currently difficult to tune
the myriad of hyperparameters introduced by the choice of noise levels and specific
samplers chosen, and new methods to automatically select and tune these hyperparameters
would make score-based diffusion models more easily deployable in practice.</p>

<h2 id="modality-diversity">Modality Diversity</h2>
<p>Diffusion models have mostly only seen applications for generating image data,
and its potential for generating other data modalities has not been as thoroughly 
investigated.
<a href="https://arxiv.org/abs/2107.03006">(Austin et al., 2021)</a> introduces 
Discrete Denoising Diffusion Probabilistic Models (D3PMs), which develops a
diffusion process for corrupting text data into noise. It would be interesting
to see how well diffusion models can be stretched to perform compared to
state-of-the-art transformer models in text generation.</p>

<h2 id="dimensionality-reduction">Dimensionality Reduction</h2>
<p>Dimensionality reduction is another technique that can be used to speed up
training and sampling speeds of diffusion models.
Diffusion models are typically trained directly in data space.
<a href="https://arxiv.org/abs/2106.05931">(Vahdat et al., 2021)</a>
instead proposes for them to be trained in latent space, which results in
dimensionality reduction in the representation learnt, and also potentially
increases the expressiveness of the framework.
In a similar vein, 
<a href="https://arxiv.org/abs/2211.16032">(Zhang et al., 2022)</a>
argues that due to redundancy in spatial data, it is not necessary
to learn in data space, and instead proposes a 
dimensionality-varying diffusion process (DVDP), where
the dimensionality of the signal is dynamically adjusted during
the both the diffusion and denoising process.</p>

<h1 id="conclusion">Conclusion</h1>
<p>We showed that score matching presents a promising new direction
for generative models, which avoids many of the limitations of other
approaches such as training instability and mode collapse in GANs,
and poor approximation guarantees in variational inference. While
score matching has several flaws, such as suffering from the manifold
hypothesis and requiring an expensive Langevin dynamics process in order
to draw samples, successive work has done well in addressing these limitations
to make score matching on diffusion models a viable contender to displace
GANs as the state-of-the-art for generative modeling.</p>

<p>Our experiments in this blog post help to provide empirical context to the theoretical results we have derived. Most notably, we have shown how annealing is an essential part of sampling via Langevin dynamics.</p>

<p>Finally, we discuss some future directions that can help to improve
the viability of using score-based diffusion models, which includes improving
its computational cost in both training and sampling and increasing the
diversity of applicable modalities.</p>

<h1 id="citation">Citation</h1>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{zeng2023diffusion,
  title   = {Score-Based Diffusion Models},
  author  = {Fan Pu Zeng and Owen Wang},
  journal = {fanpu.io},
  year    = {2023},
  month   = {Jun},
  url     = {https://fanpu.io/blog/2023/score-based-diffusion-models/}
}
</code></pre></div></div>

<h1 id="references">References</h1>
<ul>
  <li>Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den Berg, R. 
<a href="https://arxiv.org/abs/2107.03006">Structured denoising diffusion models in discrete state-spaces.</a> CoRR, abs/2107.03006, 2021.</li>
  <li>Cao, H., Tan, C., Gao, Z., Chen, G., Heng, P.-A., and Li, S. Z. 
<a href="https://arxiv.org/abs/2209.02646">A survey on generative diffusion model</a>, 2022.</li>
  <li>Ho, J., Jain, A., and Abbeel, P.  <a href="https://arxiv.org/abs/2006.11239">Denoising diffusion probabilistic
models</a>.  CoRR, abs/2006.11239, 2020. URL
https://arxiv.org/abs/2006.11239.</li>
  <li>Hyva ̈rinen, A. <a href="http://jmlr.org/papers/v6/hyvarinen05a.html">Estimation of non-normalized statistical models by score matching</a>. Journal of Machine Learning Research, 6(24):695–709, 2005.</li>
  <li>Lopes, R. G., Fenu, S., and Starner, T. <a href="http://arxiv.org/abs/1710.07535">Data-free knowledge distillation for deep neural networks</a>. CoRR, abs/1710.07535,
2017.</li>
  <li>Salimans, T. and Ho, J. <a href="https://arxiv.org/abs/2202.00512">Progressive distillation for fast sampling of diffusion models</a>. CoRR, abs/2202.00512, 2022.</li>
  <li>Song, Y. and Ermon, S. 
<a href="http://arxiv.org/abs/1907.05600">Generative modeling by estimating gradients of the data distribution</a>. CoRR, abs/1907.05600, 2019.</li>
  <li>Song, Y., Garg, S., Shi, J., and Ermon, S. 
<a href="http://arxiv.org/abs/1905.07088">Sliced score matching: A scalable approach to density and score estimation</a>. CoRR,
abs/1905.07088, 2019. URL http://arxiv.org/abs/1905.07088.</li>
  <li>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole,
B. <a href="https://arxiv.org/abs/2011.13456">Score-based generative modeling through stochastic differential
equations</a>. ICLR, abs/1907.05600, 2021.</li>
  <li>Vahdat, A., Kreis, K., and Kautz, J. <a href="https://arxiv.org/abs/2106.05931">Score-based generative modeling in latent space</a>, 2021.</li>
  <li>Welling, M. and Teh, Y. W. <a href="https://dl.acm.org/doi/10.5555/3104482.3104568">Bayesian learning via stochastic gradient langevin dynamics</a>. In Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML’11, pp. 681–688, Madison, WI, USA, 2011. Omnipress. ISBN 9781450306195.</li>
  <li>Zhang, H., Feng, R., Yang, Z., Huang, L., Liu, Y., Zhang, Y., Shen, Y., Zhao, D., Zhou, J., and Cheng, F. <a href="https://arxiv.org/abs/2211.16032">Dimensionality-varying diffusion process</a>, 2022.</li>
  <li>Zheng, H., He, P., Chen, W., and Zhou, M. <a href="https://arxiv.org/abs/2202.09671">Truncated diffusion probabilistic models and diffusion-based adversarial auto-encoders</a>, 2022.</li>
</ul>]]></content><author><name>fanpu</name></author><category term="machine-learning" /><summary type="html"><![CDATA[Score-based diffusion models are a promising direction for generative models, as they improve on both likelihood-based approaches like variational autoencoders, as well as adversarial methods like Generative Adversarial Networks (GANs). In this blog post, we survey recent developments in the field centered around the line of results developed in (Song & Ermon, 2019), analyze the current strengths and limitations of score-based diffusion models, and discuss possible future directions that can address its drawbacks. Joint work with Owen Wang.]]></summary></entry><entry><title type="html">The Art of LaTeX: Common Mistakes, and Advice for Typesetting Beautiful, Delightful Proofs</title><link href="https://fanpu.io/blog/2023/latex-tips/" rel="alternate" type="text/html" title="The Art of LaTeX: Common Mistakes, and Advice for Typesetting Beautiful, Delightful Proofs" /><published>2023-01-02T00:00:00+00:00</published><updated>2023-01-02T00:00:00+00:00</updated><id>https://fanpu.io/blog/2023/latex-tips</id><content type="html" xml:base="https://fanpu.io/blog/2023/latex-tips/"><![CDATA[<p>When was the first time you had to use <a href="https://www.latex-project.org/">LaTeX</a>?
If you are like most people, it was probably suddenly forced upon you during
your first math or CS class where you had to start writing proofs, with minimal
guidance on how to get started other than something along the lines of “hey,
check out this link on how to get things setup, and here are some basic
commands, now go wild!”.</p>

<p>Unfortunately, this meant that while many people have good operational knowledge
of LaTeX and can get the job done, there are still many small mistakes and
best practices which are not followed, which are not corrected by TAs
as they are either not severe enough to warrant a note, or perhaps even the TAs themselves
are not aware of them.</p>

<p>In this post, we cover some common mistakes that are made by LaTeX
practitioners (even in heavily cited papers), and how to address them. This
post assumes that the reader has some working knowledge of LaTeX.</p>

<h2 id="typesetting-as-a-form-of-art">Typesetting as a Form of Art</h2>
<p>It is important to get into the right mindset whenever you typeset a
document. You are not simply “writing” a document — you are crafting a work
of art that combines both the precision and creativity of your logical
thinking, as well as the elegance of a beautifully typeset writing. The amount
of attention and care you put into the presentation is indicative of the amount
of thought you put into the content. Therefore, having good style is not
only delightful and aesthetically pleasing to read, but it also serves to establish 
your ethos and character. One can tell that someone puts a lot of effort into their
work and takes great pride in them when they pay attention even to the smallest of
details.</p>

<p>Furthermore, adopting good practices also helps to avoid you making
typographical mistakes in your proof, such as missing parenthesis or wrong
positioning. This could often lead to cascading errors that are very annoying
to fix when you discover them later on. There are ways to replicate the strict
typechecking of statically typed languages to ensure that mistakes in your
expressions can be caught at compile-time.</p>

<h2 id="common-mistakes-and-how-to-fix-them">Common Mistakes, and How To Fix Them</h2>
<p>In the following section, we take a look at common mistakes that people make,
and how they can be avoided or fixed. We cover style mistakes first, since the
ideas behind them are more general. All the screenshotted examples come from
peer-reviewed papers that have been published to top conferences, so they are
definitely very common mistakes and you shouldn’t feel bad for making them.
The important thing is that you are aware of them now so that your style
will gradually improve over time.</p>

<h2 id="style-mistakes">Style Mistakes</h2>
<p>We take a look at style mistakes, which impairs reader understanding,
and makes it easy to commit other sorts of errors.</p>

<h3 id="paired-delimiters">Paired Delimiters</h3>
<p>Parenthesis, brackets, and pipes are examples of delimiters that are used to mark the start
and end of formula expressions. As they come in pairs, a common mistake is accidentally 
leaving out the closing delimiter, especially for nested expressions. Even if you don’t
forget to do so, there is the issue of incorrect sizing.</p>

<p>For instance, consider
the following way of expressing the Topologist’s sine curve, which is an example of a topology
that is connected but not path connected:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex">T = <span class="k">\{</span> ( x, <span class="k">\sin</span> <span class="k">\frac</span><span class="p">{</span>1<span class="p">}{</span>x<span class="p">}</span> ) : x <span class="k">\in</span> (0, 1] <span class="k">\}</span> <span class="k">\cup</span> <span class="k">\{</span> ( 0, 0 ) <span class="k">\}</span></code></pre></figure>

<p>which is rendered as follows:</p>

\[T = \{(x, \sin \frac{1}{x} ) : x \in (0, 1] \} \cup \{ ( 0, 0 ) \}\]

<p>The problem here is that the curly braces have the wrong
size, as they should be large enough to cover the \(\sin \frac{1}{x}\) expression vertically.</p>

<p>The wrong way of resolving this would be to use delimiter size modifiers, i.e
<code class="language-plaintext highlighter-rouge">\bigl, \Bigl, \biggl</code> paired with <code class="language-plaintext highlighter-rouge">\bigr, \Bigr, \biggr</code>  and the like. This
is tedious and error-prone, since it will even happily let you match delimiters
with different sizes. Indeed, I came across the following formula in a
paper recently, where the outer right square brackets was missing and the left one
had the wrong size:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/paired_delim.avif" class="z-depth-1 center" width="500px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      What happens when you don't use paired delimiters
    </figcaption>
</figure>

<p>The correct way to do this would be to use paired delimiters,
which will automatically adjust its size based on
its contents, and automatically result in a compile error if the matching
right delimiter is not included, or nested at the wrong level.
Some of them are given below:</p>

<table class="table table-bordered table-sm">
  <thead>
    <tr>
      <th>Raw LaTeX</th>
      <th>Rendered</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\left( \frac{1}{x} \right)   </code></td>
      <td>\(\left( \frac{1}{x} \right)\)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\left[ \frac{1}{x} \right]   </code></td>
      <td>\(\left[ \frac{1}{x} \right]\)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\left\{ \frac{1}{x} \right\} </code></td>
      <td>\(\left\{ \frac{1}{x} \right\}\)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\left\lvert \frac{1}{x} \right\lvert </code></td>
      <td>\(\left\lvert \frac{1}{x} \right\rvert\)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\left\lceil \frac{1}{x} \right\rceil </code></td>
      <td>\(\left\lceil \frac{1}{x} \right\rceil\)</td>
    </tr>
  </tbody>
</table>

<p>In fact, to make things even simpler and more readable, you can declare paired delimiters 
for use based on the <code class="language-plaintext highlighter-rouge">mathtools</code> package, with the following commands due to
<a href="http://www.cs.cmu.edu/~odonnell/">Ryan O’Donnell</a>:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="c">% Make sure you include \usepackage{mathtools}</span>
<span class="k">\DeclarePairedDelimiter\parens</span><span class="p">{</span><span class="k">\lparen</span><span class="p">}{</span><span class="k">\rparen</span><span class="p">}</span>
<span class="k">\DeclarePairedDelimiter\abs</span><span class="p">{</span><span class="k">\lvert</span><span class="p">}{</span><span class="k">\rvert</span><span class="p">}</span>
<span class="k">\DeclarePairedDelimiter\norm</span><span class="p">{</span><span class="k">\lVert</span><span class="p">}{</span><span class="k">\rVert</span><span class="p">}</span>
<span class="k">\DeclarePairedDelimiter\floor</span><span class="p">{</span><span class="k">\lfloor</span><span class="p">}{</span><span class="k">\rfloor</span><span class="p">}</span>
<span class="k">\DeclarePairedDelimiter\ceil</span><span class="p">{</span><span class="k">\lceil</span><span class="p">}{</span><span class="k">\rceil</span><span class="p">}</span>
<span class="k">\DeclarePairedDelimiter\braces</span><span class="p">{</span><span class="k">\lbrace</span><span class="p">}{</span><span class="k">\rbrace</span><span class="p">}</span>
<span class="k">\DeclarePairedDelimiter\bracks</span><span class="p">{</span><span class="k">\lbrack</span><span class="p">}{</span><span class="k">\rbrack</span><span class="p">}</span>
<span class="k">\DeclarePairedDelimiter\angles</span><span class="p">{</span><span class="k">\langle</span><span class="p">}{</span><span class="k">\rangle</span><span class="p">}</span></code></pre></figure>

<p>Then you can now use the custom delimiters as follows, taking note that you need the <code class="language-plaintext highlighter-rouge">*</code>
for it to auto-resize:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex">T = <span class="k">\braces*</span><span class="p">{</span> <span class="k">\parens*</span><span class="p">{</span> x, <span class="k">\sin</span> <span class="k">\frac</span><span class="p">{</span>1<span class="p">}{</span>x<span class="p">}</span> <span class="p">}</span> : x <span class="k">\in</span> (0, 1] <span class="p">}</span> <span class="k">\cup</span> <span class="k">\braces*</span><span class="p">{</span> <span class="k">\parens*</span><span class="p">{</span> 0, 0 <span class="p">}}</span></code></pre></figure>

<p>which gives</p>

\[T = \left\{ \left( x, \sin \frac{1}{x} \right) : x \in (0, 1] \right\} \cup \left\{ \left( 0, 0 \right) \right\} \\\]

<p>The biggest downside of using custom paired delimiters is having to remember to
add the <code class="language-plaintext highlighter-rouge">*</code>, otherwise, the delimiters will not auto-resize. This is pretty unfortunate
as it still makes it error-prone. There is a <a href="https://tex.stackexchange.com/questions/1742/automatic-left-and-right-commands/1744#1744">proposed
solution</a>
floating around on StackExchange that relies on a custom command that makes auto-resizing
the default, but it’s still a far cry from a parsimonious solution.</p>

<h3 id="macros-for-saving-time-and-preventing-mistakes">Macros for Saving Time and Preventing Mistakes</h3>
<p>Macros can be defined using the <code class="language-plaintext highlighter-rouge">\newcommand</code> command.
The basic syntax is <code class="language-plaintext highlighter-rouge">\newcommand{command_name}{command_definition}</code>.
For instance, it might get tiring to always type <code class="language-plaintext highlighter-rouge">\boldsymbol{A}</code>
to refer to a matrix \(\boldsymbol{A}\), so you can use the following macro:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="c">% Macro</span>
<span class="k">\newcommand</span><span class="p">{</span><span class="k">\bA</span><span class="p">}{</span><span class="k">\boldsymbol</span><span class="p">{</span>A<span class="p">}}</span>

<span class="p">$$</span><span class="nv">\min</span><span class="p">_</span><span class="nb">x </span><span class="nv">\lvert</span><span class="nb"> </span><span class="nv">\bA</span><span class="nb"> x </span><span class="o">-</span><span class="nb"> b </span><span class="nv">\rvert</span><span class="p">_</span><span class="m">2</span><span class="p">^</span><span class="m">2</span><span class="p">$$</span></code></pre></figure>

\[\min_x \left\lvert \boldsymbol{A} x - b \right\rvert_2^2\]

<p>Macros can also take arguments to be substituted within the definition.
This is done by adding a <code class="language-plaintext highlighter-rouge">[n]</code> argument after your command name,
where <code class="language-plaintext highlighter-rouge">n</code> is the number of arguments that it should take. You can then
reference the positional arguments using <code class="language-plaintext highlighter-rouge">#1, #2,</code> and so on.
Here, we create a <code class="language-plaintext highlighter-rouge">\dotprod</code> macro that takes two arguments:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="c">% Macros</span>
<span class="k">\newcommand</span><span class="p">{</span><span class="k">\dotprod</span><span class="p">}</span>[2]<span class="p">{</span><span class="k">\langle</span> #1, #2 <span class="k">\rangle</span><span class="p">}</span>
<span class="k">\newcommand</span><span class="p">{</span><span class="k">\bu</span><span class="p">}{</span><span class="k">\boldsymbol</span><span class="p">{</span>u<span class="p">}}</span>
<span class="k">\newcommand</span><span class="p">{</span><span class="k">\bv</span><span class="p">}{</span><span class="k">\boldsymbol</span><span class="p">{</span>v<span class="p">}}</span>

<span class="p">$$</span><span class="nv">\left\lvert</span><span class="nb"> </span><span class="nv">\dotprod</span><span class="p">{</span><span class="nv">\bu</span><span class="p">}{</span><span class="nv">\bv</span><span class="p">}</span><span class="nb"> </span><span class="nv">\right\rvert</span><span class="p">^</span><span class="m">2</span><span class="nb"> </span><span class="nv">\leq</span><span class="nb"> </span><span class="nv">\dotprod</span><span class="p">{</span><span class="nv">\bu</span><span class="p">}{</span><span class="nv">\bu</span><span class="p">}</span><span class="nb"> </span><span class="nv">\cdot</span><span class="nb"> </span><span class="nv">\dotprod</span><span class="p">{</span><span class="nv">\bv</span><span class="p">}{</span><span class="nv">\bv</span><span class="p">}$$</span></code></pre></figure>

\[\left\lvert \dotprod{\bu}{\bv} \right\rvert^2 \leq \dotprod{\bu}{\bu} \cdot \dotprod{\bv}{\bv}\]

<p>Macros are incredibly helpful as they help to save time, and ensure that our
notation is consistent. However, they can also be used to help to catch
mistakes when typesetting grammatically structured things.</p>

<p>For instance, when expressing types and terms in programming language theory, 
there is often a lot of nested syntactical structure, which could make it easy
to make mistakes.  Consider the following proof:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/macros.avif" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      A proof with a lot of syntactical structure
    </figcaption>
</figure>

<p>The details are unimportant, but it is clear that it is easy to miss a letter here
or a term there in the proof, given how cumbersome the notation is.
To avoid this, I used the following macros, due to <a href="http://www.cs.cmu.edu/~rwh/">Robert Harper</a>:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="k">\newcommand</span><span class="p">{</span><span class="k">\inval</span><span class="p">}</span>[2]<span class="p">{</span><span class="k">\in</span><span class="p">^{</span>(#1)<span class="p">}_</span><span class="k">\mathsf</span><span class="p">{</span>val<span class="p">}</span> #2<span class="p">}</span>
<span class="k">\newcommand</span><span class="p">{</span><span class="k">\foldex</span><span class="p">}</span>[2]<span class="p">{</span><span class="k">\mathsf</span><span class="p">{</span>fold<span class="p">}_{</span>#1<span class="p">}</span>(#2)<span class="p">}</span>
<span class="k">\newcommand</span><span class="p">{</span><span class="k">\recty</span><span class="p">}</span>[2]<span class="p">{</span><span class="k">\mathsf</span><span class="p">{</span>rec<span class="p">}</span>(#1.#2)<span class="p">}</span>
<span class="k">\newcommand</span><span class="p">{</span><span class="k">\Subst</span><span class="p">}</span>[3]<span class="p">{</span><span class="k">\sqbracks</span><span class="p">{{</span>#1<span class="p">}</span><span class="k">\mathord</span><span class="p">{</span>/<span class="p">}{</span>#2<span class="p">}}{</span>#3<span class="p">}}</span></code></pre></figure>

<p>And the source for the proof looks like the following:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex">We check that anti-monotonicity continues to hold for recursive types,
by showing that if <span class="p">$</span><span class="nb">m </span><span class="nv">\leq</span><span class="nb"> n</span><span class="p">$</span>, then
<span class="p">$$</span><span class="nv">\foldex</span><span class="p">{</span><span class="nb">X.A</span><span class="p">}{</span><span class="nb">V</span><span class="p">}</span><span class="nb"> </span><span class="nv">\inval</span><span class="p">{</span><span class="nb">n</span><span class="p">}{</span><span class="nv">\recty</span><span class="p">{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}</span><span class="nb"> </span><span class="nv">\text</span><span class="p">{</span><span class="nb"> implies </span><span class="p">}</span><span class="nb"> </span><span class="nv">\foldex</span><span class="p">{</span><span class="nb">X.A</span><span class="p">}{</span><span class="nb">V</span><span class="p">}</span><span class="nb"> </span><span class="nv">\inval</span><span class="p">{</span><span class="nb">m</span><span class="p">}{</span><span class="nv">\recty</span><span class="p">{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}</span><span class="nb">. </span><span class="p">$$</span>

<span class="nt">\begin{proof}</span>
We proceed by induction on <span class="p">$</span><span class="nb">n</span><span class="p">$</span>. 
When <span class="p">$</span><span class="nb">n</span><span class="o">=</span><span class="m">0</span><span class="p">$</span>, the result is trivial, so consider <span class="p">$</span><span class="nb">n </span><span class="nv">\geq</span><span class="nb"> </span><span class="m">0</span><span class="p">$</span>, with the intent to prove it for <span class="p">$</span><span class="nb">n</span><span class="o">+</span><span class="m">1</span><span class="p">$</span>.

Let <span class="p">$</span><span class="nb">m </span><span class="nv">\leq</span><span class="nb"> n </span><span class="o">+</span><span class="nb"> </span><span class="m">1</span><span class="p">$</span>, and assume
<span class="p">$</span><span class="nv">\foldex</span><span class="p">{</span><span class="nb">X.A</span><span class="p">}{</span><span class="nb">V</span><span class="p">}</span><span class="nb"> </span><span class="nv">\inval</span><span class="p">{</span><span class="nb">n</span><span class="o">+</span><span class="m">1</span><span class="p">}{</span><span class="nv">\recty</span><span class="p">{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}$</span>. If <span class="p">$</span><span class="nb">m </span><span class="o">=</span><span class="nb"> n </span><span class="o">+</span><span class="nb"> </span><span class="m">1</span><span class="p">$</span> or <span class="p">$</span><span class="nb">m</span><span class="o">=</span><span class="m">0</span><span class="p">$</span>, we are trivially done, so let <span class="p">$</span><span class="m">0</span><span class="nb"> &lt; m &lt; n</span><span class="o">+</span><span class="m">1</span><span class="p">$</span>.

We want to show that
<span class="p">$</span><span class="nv">\foldex</span><span class="p">{</span><span class="nb">X.A</span><span class="p">}{</span><span class="nb">V</span><span class="p">}</span><span class="nb"> </span><span class="nv">\inval</span><span class="p">{</span><span class="nb">m</span><span class="p">}{</span><span class="nv">\recty</span><span class="p">{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}$</span>.
By definition of step-indexed logical relations~(SILR), it suffices to show
<span class="p">$</span><span class="nb">V </span><span class="nv">\inval</span><span class="p">{</span><span class="nb">m</span><span class="o">-</span><span class="m">1</span><span class="p">}{</span><span class="nv">\Subst</span><span class="p">{</span><span class="nv">\recty</span><span class="p">{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}$</span>.

Since <span class="p">$</span><span class="nv">\foldex</span><span class="p">{</span><span class="nb">X.A</span><span class="p">}{</span><span class="nb">V</span><span class="p">}</span><span class="nb"> </span><span class="nv">\inval</span><span class="p">{</span><span class="nb">n</span><span class="o">+</span><span class="m">1</span><span class="p">}{</span><span class="nv">\recty</span><span class="p">{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}$</span>, by definition of SILR,
<span class="p">$</span><span class="nb">V </span><span class="nv">\inval</span><span class="p">{</span><span class="nb">n</span><span class="p">}{</span><span class="nv">\Subst</span><span class="p">{</span><span class="nv">\recty</span><span class="p">{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}$</span>.

By IH on <span class="p">$</span><span class="nb">V </span><span class="nv">\inval</span><span class="p">{</span><span class="nb">n</span><span class="p">}{</span><span class="nv">\Subst</span><span class="p">{</span><span class="nv">\recty</span><span class="p">{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}$</span>,
we also know <span class="p">$</span><span class="nb">V </span><span class="nv">\inval</span><span class="p">{</span><span class="nb">m</span><span class="o">-</span><span class="m">1</span><span class="p">}{</span><span class="nv">\Subst</span><span class="p">{</span><span class="nv">\recty</span><span class="p">{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}$</span>.

But then by definition of SILR,
<span class="p">$</span><span class="nv">\foldex</span><span class="p">{</span><span class="nb">X.A</span><span class="p">}{</span><span class="nb">V</span><span class="p">}</span><span class="nb"> </span><span class="nv">\inval</span><span class="p">{</span><span class="nb">m</span><span class="p">}{</span><span class="nv">\recty</span><span class="p">{</span><span class="nb">X</span><span class="p">}{</span><span class="nb">A</span><span class="p">}}$</span>, as desired. <span class="k">\qedhere</span>
<span class="nt">\end{proof}</span></code></pre></figure>

<p>It is definitely still not the most pleasant thing to read, but at least now you
will be less likely to miss an argument or forget to close a parenthesis.</p>

<h3 id="non-breaking-lines">Non-breaking lines</h3>
<p>Expressions which are logically a single unit should stay on the same line, instead
of being split apart mid-sentence. Cue the following bad example from another paper:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/nbsp.avif" class="z-depth-1 center" width="300px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Expressions that are broken apart
    </figcaption>
</figure>

<p>In the area marked in red, we had the expression that was defining \(\tau^i\)
get cut in half, which is very jarring visually and interrupts the reader’s
train of thought.</p>

<p>To ensure that expressions do not get split, simply wrap it around in curly braces.
For instance,</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="k">\tau</span>=<span class="k">\left</span>(s<span class="p">_</span>1, a<span class="p">_</span>1, <span class="k">\ldots</span>, a<span class="p">_{</span>t-1<span class="p">}</span>, s<span class="p">_</span>t<span class="k">\right</span>)</code></pre></figure>

<p>would be wrapped by <code class="language-plaintext highlighter-rouge">{</code> and <code class="language-plaintext highlighter-rouge">}</code> on both sides and become</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="p">{</span> <span class="k">\tau</span>=<span class="k">\left</span>(s<span class="p">_</span>1, a<span class="p">_</span>1, <span class="k">\ldots</span>, a<span class="p">_{</span>t-1<span class="p">}</span>, s<span class="p">_</span>t<span class="k">\right</span>) <span class="p">}</span></code></pre></figure>

<p>So if we render the following snippet, which would otherwise have expressions
split in half without the wrapped curly braces:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex">We denote the historical trajectory as 
<span class="p">${</span><span class="nb"> </span><span class="nv">\tau</span><span class="o">=</span><span class="nv">\left</span><span class="o">(</span><span class="nb">s</span><span class="p">_</span><span class="m">1</span><span class="nb">, a</span><span class="p">_</span><span class="m">1</span><span class="nb">, </span><span class="nv">\ldots</span><span class="nb">, a</span><span class="p">_{</span><span class="nb">t</span><span class="o">-</span><span class="m">1</span><span class="p">}</span><span class="nb">, s</span><span class="p">_</span><span class="nb">t</span><span class="nv">\right</span><span class="o">)</span><span class="nb"> </span><span class="p">}$</span>
and action-observation history <span class="p">$</span><span class="o">(</span><span class="nv">\mathrm</span><span class="p">{</span><span class="nb">AOH</span><span class="p">}</span><span class="o">)</span><span class="p">$</span> for
player <span class="p">$</span><span class="nb">i</span><span class="p">$</span> as 
<span class="p">${</span><span class="nb"> </span><span class="nv">\tau</span><span class="p">^</span><span class="nb">i</span><span class="o">=</span><span class="nv">\left</span><span class="o">(</span><span class="nv">\Omega</span><span class="p">^</span><span class="nb">i</span><span class="nv">\left</span><span class="o">(</span><span class="nb">s</span><span class="p">_</span><span class="m">1</span><span class="nv">\right</span><span class="o">)</span><span class="nb">, a</span><span class="p">_</span><span class="m">1</span><span class="nb">, </span><span class="nv">\ldots</span><span class="nb">, a</span><span class="p">_{</span><span class="nb">t</span><span class="o">-</span><span class="m">1</span><span class="p">}</span><span class="nb">, </span><span class="nv">\Omega</span><span class="p">^</span><span class="nb">i</span><span class="nv">\left</span><span class="o">(</span><span class="nb">s</span><span class="p">_</span><span class="nb">t</span><span class="nv">\right</span><span class="o">)</span><span class="nv">\right</span><span class="o">)</span><span class="nb"> </span><span class="p">}$</span>,
 which encodes the trajectory from player <span class="p">$</span><span class="nb">i</span><span class="p">$</span> 's point of view.</code></pre></figure>

<p>we get the following positive result where there is additional whitespace between
the justified text on the first line, to compensate for the expression assigning \(\tau\)
to stay on the same line:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/nbsp-positive.avif" class="z-depth-1 center" width="300px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Expressions that brace together stays together
    </figcaption>
</figure>

<h3 id="non-breaking-space-with-">Non-breaking space with <code class="language-plaintext highlighter-rouge">~</code></h3>
<p>When referencing figures and equations, you want the text and number (i.e Figure 10) to end up on the same line.
This is a negative example, where the region underlined in red shows how it was split up:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/figure-truncated.avif" class="z-depth-1 center" width="500px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      The phrase "Figure 2" was truncated in half
    </figcaption>
</figure>

<p>To remedy this, add a <code class="language-plaintext highlighter-rouge">~</code> after <code class="language-plaintext highlighter-rouge">Figure</code>, which LaTeX interprets as a non-breaking space:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex">We evaluated the policy periodically during training by testing it without exploration noise.
Figure~<span class="k">\ref</span><span class="p">{</span>fig:env-perf<span class="p">}</span> shows the performance curve for a selection of environments. </code></pre></figure>

<p>This would ensure that “Figure 2” always appears together.</p>

<h3 id="expressions-should-be-punctuated-like-sentences">Expressions Should Be Punctuated Like Sentences</h3>
<p>Your document is meant to be read, and it should follow the rules and structures
of English (or whichever language you are writing in). This means that
mathematical expressions should also be punctuated appropriately, which
allows it to flow more naturally and make it easier for the reader to follow.</p>

<p>Consider the following example that does not use punctuation:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/sentence-negative.avif" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Expressions which are not punctuated are tiring to read
    </figcaption>
</figure>

<p>In the region highlighted in red, the expressions do not carry any punctuation at all,
and by the end of the last equation (Equation 15), I am almost out of breath trying
to process all of the information. In addition, it does not end in a full stop, which
does not give me an affordance to take a break mentally until the next paragraph.</p>

<p>Instead, commas should be added after each expression where the expression does not terminate,
and the final equation should be ended by a full stop. Here is a good example of punctuation
that helps to guide the reader along the author’s train of thought:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/sentence-multiline.avif" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Appropriate use of commas and full stop to guide the reader
    </figcaption>
</figure>

<p>Here is another good example of how using commas for the equations 
allow the text to flow naturally, where it takes the form of
“analogously, observe that we have [foo] and [bar], where the inequality…”:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/sentence-two-exp.avif" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Punctuation allows the content to flow naturally
    </figcaption>
</figure>

<p>This even extends to when you pack several equations on a single line, which
is common when you are trying to fit the page limit for conference submissions:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/sentence-singleline.avif" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Appropriate use of punctuation when multiple equations are on a single line
    </figcaption>
</figure>

<h3 id="the-proof-environment">The <code class="language-plaintext highlighter-rouge">proof</code> environment</h3>
<p>The <code class="language-plaintext highlighter-rouge">proof</code> environment from the <code class="language-plaintext highlighter-rouge">amsthm</code> package is great for signposting to your readers
where a proof starts and ends. For instance, consider how it is used in the following example:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="k">\textit</span><span class="p">{</span>Problem: Show that if <span class="p">$</span><span class="o">(</span><span class="nb">x</span><span class="p">_</span><span class="nb">n</span><span class="o">)</span><span class="p">_</span><span class="nb">n</span><span class="p">$</span> converges to <span class="p">$</span><span class="nb">x</span><span class="p">$</span> in the usual sense, then
<span class="p">$</span><span class="nv">\lim</span><span class="p">_{</span><span class="nb">n </span><span class="nv">\to</span><span class="nb"> </span><span class="nv">\infty</span><span class="p">}</span><span class="nb"> x</span><span class="p">_</span><span class="nb">n </span><span class="o">=</span><span class="nb"> </span><span class="nv">\lim</span><span class="p">_{</span><span class="nv">\mathcal</span><span class="p">{</span><span class="nb">F</span><span class="p">}}</span><span class="nb"> x</span><span class="p">_</span><span class="nb">n</span><span class="p">$</span>.<span class="p">}</span>

Suppose that <span class="p">$</span><span class="o">(</span><span class="nb">x</span><span class="p">_</span><span class="nb">n</span><span class="o">)</span><span class="p">_</span><span class="nb">n</span><span class="p">$</span> converges to <span class="p">$</span><span class="nb">x</span><span class="p">$</span>. We show that this <span class="p">$</span><span class="nb">x</span><span class="p">$</span> is also the
<span class="p">$</span><span class="nv">\mathcal</span><span class="p">{</span><span class="nb">F</span><span class="p">}$</span>-limit of <span class="p">$</span><span class="o">(</span><span class="nb">x</span><span class="p">_</span><span class="nb">n</span><span class="o">)</span><span class="p">_</span><span class="nb">n</span><span class="p">$</span>.

<span class="nt">\begin{proof}</span>
    Take any <span class="p">$</span><span class="nv">\varepsilon</span><span class="p">$</span>. Then we know that for some large enough <span class="p">$</span><span class="nb">N</span><span class="p">$</span>, if <span class="p">$</span><span class="nb">n </span><span class="nv">\geq</span><span class="nb"> N</span><span class="p">$</span>, then
    <span class="p">$</span><span class="nb">x</span><span class="p">_</span><span class="nb">n </span><span class="nv">\in</span><span class="nb"> B</span><span class="p">_</span><span class="nv">\varepsilon</span><span class="o">(</span><span class="nb">x</span><span class="o">)</span><span class="p">$</span>. Since every non-principal ultrafilter on <span class="p">$</span><span class="nv">\N</span><span class="p">$</span> contains
    <span class="p">$</span><span class="nv">\mathcal</span><span class="p">{</span><span class="nb">F</span><span class="p">}_</span><span class="nv">\infty</span><span class="p">$</span>, then <span class="p">$</span><span class="nv">\mathcal</span><span class="p">{</span><span class="nb">F</span><span class="p">}$</span> also contains <span class="p">$</span><span class="nb"> </span><span class="nv">\left\{</span><span class="nb"> n : n </span><span class="nv">\geq</span><span class="nb"> N </span><span class="nv">\right\}</span><span class="nb"> </span><span class="p">$</span>,
    since the complement is finite. Therefore since filters are closed upwards, any
    sequence items <span class="p">$</span><span class="nb">x</span><span class="p">_</span><span class="nb">n</span><span class="p">$</span> with <span class="p">$</span><span class="nb">n &lt; N</span><span class="p">$</span> that happen to fall in the ball around <span class="p">$</span><span class="nb">x</span><span class="p">$</span>,
    i.e, <span class="p">$</span><span class="nb">x</span><span class="p">_</span><span class="nb">n </span><span class="nv">\in</span><span class="nb"> B</span><span class="p">_</span><span class="nv">\varepsilon</span><span class="o">(</span><span class="nb">x</span><span class="o">)</span><span class="p">$</span>
    is also contained in some filter element, so 
    <span class="p">$</span><span class="nv">\left\{</span><span class="nb">  n </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\N</span><span class="nb"> : </span><span class="nv">\lvert</span><span class="nb"> x</span><span class="p">_</span><span class="nb">n </span><span class="o">-</span><span class="nb"> x </span><span class="nv">\rvert</span><span class="nb"> &lt; </span><span class="nv">\varepsilon</span><span class="nb"> </span><span class="nv">\right\}</span><span class="nb"> </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\mathcal</span><span class="p">{</span><span class="nb">F</span><span class="p">}$</span>,
    as desired.
<span class="nt">\end{proof}</span></code></pre></figure>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/proof.avif" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Signposting using the `proof` environment
    </figcaption>
</figure>

<p>This will helpfully highlight the start of your argument with <em>“Proof”</em>, and
terminate it with a square that symbolizes QED.</p>

<h3 id="terminate-proofs-with-explicit-qedhere">Terminate Proofs with Explicit <code class="language-plaintext highlighter-rouge">\qedhere</code></h3>

<p>Consider the same example as previously, but now you accidentally added an additional
newline before the closing <code class="language-plaintext highlighter-rouge">\end{proof}</code>, which happens pretty often:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="c">% Same as previously, contents elided for brevity</span>
<span class="nt">\begin{proof}</span>
    <span class="c">% Same as previously, contents elided for brevity</span>
    <span class="p">$</span><span class="nb">x</span><span class="p">_</span><span class="nb">n </span><span class="nv">\in</span><span class="nb"> B</span><span class="p">_</span><span class="nv">\varepsilon</span><span class="o">(</span><span class="nb">x</span><span class="o">)</span><span class="p">$</span>
    is also contained in some filter element, so 
    <span class="p">$</span><span class="nv">\left\{</span><span class="nb">  n </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\N</span><span class="nb"> : </span><span class="nv">\lvert</span><span class="nb"> x</span><span class="p">_</span><span class="nb">n </span><span class="o">-</span><span class="nb"> x </span><span class="nv">\rvert</span><span class="nb"> &lt; </span><span class="nv">\varepsilon</span><span class="nb"> </span><span class="nv">\right\}</span><span class="nb"> </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\mathcal</span><span class="p">{</span><span class="nb">F</span><span class="p">}$</span>,
    as desired.

    <span class="c">% Extra newline here!</span>
<span class="nt">\end{proof}</span></code></pre></figure>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/qedhere.avif" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Misaligned QED symbol
    </figcaption>
</figure>

<p>This results in the above scenario, where the QED symbol now appears on the next line by itself,
which throws the entire text off-balance visually. To avoid such things happening,
always include an explicit <code class="language-plaintext highlighter-rouge">\qedhere</code> marker at the end of your proof, which would cause it
to always appear on the line that it appears after:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="c">% Same as previously, contents elided for brevity</span>
<span class="nt">\begin{proof}</span>
    <span class="c">% Same as previously, contents elided for brevity</span>
    <span class="p">$</span><span class="nb">x</span><span class="p">_</span><span class="nb">n </span><span class="nv">\in</span><span class="nb"> B</span><span class="p">_</span><span class="nv">\varepsilon</span><span class="o">(</span><span class="nb">x</span><span class="o">)</span><span class="p">$</span>
    is also contained in some filter element, so 
    <span class="p">$</span><span class="nv">\left\{</span><span class="nb">  n </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\N</span><span class="nb"> : </span><span class="nv">\lvert</span><span class="nb"> x</span><span class="p">_</span><span class="nb">n </span><span class="o">-</span><span class="nb"> x </span><span class="nv">\rvert</span><span class="nb"> &lt; </span><span class="nv">\varepsilon</span><span class="nb"> </span><span class="nv">\right\}</span><span class="nb"> </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\mathcal</span><span class="p">{</span><span class="nb">F</span><span class="p">}$</span>,
    as desired. <span class="k">\qedhere</span> <span class="c">% Always add \qedhere once you are done!</span>

    <span class="c">% Extra newline here!</span>
<span class="nt">\end{proof}</span></code></pre></figure>

<p>We would then get the same result as before originally, when we did not have the extra newline.</p>

<h3 id="spacing">Spacing</h3>
<p>Spacing matters a lot in readability, as it helps to separate logical components.
For instance, the following example fails to add spacing before the differential
of the variable \(dz\):</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/integral.avif" class="z-depth-1 center" width="500px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Lack of spacing before "dz"
    </figcaption>
</figure>

<p>This might seem innocuous, but consider the following example that makes the issue more explicit:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex">P(X) = <span class="k">\int</span> xyz dx</code></pre></figure>

\[P(X) = \int xyz dx\]

<p>Now we can really see that the quantities are running into each other, and it
becomes hard to interpret. Instead, we can add math-mode spacing, summarized in
the following table:</p>

<table class="table table-bordered table-sm">
  <thead>
    <tr>
      <th>Spacing Expression</th>
      <th>Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\;</code></td>
      <td>Thick space</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\:</code></td>
      <td>Medium space</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\,</code></td>
      <td>Thin space</td>
    </tr>
  </tbody>
</table>

<p>So our new expression now looks like:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex">P(X) = <span class="k">\int</span> xyz <span class="k">\,</span> dx</code></pre></figure>

\[P(X) = \int xyz \, dx\]

<p>which is much more readable.</p>

<h3 id="align-environment-for-multiline-equations"><code class="language-plaintext highlighter-rouge">align*</code> Environment for Multiline Equations</h3>
<p>When using the <code class="language-plaintext highlighter-rouge">align*</code> environment, make sure that your ampersands <code class="language-plaintext highlighter-rouge">&amp;</code> appear before the
symbol that you are aligning against. This ensures that you get the correct spacing.</p>

<p>For instance, the following is wrong, where the <code class="language-plaintext highlighter-rouge">&amp;</code> appears after the <code class="language-plaintext highlighter-rouge">=</code>:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="nt">\begin{align*}</span>
    <span class="k">\nabla</span><span class="p">_{</span><span class="k">\mu</span><span class="p">}</span> (<span class="k">\mathbb</span><span class="p">{</span>E<span class="p">}_{</span>x<span class="k">\sim</span> q<span class="p">_{</span><span class="k">\mu</span><span class="p">}}</span> f(x))  = <span class="p">&amp;</span> <span class="k">\nabla</span><span class="p">_{</span><span class="k">\mu</span><span class="p">}</span> <span class="k">\int</span><span class="p">_</span>x f(x) q<span class="p">_{</span><span class="k">\mu</span><span class="p">}</span>(x) dx                                     <span class="k">\\</span>
                                                    = <span class="p">&amp;</span> <span class="k">\int</span><span class="p">_</span>x f(x) (<span class="k">\nabla</span><span class="p">_{</span><span class="k">\mu</span><span class="p">}</span> <span class="k">\log</span> q<span class="p">_{</span><span class="k">\mu</span><span class="p">}</span>(x))  q<span class="p">_{</span><span class="k">\mu</span><span class="p">}</span>(x) dx                  <span class="k">\\</span>
                                                    = <span class="p">&amp;</span> <span class="k">\mathbb</span><span class="p">{</span>E<span class="p">}_{</span>x <span class="k">\sim</span> q<span class="p">_{</span><span class="k">\mu</span><span class="p">}}</span> <span class="k">\left</span>(f(x) <span class="k">\nabla</span><span class="p">_{</span><span class="k">\mu</span><span class="p">}</span> <span class="k">\log</span> q<span class="p">_{</span><span class="k">\mu</span><span class="p">}</span>(x)<span class="k">\right</span>)
<span class="nt">\end{align*}</span></code></pre></figure>

\[\begin{align*}
    \nabla_{\mu} (\mathbb{E}_{x\sim q_{\mu}} f(x))  = &amp; \nabla_{\mu} \int_x f(x) q_{\mu}(x) dx                                     \\
                                                    = &amp; \int_x f(x) (\nabla_{\mu} \log q_{\mu}(x))  q_{\mu}(x) dx                  \\
                                                    = &amp; \mathbb{E}_{x \sim q_{\mu}} \left(f(x) \nabla_{\mu} \log q_{\mu}(x)\right)
\end{align*}\]

<p>This is because there is too little spacing after the <code class="language-plaintext highlighter-rouge">=</code> sign on each line, which feels very cramped.
Putting the <code class="language-plaintext highlighter-rouge">&amp;</code> before the <code class="language-plaintext highlighter-rouge">=</code> is correct:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="nt">\begin{align*}</span>
    <span class="k">\nabla</span><span class="p">_{</span><span class="k">\mu</span><span class="p">}</span> (<span class="k">\mathbb</span><span class="p">{</span>E<span class="p">}_{</span>x<span class="k">\sim</span> q<span class="p">_{</span><span class="k">\mu</span><span class="p">}}</span> f(x)) <span class="p">&amp;</span> = <span class="k">\nabla</span><span class="p">_{</span><span class="k">\mu</span><span class="p">}</span> <span class="k">\int</span><span class="p">_</span>x f(x) q<span class="p">_{</span><span class="k">\mu</span><span class="p">}</span>(x) dx                                     <span class="k">\\</span>
                                                   <span class="p">&amp;</span> =  <span class="k">\int</span><span class="p">_</span>x f(x) (<span class="k">\nabla</span><span class="p">_{</span><span class="k">\mu</span><span class="p">}</span> <span class="k">\log</span> q<span class="p">_{</span><span class="k">\mu</span><span class="p">}</span>(x))  q<span class="p">_{</span><span class="k">\mu</span><span class="p">}</span>(x) dx                 <span class="k">\\</span>
                                                   <span class="p">&amp;</span> = <span class="k">\mathbb</span><span class="p">{</span>E<span class="p">}_{</span>x <span class="k">\sim</span> q<span class="p">_{</span><span class="k">\mu</span><span class="p">}}</span> <span class="k">\left</span>(f(x) <span class="k">\nabla</span><span class="p">_{</span><span class="k">\mu</span><span class="p">}</span> <span class="k">\log</span> q<span class="p">_{</span><span class="k">\mu</span><span class="p">}</span>(x)<span class="k">\right</span>)
<span class="nt">\end{align*}</span></code></pre></figure>

\[\begin{align*}
    \nabla_{\mu} (\mathbb{E}_{x\sim q_{\mu}} f(x)) &amp; = \nabla_{\mu} \int_x f(x) q_{\mu}(x) dx                                     \\
                                                   &amp; =  \int_x f(x) (\nabla_{\mu} \log q_{\mu}(x))  q_{\mu}(x) dx                 \\
                                                   &amp; = \mathbb{E}_{x \sim q_{\mu}} \left(f(x) \nabla_{\mu} \log q_{\mu}(x)\right)
\end{align*}\]

<p>The spacing is much more comfortable now.</p>

<h2 id="command-mistakes">Command Mistakes</h2>
<p>We now look at some mistakes that arise from using the wrong commands.</p>

<h3 id="math-operators">Math Operators</h3>
<p>Instead of <code class="language-plaintext highlighter-rouge">sin (x)</code> \((sin(x))\) or <code class="language-plaintext highlighter-rouge">log (x)</code> \((log (x))\), use <code class="language-plaintext highlighter-rouge">\sin (x)</code> \((\sin (x))\)
and <code class="language-plaintext highlighter-rouge">\log (x)</code> \((\log (x))\). The idea extends to many other common math functions.
These are math operators that will de-italicize the commands
and also take care of the appropriate math-mode spacing between characters:</p>

<table class="table table-sm">
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">O(n log n)</code></td>
      <td>\(O(n log n)\)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">O(n \log n)</code></td>
      <td>\(O(n \log n)\)</td>
    </tr>
  </tbody>
</table>

<p>Many times there is a math operator that you need to use repeatedly, but which does
not come out of the box. You can define custom math operators with
the <code class="language-plaintext highlighter-rouge">\DeclareMathOperator</code> command. For instance, here are some commonly used in probability:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="k">\DeclareMathOperator*</span><span class="p">{</span><span class="k">\Pr</span><span class="p">}{</span><span class="k">\mathbf</span><span class="p">{</span>Pr<span class="p">}}</span>
<span class="k">\DeclareMathOperator*</span><span class="p">{</span><span class="k">\E</span><span class="p">}{</span><span class="k">\mathbf</span><span class="p">{</span>E<span class="p">}}</span>
<span class="k">\DeclareMathOperator*</span><span class="p">{</span><span class="k">\Ex</span><span class="p">}{</span><span class="k">\mathbf</span><span class="p">{</span>E<span class="p">}}</span>
<span class="k">\DeclareMathOperator*</span><span class="p">{</span><span class="k">\Var</span><span class="p">}{</span><span class="k">\mathbf</span><span class="p">{</span>Var<span class="p">}}</span>
<span class="k">\DeclareMathOperator*</span><span class="p">{</span><span class="k">\Cov</span><span class="p">}{</span><span class="k">\mathbf</span><span class="p">{</span>Cov<span class="p">}}</span>
<span class="k">\DeclareMathOperator*</span><span class="p">{</span><span class="k">\stddev</span><span class="p">}{</span><span class="k">\mathbf</span><span class="p">{</span>stddev<span class="p">}}</span></code></pre></figure>

<p>Then you can use it as follows:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="k">\Pr</span> <span class="k">\left</span><span class="na">[ X \geq a \right]</span> <span class="k">\leq</span> <span class="k">\frac</span><span class="p">{</span><span class="k">\Ex</span><span class="na">[X]</span><span class="p">}{</span>a<span class="p">}</span></code></pre></figure>

\[\Pr \left[ X \geq a \right] \leq \frac{\Ex[X]}{a}\]

<h3 id="double-quotes">Double quotes</h3>
<p>This is more of a rookie mistake since it’s visually very obvious something is
wrong. Double quotes don’t work the way you would expect:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="k">\text</span><span class="p">{</span>"Hello World!"<span class="p">}</span></code></pre></figure>

\[\text{"Hello World!"}\]

<p>Instead, surround them in double backticks and single quotes, which is 
supposed to be reminiscent of the directional strokes of an actual double quote.
This allows it to know which side to orient the ticks:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex"><span class="k">\text</span><span class="p">{</span>``Hello World!''<span class="p">}</span></code></pre></figure>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/quotes.avif" class="z-depth-1 center" width="100px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Output of correct usage of quotes
    </figcaption>
</figure>

<p>Unfortunately I had to demonstrate this with a screenshot since MathJax only
performs math-mode typesetting, but this is an instance of text-mode typesetting.</p>

<h3 id="epsilons">Epsilons</h3>
<p>This is a common mistake due to laziness. Many times, people use <code class="language-plaintext highlighter-rouge">\epsilon</code> (\(\epsilon\))
when they really meant to write <code class="language-plaintext highlighter-rouge">\varepsilon</code> (\(\varepsilon\)). For instance, in analysis
this is usually the case, and therefore writing <code class="language-plaintext highlighter-rouge">\epsilon</code> results in a very uncomfortable
read:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/epsilon-wrong.avif" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Using "\epsilon" looks weird
    </figcaption>
</figure>

<p>Using <code class="language-plaintext highlighter-rouge">\varepsilon</code> makes the reader feel much more at peace:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/epsilon-right.avif" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      "\varepsilon" is usually what should be used
    </figcaption>
</figure>

<p>Similarly, people tend to get lazy and mix up <code class="language-plaintext highlighter-rouge">\phi, \Phi, \varphi</code> (\(\phi, \Phi, \varphi\)),
since they are “about the same”.  Details matter!</p>

<h3 id="sets-mathbbm-instead-of-mathbb">Sets: <code class="language-plaintext highlighter-rouge">mathbbm</code> Instead Of <code class="language-plaintext highlighter-rouge">mathbb</code></h3>
<p>For sets like \(\mathbb{N}\), you should use <code class="language-plaintext highlighter-rouge">\mathbbm{N}</code>
(from <code class="language-plaintext highlighter-rouge">bbm</code> package) instead of <code class="language-plaintext highlighter-rouge">mathbb{N}</code> (from <code class="language-plaintext highlighter-rouge">amssymb</code>). See the
difference in how the rendering of the set of natural numbers
\(\mathbb{N}\) differs, using the same example as the previous section:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/mathbbm.avif" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Use `mathbbm` instead of `mathbb`
    </figcaption>
</figure>

<p><code class="language-plaintext highlighter-rouge">mathbbm</code> causes the symbols to be bolded, which is what you want.</p>

<h3 id="dots">Dots</h3>
<p><code class="language-plaintext highlighter-rouge">...</code>  and <code class="language-plaintext highlighter-rouge">\dots</code> are different. See the difference:</p>

<figure class="highlight"><pre><code class="language-latex" data-lang="latex">X = <span class="k">\left</span>( X<span class="p">_</span>1, ..., X<span class="p">_</span>n <span class="k">\right</span>)
X = <span class="k">\left</span>( X<span class="p">_</span>1, <span class="k">\dots</span>, X<span class="p">_</span>n <span class="k">\right</span>)</code></pre></figure>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/dots.avif" class="z-depth-1 center" width="300px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Output of "..." versus "\dots"
    </figcaption>
</figure>

<p>When using “…”, the spacing between each dot, and between the final dot
and the comma character is wrong. Always use “\dots”.</p>

<h3 id="summation-and-product">Summation and Product</h3>
<p>When writing summation or products of terms, use <code class="language-plaintext highlighter-rouge">\sum</code> and <code class="language-plaintext highlighter-rouge">\prod</code> instead of <code class="language-plaintext highlighter-rouge">\Sigma</code> and <code class="language-plaintext highlighter-rouge">\Pi</code>. This helps to handle the
relative positioning of the limits properly, and is much more idiomatic to read
from the raw script:</p>

<table class="table table-bordered table-sm">
  <thead>
    <tr>
      <th>Raw LaTeX</th>
      <th>Rendered</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\Sigma_{i=1}^n X_i</code></td>
      <td>\(\Sigma_{i=1}^n X_i\)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\sum_{i=1}^n X_i</code></td>
      <td>\(\sum_{i=1}^n X_i\)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\Pi_{i=1}^n X_i</code></td>
      <td>\(\Pi_{i=1}^n X_i\)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\prod_{i=1}^n X_i</code></td>
      <td>\(\prod_{i=1}^n X_i\)</td>
    </tr>
  </tbody>
</table>

<h3 id="multiplication">Multiplication</h3>
<p>To denote multiplication, use <code class="language-plaintext highlighter-rouge">\cdot</code> or <code class="language-plaintext highlighter-rouge">times</code> instead of <code class="language-plaintext highlighter-rouge">*</code>. See the difference below
in the equation:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/latex-mistakes/multiplication.avif" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Use "\cdot" looks much better than "*"
    </figcaption>
</figure>

<h3 id="mid">Mid</h3>
<p>For set builder notation or conditional probability, use <code class="language-plaintext highlighter-rouge">\mid</code> instead of the pipe <code class="language-plaintext highlighter-rouge">|</code>.
This helps to handle the spacing between the terms properly:</p>

<table class="table table-bordered table-sm">
  <thead>
    <tr>
      <th>Raw LaTeX</th>
      <th>Rendered</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">p(\mathbf{z}, \mathbf{x}) = p(\mathbf{z}) p(\mathbf{z} | \mathbf{z})</code></td>
      <td>\(p(\mathbf{z}, \mathbf{x}) = p(\mathbf{z}) p(\mathbf{z} | \mathbf{z})\)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">p(\mathbf{z}, \mathbf{x}) = p(\mathbf{z}) p(\mathbf{z} \mid \mathbf{z})</code></td>
      <td>\(p(\mathbf{z}, \mathbf{x}) = p(\mathbf{z}) p(\mathbf{z} \mid \mathbf{z})\)</td>
    </tr>
  </tbody>
</table>

<h3 id="angle-brackets">Angle Brackets</h3>
<p>When writing vectors, use the <code class="language-plaintext highlighter-rouge">\langle</code> and <code class="language-plaintext highlighter-rouge">\rangle</code> instead of the keyboard angle brackets:</p>

<table class="table table-bordered table-sm">
  <thead>
    <tr>
      <th>Raw LaTeX</th>
      <th>Rendered</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">&lt;u, v&gt;</code></td>
      <td>\(&lt;u, v&gt;\)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">\langle u, v \rangle</code></td>
      <td>\(\langle u, v \rangle\)</td>
    </tr>
  </tbody>
</table>

<h3 id="labels">Labels</h3>
<p>Use <code class="language-plaintext highlighter-rouge">\label</code> to label your figures, equations, tables, and so on, and reference them using <code class="language-plaintext highlighter-rouge">\ref</code>, instead of hardcoding the number. 
For instance, <code class="language-plaintext highlighter-rouge">\label{fig:myfig}</code> and <code class="language-plaintext highlighter-rouge">\ref{fig:myfig}</code>.
Including the type of the object in the tag helps to keep track
of what it is and ensures that you are referencing it correctly, i.e
making sure you write <code class="language-plaintext highlighter-rouge">Figure \ref{fig:myfig}</code> instead of accidentally saying
something like <code class="language-plaintext highlighter-rouge">Table \ref{fig:myfig}</code>.</p>

<h2 id="conclusion">Conclusion</h2>
<p>That was a lot, and I hope it has been a helpful read! 
I will continue updating this post in the future as and when I feel like
there are other important things which should be noted which I missed.</p>

<p>I would like to thank my friend <a href="https://github.com/zack-lee">Zack Lee</a> for reviewing this
article and for providing valuable suggestions.
I would also like to express my thanks to <a href="http://www.cs.cmu.edu/~odonnell/">Ryan
O’Donnell</a>, and my 15-751 A Theorist’s Toolkit
TAs <a href="https://jthsieh.github.io/">Tim Hsieh</a> and <a href="https://www.cs.cmu.edu/~eyolcu/">Emre
Yolcu</a> for helping me realize a lot of the
style-related LaTeX issues mentioned in this post, many of which I made
personally in the past.</p>]]></content><author><name>fanpu</name></author><category term="code" /><category term="general" /><summary type="html"><![CDATA[When was the first time you had to use LaTeX? If you are like most people, it was probably suddenly forced upon you during your first math or CS class where you had to start writing proofs, with minimal guidance on how to get started. Unfortunately, this meant that while many people have good operational knowledge of LaTeX, there are still many small mistakes and best practices which are not followed, which are not corrected by TAs as they are either not severe enough to warrant a note, or perhaps even the TAs themselves are not aware of them. In this post, we cover some common mistakes that are made by LaTeX practitioners (even in heavily cited papers), and how to address them.]]></summary></entry><entry><title type="html">A Concise Proof of the Central Limit Theorem, and Its Actually Useful Version, the Berry-Esseen Theorem</title><link href="https://fanpu.io/blog/2022/central-limit-theorem-and-berry-esseen/" rel="alternate" type="text/html" title="A Concise Proof of the Central Limit Theorem, and Its Actually Useful Version, the Berry-Esseen Theorem" /><published>2022-12-28T00:00:00+00:00</published><updated>2022-12-28T00:00:00+00:00</updated><id>https://fanpu.io/blog/2022/central-limit-theorem-and-berry-esseen</id><content type="html" xml:base="https://fanpu.io/blog/2022/central-limit-theorem-and-berry-esseen/"><![CDATA[<p>The Central Limit Theorem is widely used in statistics and machine learning,
as it allows us to assume that given enough samples, the mean of the samples
will follow a normal distribution. This holds even if the samples come
from a distribution that is not normally distributed.
In this post, we prove the Central Limit Theorem, and then take a look
at the Berry-Esseen Theorem, which actually provides a quantitative bound
on the convergence of the distribution and can therefore be actually used in
deriving theoretical bounds.</p>

<h3 id="the-central-limit-theorem">The Central Limit Theorem</h3>

<p>The Central Limit states that the mean of an appropriately transformed random variable
converges in distribution to a standard normal. We first need to introduce the 
definition of convergence of probability distributions:</p>

<div class="definition">
    <div class="theorem-title">Definition
        
        
            (Convergence in Distribution)
        
    </div>
    <div class="theorem-contents">
        
  Let \( F_{X_n} \) and \( F_{X} \) denotes the cumulative density functions (CDF) of 
  \( X_n \) and \( X \) respectively.

  A sequence \( X_n \) converges to \( X \) in distribution if
  $$ \lim_{n \to \infty } F_{X_n}(t) = F_X (t)$$
  
  for all points \( t \) where \( F_X \) is continuous.
  
    </div>
</div>

<p>Note that the requirement that it only holds for points of continuity is not superfluous, as there
can be distributions that converge but disagree in value at points of discontinuities
(i.e take \(X_n = N(0, 1/n)\) and \(X\) to be the point mass at 0, they converge but their CDF take different values at \(t=0\)).</p>

<p>The Central Limit Theorem can then be stated in the following form (there are many other equivalent statements):</p>

<div class="theorem">
    <div class="theorem-title">Theorem
        
        
            (Central Limit Theorem)
        
    </div>
    <div class="theorem-contents">
        
  Let \( X_1, X_2, \dots, X_n \) be a sequence of independent random variables with mean \( \mu \) and variance \( \sigma^2 \).
  Assume that the moment generating function \( \E \left[ \exp(t X_i) \right] \) is finite for \( t \) in a neighborhood around zero.
  Let \( \overline{X}_n = \frac{1}{n} \sum\limits_{i=1}^n X_i \). Let
  
  $$ Z_n = \frac{\sqrt{n} \left( \overline{X}_n - \mu \right)}{\sigma}. $$
  
  Then \( Z_n \) converges in distribution to \( Z \sim N(0, 1) \).
  
    </div>
</div>

<h3 id="proof-of-the-central-limit-theorem">Proof of the Central Limit Theorem</h3>

<p>There are several ways of proving the Central Limit Theorem. 
The proof that we will explore today relies on the methods of moments.
An alternative measure-theoretic version of the proof relies on Lévy’s
Continuity Theorem, and makes use of convolutions and Fourier transforms.</p>

<p>Our goal is to show that \(Z_n\) converges in distribution to \(Z \sim N(0,
1)\). To do so, we will show that all the moments of \(Z_n\) converges to
the respective moments of \(Z\).</p>

<h4 id="moment-generating-functions">Moment Generating Functions</h4>
<p>The moments of a random variable can be obtained from its moment-generating function (MGF),
defined as follows:</p>

<div class="definition">
    <div class="theorem-title">Definition
        
        
            (Moment Generating Function)
        
    </div>
    <div class="theorem-contents">
        
  The moment generating function of a random variable \( X \) is given by
  
  $$ M_X(t) = \E \left[ e^{tX} \right].$$
  
    </div>
</div>

<p>It is called a moment generating function since the \(k\)th moment of \(X\),
i.e \(\E \left[X^k \right]\), can be obtained by taking the
\(k\)th derivative of its moment-generating function (MGF) at 0:</p>

\[\E \left[X^k \right]  = M^{(k)}(0).\]

<p>This is not too hard to see by induction on the fact that
\(M_X^k(t) = \E \left[ X^k e^{tX} \right]\). The base case is trivial. 
For the inductive case,</p>

\[\begin{align*}
    M_X^{(k)}(t) &amp; = \frac{d^k}{dt^k} \E \left[ e^{tX} \right] \\ 
               &amp; = \frac{d}{dt} \E \left[ X^{k-1} e^{tX} \right] &amp; \text{(by IH)}\\
               &amp; = \frac{d}{dt} \int f(x) x^{k-1} e^{tx} \; dx \\ 
               &amp; = \int \frac{d}{dt} f(x) x^{k-1} e^{tx} \; dx \\
               &amp; = \int f(x) x^{k} e^{tx} \; dx \\
               &amp; = \E \left[ X^{k} e^{tX} \right].
\end{align*}\]

<p>Substituting \(t=0\) gives us the desired result.</p>

<h4 id="normal-distribution-is-determined-by-its-moments">Normal Distribution is Determined by its Moments</h4>
<p>Distributions are determined uniquely by its moments under certain conditions. This is made precise
in the following theorem:</p>

<div class="theorem">
    <div class="theorem-title">Theorem
        
        
            (Sufficient Condition for Distribution to be Determined by Moments)
        
    </div>
    <div class="theorem-contents">
        
  Let \( s_0 &gt; 0 \), and let \( X \) be a random variable with moment generating
  function \( M_X(s) \) which is finite for \( |s| &lt; s_0 \). Then \( f_X \)
  is determined by its moments (and also by \( M_X(s)\)).
  
    </div>
</div>

<p>In words, it means that for some open interval around 0 we have that all moments are finite,
then the moments determine the distribution. This is true for the normal distribution,
where it can be shown that the following recurrence holds for the \(k\)th moment:</p>

\[M^k(t) = \mu M^{k-1}(t) + (k-1) \sigma^2 M^{k-2}(t).\]

<p>This is also not hard to show by induction, and the proof is omitted for brevity. Since the
first two moments of the standard normal distribution are 1 and 0 respectively which are both finite,
and our mean and standard deviation are both finite, then all our moments generated by the
recurrence must also be finite. So our standard normal is determined by its moments.</p>

<h4 id="method-of-moments">Method of Moments</h4>
<p>Now cue the theorem that ties things together:</p>

<div class="theorem">
    <div class="theorem-title">Theorem
        
        
            (Method of Moments)
        
    </div>
    <div class="theorem-contents">
        
  Suppose that \( X \) is determined by its moments. Let \( X_n \) be a sequence of
  distributions, such that \( \int f_{X_n}(x) x^k \; dx \) is finite for all \( n, k \in \N \),
  and such that \( \lim_{n \to \infty} \int f_{X_n}(x) x^k \; dx = \int f_{X}(x) x^k \; dx \)
  for each \( k \in \N \). Then \( X_n \) converges in distribution to \( X \).
  
    </div>
</div>

<p>In words, it states that if the \(k\)th moment of \(X_n\) is finite and converges to the \(k\)th moment
of \(X\) in the limit of \(n\), then \(X_n\) converges to \(X\).</p>

<p>This is great, since now we just have to show that all the moments of 
\(Z_n = \frac{\sqrt{n} \left( \overline{X}_n - \mu \right)}{\sigma}\) converges to
the moments of the standard normal \(Z\).</p>

<h3 id="moment-generating-function-of-z">Moment Generating Function of \(Z\)</h3>
<p>Let’s first find the moment generating function of \(Z\):</p>

\[\begin{align*}
    M_{Z} &amp; = \E \left[ e^{tZ} \right]                                                                                                \\
          &amp; = \int f_Z(x) e^{tx} \; dx                                                                                                \\
          &amp; = \int \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}x^2} e^{tx} \; dx                 &amp; \text{(subst. pdf of standard Gaussian)} \\
          &amp; = \int \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}x^2 + tx} \; dx                                                              \\
          &amp; = \int \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}(x - t)^2 + \frac{1}{2}t^2} \; dx &amp; \text{(completing the square)}           \\
          &amp; = e^{\frac{1}{2}t^2} \int \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}(x - t)^2 } \; dx &amp; \text{($e^{\frac{1}{2}t^2}$ does not depend on $x$)}           \\
          &amp; = e^{\frac{1}{2}t^2} \cdot  1 \\
          &amp; = e^{\frac{1}{2}t^2},
\end{align*}\]

<p>where the second last step comes from the fact that
\(\frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}(x - t)^2 }\) is a probability distribution of a Gaussian with mean \(t\) and variance 1, 
and therefore the integral integrates to 1.</p>

<h3 id="moment-generating-function-of-z_n">Moment Generating Function of \(Z_n\)</h3>

<p>Now we find the moment generating function of \(Z_n\). 
To simplify notation, define
\(A_i = \frac{X_i - \mu}{\sigma}\),
and see that we can write \(Z_n = \frac{1}{\sqrt{n}} \sum\limits_{i=1}^n A_i\), since</p>

\[\begin{align*}
    \frac{1}{\sqrt{n}} \sum\limits_{i=1}^n A_i
    &amp;= \frac{1}{\sqrt{n}} \sum\limits_{i=1}^n \frac{X_i - \mu}{\sigma} \\
    &amp;= \sqrt{n} \sum\limits_{i=1}^n \frac{X_i - \mu}{ n \sigma} \\
    &amp;= \sqrt{n} \frac{\overline{X}_n - \mu}{ \sigma} \\
    &amp;= Z_n.
\end{align*}\]

<p>See that \(\E[A_i] = 0\), and \(\Var(A_i) = 1\).</p>

<p>Then starting from the definition of the moment generating function of \(Z_n\),</p>

\[\begin{align*}
    M_{Z_n}(t) &amp; = \E \left[ e^{t Z_n} \right]                                                                                                   \\
               &amp; = \E \left[ \exp\left(t \frac{1}{\sqrt{n}} \sum\limits_{i=1}^n A_i \right) \right] &amp; \text{(by equivalent definition of $Z_n$)} \\
               &amp; = \prod_{i=1}^n \E \left[ \exp\left( \frac{t}{\sqrt{n}} A_i \right) \right]        &amp; \text{(by independence of $A_i$'s)}        \\
               &amp; = \prod_{i=1}^n M_{A_i}(t/\sqrt{n})                                                &amp; \text{(definition of $M_{A_i}$)}           \\
               &amp; = M_{A_i}(t/\sqrt{n} )^n.
\end{align*}\]

<p>Let’s analyze each individual term \(M_{A_i}(t / \sqrt{n})\) by performing a Taylor expansion around 0.
Recall that the Taylor expansion of a function \(f(x)\) about a point \(a\) is
given by 
\(f(x)= \sum\limits_{n=0}^\infty \frac{f^{(n)(a)}}{n!}(x-a)^n.\). We will expand up to the
second order term, which requires us to find the first three moments of the MGF.</p>

<p>These are:</p>

\[\begin{align*}
    M_{A_i}(0)                 &amp; = \E \left[ e^{t A_i} \right] \Big|_{t=0}                                                                                                                 \\
                               &amp; = \E \left[ 1 \right]                                                                                                                                     \\
                               &amp; = 1,                                                                                                                                                      \\
    M_{A_i}^\prime(0)          &amp; = \E \left[ A_i \right]                                                       &amp; \text{(by the $k$th moment property proved previously)}                   \\
                               &amp; = 0,                                                                                                                                                      \\
    M_{A_i}^{\prime \prime}(0) &amp; = \E \left[ A_i^2 \right]                                                     &amp; \text{(by the $k$th moment property proved previously)}                   \\
                               &amp; = \E \left[ A_i^2 \right] - \E \left[ A_i \right]^2 + \E \left[ A_i \right]^2                                                                             \\
                               &amp; = \Var(A_i) + \E \left[ A_i \right]^2                                         &amp; \text{($\Var(A_i) = \E \left[ A_i^2 \right] - \E \left[ A_i \right]^2 $)} \\
                               &amp; = 1 + 0 \\
                               &amp; = 1.
\end{align*}\]

<p>Taking all terms up to the second order Taylor expansion allows us to approximate \(M_{A_i}\) as</p>

\[\begin{align*}
    M_{A_i}(t/\sqrt{n}) &amp; \approx M_{A_i}(0) + M_{A_i}^\prime(0) + M_{A_i}^{\prime \prime}(0) \frac{t^2}{2n} \\
                        &amp; = 1 + 0 + \frac{t^2}{2n}                                                           \\
                        &amp; = 1 + \frac{t^2}{2n}.
\end{align*}\]

<p>Then now we can write the limit of the MGF of \(Z_n\) as the following:</p>

\[\begin{align*}
    M_{Z_n}(t) &amp; = M_{A_i}(t/\sqrt{n})^n       \\
               &amp; \approx \left( 1 + \frac{t^2}{2n}  \right)^n \\
               &amp; \to e^{t^2/2}, &amp; \text{(by identity $\lim_{n \to \infty} (1 + x/n)^n \to e^x$)}
\end{align*}\]

<p>which shows that it converges to the MGF of \(Z\), as desired. Hooray!</p>

<h3 id="an-uncomfortable-feeling">An Uncomfortable Feeling</h3>
<p>However, there is one thing in this proof that might have bothered you.
Our result came from making use of the Taylor approximation and taking limits, 
but there is no bound on how large \(n\) must be for the distributions to converge
up to a maximum amount of error. This makes it unsuitable for much theoretical analysis,
since usually we would like to know that \(n\) does not have to be too large
for us to obtain a sufficiently good approximation to the standard normal.</p>

<h3 id="the-useful-form-of-the-central-limit-theorem-the-berry-esseen-theorem">The Useful Form of the Central Limit Theorem: The Berry-Esseen Theorem</h3>

<p>The Berry-Esseen theorem solves this limitation by also providing explicit error bounds. 
This was proved independently by Andrew Berry and Carl-Gustav Esseen in the 40s,
and the statement goes as follows:</p>

<div class="theorem">
    <div class="theorem-title">Theorem
        
        
            (Berry-Esseen)
        
    </div>
    <div class="theorem-contents">
        
    Let \( X_1, \dots, X_n \) be independent random variables.

    Assume \( \E [X_i] = 0 \; \forall i \).

    Write \( \sigma_i^2 = \Var [ X_i] = \E[X_i^2] - \E[X_i]^2 = \E[X_i^2] \).

    Assume \( \sum\limits_{i=1}^n \sigma_i^2 = 1 \). 

    Let \( S = \sum\limits_{i=1}^n X_i \). Then \( \forall u \in \R \),
    
    $$
        \lvert \Pr \left[ S \leq u \right] - \Pr \left[ Z \leq u \right] \rvert
        \leq \mbox{const} \cdot \beta,
    $$

    where the exact constant depends on the proof, with the best known constant
    being \(.5600\) proven by Shevtsova in 2010, and 
    \(\beta = \sum\limits_{i=1}^n \E \left[ \lvert X_i \rvert^3 \right]\).

  
    </div>
</div>

<p>In words, the theorem says that the difference between the CDF of the sum of
the mean-0 random variables and the CDF of the standard normal is bounded by a
value proportionate to the third moment. This then becomes useful as a tool in
proving high probability statements if we can show that the third moment is
inversely polynomially small, i.e \(\beta = 1/\poly(n)\).</p>

<p>Another thing to note is that the theorem only provides an absolute bound for all values of \(u\).
Therefore, when \(u\) is very negative and \(\Pr [Z \leq u ] = \Phi(u)\) is very small, the
relative error is actually very large, and therefore is not as helpful.</p>

<p>I hope this article has been helpful!</p>

<p><em>I would like to express my thanks to my friend <a href="https://adbforlife.github.io/">Albert Gao</a>
for reviewing this article and for providing valuable suggestions</em>.</p>

<h3 id="references">References</h3>
<ul>
  <li>Rosenthal, J. S. (2016). A first look at rigorous probability theory. World Scientific.</li>
  <li>Larry Wasserman, CMU 36-705 Intermediate Statistics Lecture Notes. URL: <a href="https://www.stat.cmu.edu/~larry/=stat705/">https://www.stat.cmu.edu/~larry/=stat705/</a></li>
  <li>Ryan O’Donnell, CMU 15-751 A Theorist’s Toolkit. URL: <a href="https://www.youtube.com/watch?v=Ig5TuZauhW4">https://www.youtube.com/watch?v=Ig5TuZauhW4</a></li>
</ul>]]></content><author><name>fanpu</name></author><category term="math" /><category term="machine-learning" /><summary type="html"><![CDATA[The Central Limit Theorem is widely used in statistics and machine learning, as it allows us to assume that given enough samples, the mean of the samples will follow a normal distribution. This holds even if the samples come from a distribution that is not normally distributed. In this post, we prove the Central Limit Theorem, and then take a look at the Berry-Esseen Theorem, which actually provides a quantitative bound on the convergence of the distribution and can therefore be actually used in deriving theoretical bounds.]]></summary></entry><entry><title type="html">Reinforcement Learning Policy Optimization: Deriving the Policy Gradient Update</title><link href="https://fanpu.io/blog/2022/deriving-the-policy-gradient-update/" rel="alternate" type="text/html" title="Reinforcement Learning Policy Optimization: Deriving the Policy Gradient Update" /><published>2022-12-26T00:00:00+00:00</published><updated>2022-12-26T00:00:00+00:00</updated><id>https://fanpu.io/blog/2022/deriving-the-policy-gradient-update</id><content type="html" xml:base="https://fanpu.io/blog/2022/deriving-the-policy-gradient-update/"><![CDATA[<p>Reinforcement learning algorithms that learn a policy (as opposed to implicit policy
methods like \(\epsilon\)-greedy) optimize their policies by
updating their policies in the direction of the gradient. However, 
the precise environment dynamics are not usually known to us, 
and the state space is usually also too large to enumerate, which means that
we still cannot compute the gradient analytically. In this post, we derive
the policy gradient update from scratch, and show how it can be approximated
by sampling sufficiently many trajectories.</p>

<h2 id="optimization-objective">Optimization Objective</h2>
<p>Our goal is to train an agent that is able to maximize its rewards in a given task.
For instance, its goal could be to balance a cartpole for as long as possible, where
for each time step the pole does not fall down, the agent receives 1
reward, and when the pole falls down the episode is terminated and
the agent no longer receives any rewards:</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/cartpole.gif" class="z-depth-1 center" width="400px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">
      Balancing a cartpole, OpenAI Gym
    </figcaption>
</figure>

<p>Formally, we want to maximize the expected rewards for our policy over the trajectories
that it visits. A trajectory \(\tau\) is defined as state-action pairs
\(\tau = (s_0, a_0, s_1, a_1, \dots, s_H, a_H, s_{H+1})\), where \(H\) is horizon of the trajectory,
i.e the duration until the episode is terminated, and \(s_t, a_t\) are the states and actions
performed at each time step \(t\).</p>

<p>This can be formalized as the following objective:</p>

\[\begin{align}
     &amp; \max_\theta \E_{\tau \sim P_\theta(\tau)} [R(\tau)] \\
   = &amp; \max_\theta \sum\limits_\tau P_\theta(\tau) R(\tau) \\
   = &amp; \max_\theta U(\theta),
\end{align}\]

<p>where \(\tau\) refers to a trajectory of state-action pairs, \(P_\theta(\tau)\)
denotes the probability of experiencing trajectory \(\tau\) under policy \(\theta\),
and \(R(\tau)\) is the reward under trajectory \(\tau\),
and \(U(\theta)\) is shorthand for the expression for brevity.</p>

<p>The probability of \(P_\theta(\tau)\) is given by the following:</p>

\[\begin{align}
    P_\theta(\tau) = \prod_{t=0}^H P(s_{t+1} \mid s_t, a_t) \cdot \pi_\theta (a_t \mid s_t),
\end{align}\]

<p>where in words, it is the product over each time step \(t\),
of the probability of taking the action at time \(t\) in the trajectory \(a_t\)
when we were in state \(s_t\) under our policy \(\pi_\theta\), given by \(\pi_\theta(a_t \mid s_t)\),
multiplied by the probability that the environment transitions us from \(s_t\) to
\(s_{t+1}\) given that we performed action \(a_t\). Note that we do not
necessarily know this environment transition probability \(P(s_{t+1} \mid s_t, a_t)\).</p>

<h2 id="deriving-the-gradient-update">Deriving the Gradient Update</h2>

<p>To perform a gradient-based update on \(\theta\) to increase the reward, we 
need to compute the derivative with respect to our policy \(\theta\), i.e
\(\nabla_\theta \E_{\tau \sim P(\tau; \theta)} [R(\tau)]\). 
Let’s walk through the derivation step by step:</p>

\[\begin{align*}
    \nabla_\theta \E_{\tau \sim P_\theta(\tau)} [R(\tau)]
     &amp; = \nabla_\theta \sum\limits_\tau P_\theta(\tau) R(\tau) \\
     &amp; = \sum\limits_\tau \nabla_\theta  P_\theta(\tau) R(\tau) &amp; \text{(uh oh...)}\\
\end{align*}\]

<p>It appears that we are already stuck here, since 
\(\nabla_\theta P_\theta(\tau)\) will result in many
repeated applications of the chain rule since \(P_\theta(\tau)\)
is a huge product containing our policy transition probabilities,
and will quickly get out of hand to be computed feasibly.</p>

<p>Instead, the trick is to multiply by 1 on the left:</p>

\[\begin{align*}
    \sum\limits_\tau \nabla_\theta  P_\theta(\tau) R(\tau)
    &amp;= \sum\limits_\tau \frac{ P_\theta(\tau) }{ P_\theta(\tau) } \nabla_\theta  P_\theta(\tau) R(\tau) &amp; \text{(multiplying by 1)} \\
    &amp;= \sum\limits_\tau P_\theta(\tau) \frac{ \nabla_\theta  P_\theta(\tau)  }{ P_\theta(\tau) } R(\tau) &amp; \text{(rearranging)} \\
    &amp;= \sum\limits_\tau P_\theta(\tau) \nabla_\theta  \log  P_\theta(\tau) R(\tau) &amp; \text{($\frac{d}{dx} \log f(x) = \frac{f'(x)}{f(x)} $)} \\
    &amp;= \E_{\tau \sim P_\theta(\tau)} \left[ \nabla_\theta  \log  P_\theta(\tau) R(\tau)  \right] \\
    &amp;\approx \frac{1}{N} \sum\limits_{i=1}^N \nabla_\theta  \log  P_\theta(\tau_i) R(\tau_i), \\
\end{align*}\]

<p>where we can use \(\frac{1}{N} \sum\limits_{i=1}^N \nabla_\theta  \log  P_\theta(\tau_i) R(\tau_i)\) as our estimator, which converges
to the true expectation as our number of trajectory samples \(N\) increases.</p>

<p>We can compute \(\nabla_\theta  \log  P_\theta(\tau_i)\) for each sampled trajectory \(\tau_i\),
and then take their average. This can be done as follows:</p>

\[\begin{align*}
    \nabla_\theta  \log  P_\theta(\tau_i)
     &amp; = \nabla_\theta  \log  P_\theta(s_0, a_0, \dots, s_H, a_H, s_{H+1}) \\
     &amp; = \nabla_\theta  \log  \left[
        \prod_{t=0}^H P(s_{t+1} \mid s_t, a_t) \cdot \pi_\theta (a_t \mid s_t),
    \right]                          \\
     &amp; = \nabla_\theta  \left[
        \sum\limits_{t=0}^H \log P(s_{t+1} \mid s_t, a_t) + \log \pi_\theta (a_t \mid s_t)
        \right] \\
     &amp; = \nabla_\theta \sum\limits_{t=0}^H \log \pi_\theta (a_t \mid s_t) \\
     &amp; \qquad \qquad \text{(first term does not depend on $\theta$, becomes zero)} \\
     &amp; = \sum\limits_{t=0}^H \nabla_\theta \log \pi_\theta (a_t \mid s_t),\\
\end{align*}\]

<p>where the last expression is easily computable for models such as neural
networks since it is end-to-end differentiable.</p>

<p>With the approximate gradient
\(\nabla_\theta U(\theta)\)
in hand, we can now perform our policy gradient update as</p>

\[\begin{align*}
    \theta_{\mbox{new}} = \theta_{\mbox{old}} + \alpha \nabla_\theta U(\theta),
\end{align*}\]

<p>for some choice of step size \(\alpha\).</p>

<h2 id="takeaways">Takeaways</h2>
<p>In this post, we saw from first principles how taking the gradients
of many sampled trajectories does indeed converge to the true policy gradient.</p>

<p>This method of multiplying by 1 to pull out a probability term so that
a summation can be converted into an expectation is widely used
in machine learning, such as for computing variational autoencoder (VAE) loss.
It is known as the log derivative trick.</p>

<p>The estimator \(\frac{1}{N} \sum\limits_{i=1}^N \nabla_\theta  \log
P_\theta(\tau_i) R(\tau_i)\) is also sometimes known as the REINFORCE
estimator, after the popular REINFORCE algorithm.</p>

<p>One limitation of this approach is that it requires \(\pi_\theta\) to be
differentiable. However, given how most RL models rely on neural networks,
this is not a significant restriction.</p>

<p>Choosing the right step size \(\alpha\) is actually not straightforward.
It is different from the offline supervised-learning context, where
you can use methods like AdaGrad or RMSProp which adaptively
chooses a learning rate for you, and even if the learning rate
was not optimal it just takes more iterations to converge.
On the other hand, in reinforcement learning,
a learning rate that is too small results in inefficient use
of trajectory samples as they cannot be trivially re-used
since it depends on your current policy, and a learning
rate that is too large can result in the policy becoming bad,
which is difficult to recover from since future trajectories
would also become bad.</p>

<p>We will discuss three important methods to
choose an appropriate step size in a future post: Natural Policy Gradients,
Proximal Policy Optimization (PPO), and Trust Region Policy Optimization
(TRPO). Hope to see you around!</p>

<p><em>I would like to express my thanks to my friend <a href="https://jytan.net/about/">Jun Yu Tan</a>
for reviewing this article and for providing valuable suggestions</em>.</p>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://cmudeeprl.github.io/703website_f22/lectures/">Carnegie Mellon University 10-703 Deep Reinforcement Learning and Control Course Slides</a></li>
</ul>]]></content><author><name>fanpu</name></author><category term="machine-learning" /><summary type="html"><![CDATA[Reinforcement learning algorithms that learn a policy (as opposed to implicit policy methods like \(\epsilon\)-greedy) optimize their policies by updating their policies in the direction of the gradient. However, the precise environment dynamics are not usually known to us, and the state space is usually also too large to enumerate, which means that we still cannot compute the gradient analytically. In this post, we derive the policy gradient update from scratch, and show how it can be approximated by sampling sufficiently many trajectories.]]></summary></entry><entry><title type="html">Pseudo-determinism for Graph Streaming Problems</title><link href="https://fanpu.io/blog/2022/pseudo-determinism-for-graph-streaming-problems/" rel="alternate" type="text/html" title="Pseudo-determinism for Graph Streaming Problems" /><published>2022-12-19T00:00:00+00:00</published><updated>2022-12-19T00:00:00+00:00</updated><id>https://fanpu.io/blog/2022/pseudo-determinism-for-graph-streaming-problems</id><content type="html" xml:base="https://fanpu.io/blog/2022/pseudo-determinism-for-graph-streaming-problems/"><![CDATA[<h3 id="summary">Summary</h3>
<p>Given a fixed input for a search problem, pseudo-deterministic algorithms
produce the same answer over multiple independent runs, with high probability.
For example, we can efficiently find a certificate for inequality of
multivariate polynomials pseudo-deterministically, but it is not known how to
do so deterministically. The same notion can be extended to the streaming
model. The problem of finding a nonzero element from a turnstile stream is
previously shown to require linear space for both deterministic and
pseudo-deterministic algorithms. Another model of streaming problems is that
of graphs, where edge insertions and deletions occur along a stream. Some
natural problems include connectivity, bipartiteness, and colorability of a
graph. While the randomized and deterministic graph streaming algorithms
have been mostly well-studied, we investigate pseudo-deterministic space
lower bounds and upper bounds for graph theoretic streaming problems.</p>

<p>Joint work with <a href="https://adbforlife.github.io/">Albert Gao</a>,
<a href="https://www.linkedin.com/in/andrew-caosun-a237ab19b/">Andrew Caosun</a>,
<a href="https://www.linkedin.com/in/puhuacheng/">Puhua Cheng</a>
for the course project of 
<a href="https://www.cs.cmu.edu/~dwoodruf/teaching/15859-fall22/index.html">15-859CC Algorithms for Big Data</a>.</p>

<h3 id="paper">Paper</h3>
<p><a href="/assets/research/Pseudodeterminism_For_Graph_Streaming_Problems.pdf">Link to our paper</a>.</p>]]></content><author><name>fanpu</name></author><category term="theory" /><category term="project" /><summary type="html"><![CDATA[Given a fixed input for a search problem, pseudo-deterministic algorithms produce the same answer over multiple independent runs, with high probability. For example, we can efficiently find a certificate for inequality of multivariate polynomials pseudo-deterministically, but it is not known how to do so deterministically. The same notion can be extended to the streaming model. The problem of finding a nonzero element from a turnstile stream is previously shown to require linear space for both deterministic and pseudo-deterministic algorithms. Another model of streaming problems is that of graphs, where edge insertions and deletions occur along a stream. Some natural problems include connectivity, bipartiteness, and colorability of a graph. While the randomized and deterministic graph streaming algorithms have been mostly well-studied, we investigate pseudo-deterministic space lower bounds and upper bounds for graph theoretic streaming problems.]]></summary></entry><entry><title type="html">Graphical Bayesian Networks with Topic Modeling Priors for Predicting Asset Covariances</title><link href="https://fanpu.io/blog/2022/graphical-bayesian-network-for-predicting-asset-covariances/" rel="alternate" type="text/html" title="Graphical Bayesian Networks with Topic Modeling Priors for Predicting Asset Covariances" /><published>2022-12-13T00:00:00+00:00</published><updated>2022-12-13T00:00:00+00:00</updated><id>https://fanpu.io/blog/2022/graphical-bayesian-network-for-predicting-asset-covariances</id><content type="html" xml:base="https://fanpu.io/blog/2022/graphical-bayesian-network-for-predicting-asset-covariances/"><![CDATA[<h3 id="summary">Summary</h3>

<p>Covariance matrix prediction is a long-standing challenge in modern portfolio
theory and quantitative finance. In this project, we investigate the
effectiveness of Bayesian networks in predicting the covariance matrix of
financial assets (specifically a subset of the S&amp;P 500), evaluated against
Heterogeneous Autoregressive (HAR) models. In particular, we consider both
HAR-DRD, based on the DRD decomposition of the covariance matrix, and Graphical
HAR (GHAR)-DRD, which is also based on DRD decomposition but also makes use of
graphical relationships between the assets. To build the graph representing
relationships between the assets, we apply Latent Dirichlet allocation (LDA) on
the 10-K filings of each of the companies, and infer edges based on topic
overlap. We show that this technique has limited usefulness in our setup, but
provides recommendations on how it could be further improved based on our
observations of its predictions.</p>

<p>Joint work with <a href="https://www.cmu.edu/mscf/student-experience/meet-our-current-students/2023-minghanl.html">Kevin Minghan
Li</a>
for the course project of <a href="https://andrejristeski.github.io/10708-F22/">10-708 Probabilistic Graphical Models</a>.</p>

<h3 id="paper">Paper</h3>

<p><a href="/assets/research/Graphical_Bayesian_Networks_For_Predicting_Asset_Covariances.pdf">Link to our paper</a>.</p>]]></content><author><name>fanpu</name></author><category term="machine-learning" /><category term="project" /><summary type="html"><![CDATA[Covariance matrix prediction is a long-standing challenge in modern portfolio theory and quantitative finance. In this project, we investigate the effectiveness of Bayesian networks in predicting the covariance matrix of financial assets (specifically a subset of the S&P 500), evaluated against Heterogeneous Autoregressive (HAR) models. In particular, we consider both HAR-DRD, based on the DRD decomposition of the covariance matrix, and Graphical HAR (GHAR)-DRD, which is also based on DRD decomposition but also makes use of graphical relationships between the assets. To build the graph representing relationships between the assets, we apply Latent Dirichlet allocation (LDA) on the 10-K filings of each of the companies, and infer edges based on topic overlap.]]></summary></entry><entry><title type="html">Analysis of Symmetry and Conventions in Off-Belief Learning (OBL) in Hanabi</title><link href="https://fanpu.io/blog/2022/symmetry-and-conventions-in-obl-hanabi/" rel="alternate" type="text/html" title="Analysis of Symmetry and Conventions in Off-Belief Learning (OBL) in Hanabi" /><published>2022-12-09T00:00:00+00:00</published><updated>2022-12-09T00:00:00+00:00</updated><id>https://fanpu.io/blog/2022/symmetry-and-conventions-in-obl-hanabi</id><content type="html" xml:base="https://fanpu.io/blog/2022/symmetry-and-conventions-in-obl-hanabi/"><![CDATA[<h3 id="summary">Summary</h3>
<p>We investigate if policies learnt by agents using the Off-Belief Learning (OBL)
algorithm in the multi-player cooperative game Hanabi in the zero-shot
coordination (ZSC) context are invariant across symmetries of the game, and if
any conventions formed during training are arbitrary or natural. We do this by
a convention analysis on the action matrix of what the agent does, introduce a
novel technique called the Intervention Analysis to estimate if the actions
taken by the policies learnt are equivalent between isomorphisms of the same
game state, and finally evaluate if our observed results also hold in a
simplified version of Hanabi which we call Mini-Hanabi.</p>

<p>Joint work with <a href="https://17zhangw.github.io/">William Zhang</a>
for the course project of 
<a href="https://www.cs.cmu.edu/~15784/">15-784 Foundations of Cooperative AI</a></p>

<h3 id="paper">Paper</h3>

<p><a href="/assets/research/Analysis_Of_Symmetry_And_Conventions_In_Off_Belief_Learning_In_Hanabi.pdf">Link to our paper</a>.</p>]]></content><author><name>fanpu</name></author><category term="machine-learning" /><category term="project" /><summary type="html"><![CDATA[Hanabi has been proposed as the new frontier for developing strategies in cooperative AI, currently a very nascent area of AI research. A recent algorithm that has been developed for multi-agent reinforcement learning in a cooperative context is the Off-Belief Learning (OBL) algorithm, which is based on iterated reasoning starting from a base policy. We investigate if policies learnt by agents using the OBL algorithm in the multi-player cooperative game Hanabi in the zero-shot coordination (ZSC) context are invariant across symmetries of the game, and if any conventions formed during training are arbitrary or natural, both of which are desirable properties.]]></summary></entry><entry><title type="html">Improving Domain Adaptation of Transformer Models For Generating Reddit Comments</title><link href="https://fanpu.io/blog/2022/improving-domain-adaptation-of-transformer-models-for-reddit-comments/" rel="alternate" type="text/html" title="Improving Domain Adaptation of Transformer Models For Generating Reddit Comments" /><published>2022-12-05T00:00:00+00:00</published><updated>2022-12-05T00:00:00+00:00</updated><id>https://fanpu.io/blog/2022/improving-domain-adaptation-of-transformer-models-for-reddit-comments</id><content type="html" xml:base="https://fanpu.io/blog/2022/improving-domain-adaptation-of-transformer-models-for-reddit-comments/"><![CDATA[<h3 id="summary">Summary</h3>
<p>We improve upon the recent success of large language models based on the
transformer architecture by investigating and showing several methods that
have empirically improved its performance in domain adaptation. We use a
pre-trained GPT-2 model and perform fine-tuning on 5 different subreddits,
and use different methods of ordering the training data based on our priors
about the input to see how this affects the prediction quality of the trained
model. We propose a new metric for evaluating causal language modeling tasks
called APES (Average Perplexity Evaluation for Sentences) to address the
limitations of existing metrics, and apply them to our results. Our results
are evaluated against both LSTM and GPT-2 baselines.</p>

<p>There have been many exciting breakthroughts in language generation models in
recent years. From the simple n-gram model that has been studied since the
early 20th century, to the introduction of neural language models that utilizes
word embeddings at the turn of the century (2001), in the past few years we saw
the development of powerful language models such as Word2Vec (2013),
Transformer (2017), BERT (2018), GPT (2018), GPT-3 (2020). Such models have
already beaten humans in accuracy in tasks such as reading comprehension, and
have displayed high levels of fluency in language-generation tasks. These
successes can be attributed to the great strides taken in Deep Learning, the
increase in computational resources, and the proliferation of publicly
available datasets and benchmarks.</p>

<p>We aim to replicate and build upon existing work in causal language models by
investigating and answering the following research questions in this paper:</p>

<ol>
  <li>
    <p>Can we improve the performance of domain adaptation of transformer language
models by various methods of ordering the inputs seen at training time,
based on our priors about the input?</p>
  </li>
  <li>
    <p>Can we successfully perform domain adaptation using just commodity hardware
on unstruc- tured internet discourse that can contain new vocabulary, carry
unique speech patterns, and require expert domain knowledge in order to
provide a cogent response to a prompt?</p>
  </li>
  <li>
    <p>Can we successfully perform domain adaptation on large language models with
billions of parameters even with very small datasets?</p>
  </li>
  <li>
    <p>What is a suitable evaluation metric to determine the success of domain
adaptation for text generation, given the limitations of existing metrics
for evaluating causal language models?</p>
  </li>
</ol>

<p>Our paper shows mild results for point 1, and answers points 2 and 3
affirmatively empirically. We propose a new metric to address point 4 which we
call APES (Average Perplexity Evaluation for Sentences), and use it to evaluate
our results. We achieve our results by fine-tuning a pre-trained GPT-2 language
model using data from 5 subreddits with distinct topics and talking styles, and
use various methods of ordering the input data to improve the performance of
fine-tuning as measured by the Bilingual Evaluation Understudy (BLEU) score and
a qualitative evaluation of the results given a prompt from different
subreddits. Finally, we use the APES metric to manually score 200 results from
the test set from each dataset from 1 to 5. Our results are evaluated against a
LSTM baseline and a GPT-2 model without fine-tuning.</p>

<p>Joint work with Joseph Salmento
for the course project of 
<a href="https://rsalakhucmu.github.io/10417-22/">10-617 Intermediate Deep Learning</a>.</p>

<h3 id="paper">Paper</h3>

<p><a href="/assets/research/Improving_Domain_Adaptation_Of_Transformer_Models_For_Generating_Reddit_Comments.pdf">Link to our paper</a>.</p>]]></content><author><name>fanpu</name></author><category term="machine-learning" /><category term="project" /><summary type="html"><![CDATA[We improve upon the recent success of large language models based on the transformer architecture by investigating and showing several methods that have empirically improved its performance in domain adaptation. We use a pre-trained GPT-2 model and perform fine-tuning on 5 different subreddits, and use different methods of ordering the training data based on our priors about the input to see how this affects the prediction quality of the trained model. We propose a new metric for evaluating causal language modeling tasks called APES (Average Perplexity Evaluation for Sentences) to address the limitations of existing metrics, and apply them to our results. Our results are evaluated against both LSTM and GPT-2 baselines.]]></summary></entry><entry><title type="html">Efficient Low Rank Approximation via Affine Embeddings</title><link href="https://fanpu.io/blog/2022/affine-embeddings-for-low-rank-approximation/" rel="alternate" type="text/html" title="Efficient Low Rank Approximation via Affine Embeddings" /><published>2022-09-29T00:00:00+00:00</published><updated>2022-09-29T00:00:00+00:00</updated><id>https://fanpu.io/blog/2022/affine-embeddings-for-low-rank-approximation</id><content type="html" xml:base="https://fanpu.io/blog/2022/affine-embeddings-for-low-rank-approximation/"><![CDATA[<h3 id="affine-embeddings">Affine Embeddings</h3>
<p>Suppose we have a \(n \times d\) matrix \(A\) that is tall and thin, and a \(n \times m\) matrix \(B\) that can have a very large number of columns. The goal is to solve</p>

\[\begin{equation}
    \min_X | AX - B|_F^2.
\end{equation}\]

<p>An affine embedding is a matrix \(S\) such that</p>

\[\begin{equation} \label{eq:affine_desired}
    \|S(A X-B)\|_{\mathrm{F}}=(1 \pm \varepsilon)\|A X-B\|_{\mathrm{F}}
\end{equation}\]

<p>holds with high probability. CountSketch matrices are one class
of matrices that can be used for affine embeddings.
CountSketch matrices are \(k \times n\) matrices with \(k = O(d^2/\varepsilon^2)\),
and where one entry is taken per column at random and set to \(\pm 1\) with
equal probability, with all other entries zero.</p>

<p>For the rest of this post, we will see how to apply affine embeddings to help solve low-rank approximations.</p>

<h3 id="low-rank-approximation">Low-Rank Approximation</h3>
<p>Low-rank approximation is also referred to as Principal Component Analysis
(PCA).  It is one of the most popular ways of performing linear dimensionality
reduction, with other more complicated ways being via neural networks.</p>

<h3 id="motivation">Motivation</h3>
<p>Suppose you have a \(n \times d\) matrix \(A\), where both dimensions are large.
This could represent something like a customer-product matrix used in online
recommender systems, where each cell \(A_{i,j}\) denotes how many times customer \(i\)
purchased item \(j\). Then it is typically the case that \(A\) can be
well-approximated by a low-rank matrix. For instance, using the previous
example, there might only be a few dominant patterns that describes purchasing
behavior in \(A\), and the rest of it is just noise.</p>

<p>Therefore, if we can find such a low-rank approximation, we can achieve
significant space savings, and can also help to make the data more
interpretable.</p>

<p>We can thus formulate our problem as finding a rank-\(k\) matrix \(A_k\) such that</p>

\[\begin{equation}\label{eq:best-rank-k}
    \argmin_{\text {rank-$k$ matrices $B$} }\|A - B\|_{\mathrm{F}}
\end{equation}\]

<h3 id="exact-solution-truncated-singular-value-decomposition">Exact Solution: Truncated Singular Value Decomposition</h3>
<p>A natural solution that comes to mind is to use truncated singular value
decomposition (SVD), which in fact gives us the best rank-\(k\) approximation for
\(A\) as per Equation \ref{eq:best-rank-k}.  To approximate the \(n \times d\)
matrix \(A\) with a rank-\(k\) matrix, compute the SVD of \(A\) to get \(A=U \Sigma
    V^{\top}\). Define \(\Sigma_{k}\) by zeroing out all the singular values in
\(\Sigma\) below the \(k\)-th row, and define \(A_{k}=U \Sigma_{k} V^{\top}\). Note
that we can also drop all but the top \(k\) rows or \(V^{\top}\) and drop all but
the top \(k\) columns of \(U\) without changing \(A_{k}\).</p>

<p>It turns out that \(A_{k}\) minimizes \(\left\|A_{k}-A\right\|\) among all rank-\(k\) matrices for many norms, including the Frobenius norm and the spectral norm. More generally, \(A_{k}\) is the best rank-\(k\) approximation under any unitarily invariant norm.</p>

<p>We can compute the SVD of \(A\) in \(O\left(\min \left(n d^{2}, n^{2}
d\right)\right)\) time. The \(\min\) comes from the fact that we can transpose
\(A\) if necessary before performing SVD, so that the linear dimension matches
the larger dimension.</p>

<p>However, the problem is that performing SVD is too expensive, since \(n\) and \(d\) are large.</p>

<h3 id="approximate-solution-truncated-singular-value-decomposition">Approximate Solution: Truncated Singular Value Decomposition</h3>
<p>In order to get a better runtime, we relax the problem such that we are satisfied
with an approximate solution instead of an exact solution. Our goal is now to
find a rank \(k\) matrix \(A'\) such that</p>

\[\begin{equation}
    \left\|A^{\prime}-A\right\|_F \leq(1+\varepsilon)\left\|A_{k}-A\right\|_F
\end{equation}\]

<p>with a constant failure probability (say 9/10).</p>

<p>We will now show an algorithm that can do this
\({O(\nnz{A} +(n+d) \operatorname{poly}(k / \varepsilon))}\) time,
due to Woodruff, Clarkson, and Sarlos~\cite{Woodruff-LRA}~\cite{sarlos}. Recall that \(\nnz{A}\) is the number of non-zero entries in \(A\). When the input is dense, this is in approximately \(O(n d)\) time, which is significantly
faster than SVD. When it is sparse, it becomes even faster.</p>

<h3 id="intuition">Intuition</h3>
<p>The key idea is to compute \(SA\), such that the number of rows of \(S\) is \(O(k/\epsilon)\) which is small.</p>

<p>Think of the rows of \(A\) as points in \(\mathbb{R}^d\).
Think of \(SA\) as taking a small number of linear combinations of \(A\),
where each row of \(S\) is picking a random linear combination of all the rows of \(A\), i.e points in \(\mathbb{R}^d\):</p>

<figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/low-rank-approx-intuition-matrix.avif" class="z-depth-0 center" width="200px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>
</figure>

<p>This allows us to think of \(SA\) as a \(\frac{k}{\epsilon} \times d\) matrix where
we performed this operation \(k/\epsilon\) times.  Then each row of \(SA\) is a
linear combination of the rows of \(A\), but now in a subspace of only dimension
\(k/\epsilon\). If we run SVD on these projected points, we can do it in only
\(O \left( n\left( \frac{k}{\epsilon} \right)^2 \right)\) time!</p>

<p>To summarize, the main idea is to:</p>

<ol>
  <li>Project all the rows of \(A\) onto a lower dimensional space \(SA\).</li>
  <li>Find the best rank-\(k\) approximation to the points in \(SA\) via SVD.</li>
</ol>

<h3 id="choice-of-sketching-matrix">Choice of Sketching Matrix</h3>
<p>One might wonder which sketching matrices \(S\) would work for this paradigm.
In fact, all of the following sketching matrices work:</p>

<ol>
  <li>\(S\) as the \(\frac{k}{\epsilon} \times n\) matrix of i.i.d normal random variables. We can compute \(SA\) in time \(O\left( \nnz{A} \frac{k}{\epsilon} \right)\) since \(S\) is dense. However, we could do a lot better.</li>
  <li>\(S\) as the \(\tilde{O}\left(\frac{k}{\epsilon}\right) \times n\) Subsampled Randomized Hadamard Transform (also called the Fast Johnson Lindenstrauss) matrix. \(SA\) can be computed in \(O(nd \log n)\) time.</li>
  <li>\(S\) as the \(\poly\left( \frac{k}{\epsilon} \right) \times n\) CountSketch matrix. While the number of rows is larger, \(SA\) can be computed in just \(\nnz{A}\) time.</li>
</ol>

<p>In this post, we will focus on using the CountSketch matrix for low-rank approximation.</p>

<h3 id="showing-the-existence-of-a-good-solution-in-the-row-span-of-sa">Showing the Existence of a Good Solution in the Row Span of \(SA\)</h3>
<p>For our proposed algorithm to actually work, we must first guarantee that some
good rank-\(k\) approximation must live in the row span of \(SA\) in the first
place. We will use the following thought experiment to show that such a solution
indeed exists. We will not actually attempt to recover the solution.</p>

<p>Consider the following hypothetical regression problem:</p>

\[\begin{equation}\label{eq:hypothetical-regression}
    \min _{X}\left\|A_{k} X-A\right\|_{\mathrm{F}}=\left\|A_{k}-A\right\|_{\mathrm{F}}.
\end{equation}\]

<p>Note again that this is only hypothetical: we don’t know what \(A_k\) is, because
if we did, then we are already done!</p>

<h4 id="best-solution-to-hypothetical-regression-problem">Best Solution to Hypothetical Regression Problem</h4>
<p>The best solution of \(X\) to Equation \ref{eq:hypothetical-regression} is just
the identity matrix \(I_d\), since \(A_k\) is already defined to be the best
rank-\(k\) approximation to \(A\), and we cannot increase the rank by multiplying
something in hopes of further decreasing the Frobenius norm. This gives us</p>

\[\begin{equation}
    \min _{X}\left\|A_{k}
    X-A\right\|_{\mathrm{F}}=\left\|A_{k}-A\right\|_{\mathrm{F}}.
\end{equation}\]

<h4 id="sketching-the-hypothetical-regression-problem-with-countsketch">Sketching the Hypothetical Regression Problem with CountSketch</h4>
<p>Now take the CountSketch matrix \(S\) to be our affine embedding. We claim that
just \(\poly \left( \frac{k}{\epsilon} \right)\) rows suffices, instead of the
usual \(\poly \left( \frac{d}{\epsilon} \right)\) rows. This is because since
\(A_k\) only has rank \(k\), then we could replace it with some other \(n \times k\)
rank \(k\) matrix \(U_k\) that has the same column span as \(A_k\). This works because
for every \(X\) there is some \(Y\) such that \(A_{k} X=U_{k} Y\) and vice versa. Thus
we can replace \(A_{k}\) with \(U_{k}\) without generality, and \(S\) can be taken as
a CountSketch matrix for the smaller matrix \(U_{k}\).</p>

<p>Recall our affine embedding result:</p>

\[\begin{equation}\label{eq:embedding}
    \left\|S A_{k} X-S A\right\|_{\mathrm{F}}=(1 \pm \varepsilon)\left\|A_{k}
    X-A\right\|_{\mathrm{F}}.
\end{equation}\]

<p>To solve for the \(X\) that minimizes the left hand side quantity, recall that the
normal equation gives us</p>

\[\begin{equation}\label{eq:normal}
    \argmin_{X}\left\|S A_{k} X-S A\right\|_{\mathrm{F}}=\left(S
    A_{k}\right)^{-} S A.
\end{equation}\]

<p>Plug in Equation \ref{eq:normal} into the right side of \ref{eq:embedding} to obtain</p>

\[\begin{equation}\label{eq:exists}
    \|     A_k (SA_k)^- SA - A \|_F \leq (1 \pm \epsilon) \| A_k - A \|_F.
\end{equation}\]

<p>What is exciting about Equation \ref{eq:exists} is that it tells us that
\(A_k (SA_k)^- SA\) is a \((1 \pm \epsilon)\) approximation for \(A_k\).
Furthermore, \(A_k (SA_k)^- SA\) is rank-\(k\) and is in the row span of \(SA\),
which precisely answers our original question of whether a good rank-\(k\)
approximation in the row span of \(SA\) exists in the affirmative!</p>

<p>To belabor the point, we do not know either \(A_k\) or \(A_k (SA_k)^- SA\);
we simply used them to prove that our desired rank-\(k\) approximation in \(SA\) exists.</p>

<h3 id="considering-the-optimal-sketched-solution">Considering the Optimal Sketched Solution</h3>
<p>Our conclusion from the previous section allows us to conclude that</p>

\[\begin{equation}\label{eq:sketch-1}
    \min _{\text {rank-} k X}\|X S A-A\|_{\mathrm{F}}^2 \leq\left\|A_{k}\left(S
    A_{k}\right)^{-} S A-A\right\|_{\mathrm{F}}^2
    \leq(1+\varepsilon)\left\|A_{k}-A\right\|_{\mathrm{F}}^2.
\end{equation}\]

<p>However, solving for the left hand side of Equation \ref{eq:sketch-1} is
different from normal regression, because the rank of our solution for \(X\) is
constrained. Suppose for a moment that we ignore this rank constraint and proceed
to solve for \(X\) as per usual using our normal equations. Then by plugging in
the normal equations into Equation \ref{eq:sketch-1}, we obtain</p>

\[\begin{equation}\label{eq:pyt-1}
    \|X S A-A\|_{\mathrm{F}}^{2}=
    \left\|X S A-A(S A)^{-} S
    A\right\|_{\mathrm{F}}^{2}
    + \left\|A(S A)^{-} S
    A-A\right\|_{\mathrm{F}}^{2}
\end{equation}\]

<p>by considering the Pythagorean theorem for the elements row-wise: \(X_i SA\) is a projection of \(X_i\)
onto \(SA\) in a rank-\(k\) space, and \(A_i (SA)^- SA\) is the projection of \(A_i\) onto \(SA\).</p>

<p>Equation \ref{eq:pyt-1} tells us that the second term on the right hand side does not depend on \(X\),
so we can re-formulate Equation \ref{eq:sketch-1} as</p>

\[\begin{equation}\label{eq:smaller-min}
    \min _{\text {rank-}k\, X}\|X S A-A\|_{\mathrm{F}}^{2}=\left\|A(S A)^{-} S
    A-A\right\|_{\mathrm{F}}^{2}+
    \min _{\text {rank}-k\, X}\left\|X S A-A(S A)^{-} S
    A\right\|_{\mathrm{F}}^{2}.
\end{equation}\]

<h3 id="a-simpler-minimization-problem">A Simpler Minimization Problem</h3>
<p>Equation \ref{eq:smaller-min} reduces our original minimization problem to just solving</p>

\[\begin{equation}
    \min _{\text {rank}-k\, X}\left\|X S A-A(S A)^{-} S
    A\right\|_{\mathrm{F}}^{2}.
\end{equation}\]

<p>To solve this, let’s begin by writing \(S A=U \Sigma V^{\top}\) in SVD form. We can do
this in \(d \cdot \operatorname{poly} \left( \frac{k}{\epsilon} \right)\) time. Then simplify our
expression from Equation \ref{eq:smaller-min} to obtain</p>

\[\begin{align}
      &amp; \min _{\text {rank-}k\, X}\left\|X S A-A(S A)^{-} S
    A\right\|_{\mathrm{F}}^{2}                                                                                                                               \\
    = &amp; \min _{\text {rank}-k\, X}\left\|X U \Sigma
    V^{\top}-A(S A)^{-} U \Sigma V^{\top}\right\|_{\mathrm{F}}^{2}
      &amp; \text{(substituting SVD representation of $S$)}                                                                                                      \\
    = &amp; \min _{\text
        {rank}-k\, X}\left\|\left(X U \Sigma-A(S A)^{-} U \Sigma\right)
    V^{\top}\right\|_{\mathrm{F}}^{2}                                                                                                                        \\
    = &amp; \min _{\text
        {rank}-k\, X}\left\|X U \Sigma-A(S A)^{-} U \Sigma
    \right\|_{\mathrm{F}}^{2}
      &amp; \text{($V^\top$ is orthonormal and does not change norm)}                                                                                            \\
    = &amp; \min _{\text {rank-k } Y}\left\|Y - A (SA)^- U \Sigma\right\|_{F}^{2}. &amp; \text{(change of variables from $X$ to $Y$, since $U\Sigma$ has full rank)} \\
\end{align}\]

<p>This shows that it suffices to just compute the SVD of \(A (SA)^- U \Sigma\) to solve our original problem!</p>

<h3 id="issues-with-running-time">Issues with Running Time</h3>
<p>Unfortunately, we still haven’t achieved the running time we want.
Recall that our goal was to find the best rank-\(k\) approximation in time
\({O(\nnz{A} +(n+d) \operatorname{poly}(k / \varepsilon))}\).
\(A (SA)^- U \Sigma\) is a \(n \times \poly \left( \frac{k}{\epsilon} \right)\) matrix,
so one might claim that computing its SVD only takes \(O\left( n \cdot \poly \left(
        \frac{k}{\epsilon} \right) \right)\), which is within our time bounds.
We could also compute \(SA\), its pseudoinverse \((SA)^-\), and the multiplication \((SA)^-U\Sigma\) quickly.</p>

<p>However, the problem is that we cannot multiply on the left by \(A\) within the
time bounds, because we have no guarantees about the sparsity of \((SA)^- U
    \Sigma\). It may as well be a very dense matrix. This means that computing
\(A(SA)^- U \Sigma\) takes time at least \(O\left( \nnz{A} \cdot \poly\left(
        \frac{k}{\epsilon} \right) \right)\), which does not meet our bounds.</p>

<h3 id="summary-of-current-progress">Summary of Current Progress</h3>
<p>Now is a good time to stop and review the progress that we have made so far. To recap, our current
algorithm works as follows:</p>

<ol>
  <li>Compute \(SA\). This is in time \((\nnz{(A)})\)</li>
  <li>Project each row of \(A\) onto \(SA\). We just showed that we couldn’t do this in our desired time bound
       \({O\left(\nnz{A} +(n+d) \operatorname{poly}\left( \frac{k}{\epsilon} \right)\right)}\).</li>
  <li>Find the best rank-\(k\) approximation of the projected points inside the rowspace of \(SA\).
       This is in time \(O\left( n \cdot \poly \left( \frac{k}{\epsilon}
               \right) \right)\).</li>
</ol>

<p>Therefore Step 2 is our bottleneck that we hope to improve. We will do this by
approximating the projection of \(A\) onto \(SA\).</p>

<h3 id="approximating-the-projection">Approximating the Projection</h3>
<p>Projection is in fact just least-squares regression in disguise.
This inspires us to sketch again to reduce the dimensions of the problem.</p>

<p>Recall that previously we wanted to solve for</p>

\[\begin{equation}\label{eq:target}
    \min _{\text {rank-} k \, X}\|X S A-A\|_{\mathrm{F}}^{2}.
\end{equation}\]

<p>We can’t sketch on the left anymore since \(X\) appears on the left, so we try to
sketch on the right instead. Let \(R\) be an affine embedding matrix, say a
transposed CountSketch matrix with \(\poly \cdot \left( \frac{k}{\epsilon}
    \right)\) columns. Then we sketch on the right in Equation \ref{eq:target} to
change our target to be</p>

\[\begin{equation}\label{eq:new-target}
    \min _{\text {rank-} k \, X}   \|X (S A) R-A R\|_{\mathrm{F}}^{2}.
\end{equation}\]

<p>It would be wise to first verify that we can compute all our quantities in the
desired bounds.  Indeed, computing \(AR\) takes \(\nnz{A}\) time, and computing
\(SAR\) can be done in \(\nnz{SA} \leq \nnz{A}\) by recalling that \(SA\) cannot
increase the number of non-zero entries in \(A\).</p>

<p>Since \(R\) is an affine embedding, by Equation \ref{eq:new-target} we obtain</p>

\[\begin{equation}\label{eq:applied}
    \|X (S A) R-A R\|_{\mathrm{F}}^{2}.
    =(1 \pm \varepsilon)\|X (S
    A)-A\|_{\mathrm{F}}^{2}.
\end{equation}\]

<p>We can rewrite our new target in Equation \ref{eq:new-target} to obtain</p>

\[\begin{align}
    \min _{\text {rank-} k\, X}\|X S A R-A R\|_{\mathrm{F}}^{2}
     &amp; =\left\|A R(S A
    R)^{-} S A R-A R\right\|_{\mathrm{F}}^{2}+\min _{\text {rank-}k\, X}\left\|X S A
    R-A R(S A R)^{-} S A R\right\|_{\mathrm{F}}^{2}
\end{align}\]

<p>by applying the Pythagorean theorem. Then similarly as before, note that only the
second term depends on \(X\), so again we have a smaller minimization problem:</p>

\[\begin{equation}\label{eq:change-var}
    \min _{\text {rank-k } X}\left\|X S A R-A R(S A
    R)^{-}(S A R)\right\|_{\mathrm{F}}^{2}
    =
    \min _{\text {rank-} k\,
        Y}\left\|Y-A R(S A R)^{-} S A R\right\|_{\mathrm{F}}^{2},
\end{equation}\]

<p>where we perform a change of variables to \(Y\). We can do this because the optimal \(Y\) must
live in the row space of \(SAR\) and therefore take the form \(XSAR\). Suppose if it did not, then since \(AR(SAR)^-SAR\) is
in the row space of \(SAR\), then we could simply remove the orthogonal components to \(SAR\)
in \(Y\) to obtain an even better solution, contradicting its optimality.</p>

<h3 id="analyzing-the-runtime">Analyzing the Runtime</h3>
<p>This time round, we have truly solved the problem. Previously, we failed because we could not
project \(A\) onto the column space of \(SA\) in the desired time bounds. But this
time round, \(SAR\) has dimensions \(\poly \left( \frac{k}{\epsilon} \right) \times
    \poly \left( \frac{k}{\epsilon} \right)\), and \(AR\) is a \(n \times \left(
    \frac{k}{\epsilon} \right)\) matrix that takes \(\nnz{A}\) time to compute, which
means that \(AR(SAR)^-(SAR)\) can be computed in time \(n \cdot \poly\left(
    \frac{k}{\epsilon} \right) \in {O\left(\nnz{A} +(n+d) \operatorname{poly}\left(
        \frac{k}{\epsilon} \right)\right)}\).</p>

<p>As a result, we can proceed to compute \(Y\) in Equation \ref{eq:change-var} by
performing truncated SVD in time \(O\left(n \cdot \poly \left( \frac{k}{\epsilon}
        \right)\right)\).</p>

<p>As noted previously, our solution \(Y\) must look like \(Y = XSAR\) for some \(X\).
We can thus solve for \(X = Y(SAR)^-\).</p>

<p>Then by Equation \ref{eq:target}, the final rank-\(k\) approximation that we wish
to return is \(XSA = Y(SAR)^-SA\). A final caveat here is that we must
return \(Y(SAR)^-SA\) in factored form, because if we multiply out the matrices it
is a \(n \times d\) matrix, which does not fit in our desired running time.</p>

<h3 id="returning-the-output-in-factored-form">Returning the Output in Factored Form</h3>
<p>We want to output \(LR = Y(SAR)^-SA\) as \(L, R\) where \(L\) is \(n \times k\) and \(R\) is \(k \times d\).</p>

<p>To do so, recall that \(Y\) is \(n \times \poly \left( \frac{k}{\epsilon} \right)\) and so we
can recover its SVD representation as \(Y = U \Sigma V^\top\). Then
let</p>

\[\begin{align}
    L &amp; = U \Sigma,         \\
    R &amp; = V^\top (SAR)^-SA,
\end{align}\]

<p>where \(L\) is \(n \times  k\) and \(R\) is \(k \times d\). One can check that we can perform all
the matrix multiplications within the time bounds.</p>

<h3 id="bounding-the-failure-probability">Bounding the Failure Probability</h3>
<p>The sketch by \(S\) and \(R\) may both fail to be an affine embedding with some
constant probability, say \(\delta = \frac{1}{100}\).
Then we can simply union bound over \(S\) and \(R\) failing to show that the construction
works with small constant failure probability.</p>

<h3 id="summary">Summary</h3>
<p>That was a lot of discussion and analysis, but in the end, the low-rank
approximation algorithm is very simple.</p>

<ol>
  <li>Compute \(SA\).</li>
  <li>Compute \(SAR\) and \(AR\).</li>
  <li>Compute \(\argmin_Y \left\|Y-A R(S A R)^{-}(S A R)\right\|_{\mathrm{F}}^{2}\) by truncated SVD.</li>
  <li>Output \(Y(SAR)^{-}SA\) in factored form.</li>
</ol>

<p>This takes \(O\left(\nnz{A}+(n+d) \poly \left( \frac{k}{\epsilon} \right)
    \right)\) time overall, which is much faster than applying truncated SVD to \(A\)
directly.</p>

<h2 id="references">References</h2>
<ul>
  <li>Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in
input sparsity time. CoRR, abs/1207.6365, 2012. 
URL: <a href="http://arxiv.org/abs/1207.6365">http://arxiv.org/abs/1207.6365</a>, arXiv:1207.6365.</li>
  <li>Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06), pages
143–152, 2006. doi:10.1109/FOCS.2006.37.</li>
</ul>]]></content><author><name>fanpu</name></author><category term="theory" /><category term="machine-learning" /><category term="math" /><summary type="html"><![CDATA[Suppose you have a \(n \times d\) matrix \(A\), where both dimensions are large. This could represent something like a customer-product matrix used in online recommender systems, where each cell \(A_{i,j}\) denotes how many times customer \(i\) purchased item \(j\). Then it is typically the case that \(A\) can be well-approximated by a low-rank matrix. For instance, using the previous example, there might only be a few dominant patterns that describes purchasing behavior in \(A\), and the rest of it is just noise. Therefore, if we can find such a low-rank approximation, we can achieve significant space savings, and can also help to make the data more interpretable. In this post, we explore how affine embeddings via the CountSketch matrix allows us to perform low rank approximation in time \(O\left(\nnz{A}+(n+d) \poly \left( \frac{k}{\epsilon} \right)\right)\).]]></summary></entry></feed>