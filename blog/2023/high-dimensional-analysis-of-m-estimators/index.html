<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>A Unified Framework for High-Dimensional Analysis of M-Estimators with Decomposable Regularizers: A Guided Walkthrough | Fan Pu  Zeng</title>
    <meta name="author" content="Fan Pu  Zeng">
    <meta name="description" content="Imagine doing high-dimensional statistical inference, but instead of repeatedly studying different settings with specific low-dimensional constraints (such as linear regression with sparsity constraints, or estimation of structured covariance matrices), there is a method for performing a unified analysis using appropriate notions.  &lt;br&gt; &lt;br&gt; Well, you're in luck! 'A Unified Framework for High-Dimensional Analysis of \( M \)-Estimators with Decomposable Regularizers' by Negahban, Ravikumar, Wainwright, and Yu shows that the \( \ell_2 \) difference between any regularized \(M\)-estimator and its true parameter can be bounded if the regularization function is decomposable, and the loss function satisfies restricted strong convexity. &lt;br&gt; &lt;br&gt; The goal of this post is to provide intuition for the result and develop sufficient background for understanding the proof of this result, followed by a walkthrough of the proof itself.
">
    <meta name="keywords" content="fanpu, fan pu, fanpu zeng, fan pu zeng, zengfanpu, fanpuzeng, fzeng, CMU, cmu, carnegie mellon, cmu courses, school of computer science, scs, machine learning, computer science, ml, theory, courses, course reviews, CS, Jane Street">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    
    <!-- Sidebar Table of Contents -->
    <link href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css" rel="stylesheet">
    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon_new.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fanpu.io/blog/2023/high-dimensional-analysis-of-m-estimators/">
    <!-- Dark Mode -->
    

    <!-- Twitter cards -->
    <meta name="twitter:site" content="@FanPu_Zeng">
    <meta name="twitter:creator" content="@fanpu">
    <meta name="og:title" content="A Unified Framework for High-Dimensional Analysis of M-Estimators with Decomposable Regularizers: A Guided Walkthrough">

    
    <meta name="twitter:card" content="summary_large_image">
    
    <meta name="og:image" content="https://fanpu.io/assets/img/posts/dionysus_delphi.webp">

    

    
    <meta name="og:description" content="Imagine doing high-dimensional statistical inference, but instead of repeatedly studying different settings with specific low-dimensional constraints (such as linear regression with sparsity constraints, or estimation of structured covariance matrices), there is a method for performing a unified analysis using appropriate notions.  &lt;br&gt; &lt;br&gt; Well, you're in luck! 'A Unified Framework for High-Dimensional Analysis of \( M \)-Estimators with Decomposable Regularizers' by Negahban, Ravikumar, Wainwright, and Yu shows that the \( \ell_2 \) difference between any regularized \(M\)-estimator and its true parameter can be bounded if the regularization function is decomposable, and the loss function satisfies restricted strong convexity. &lt;br&gt; &lt;br&gt; The goal of this post is to provide intuition for the result and develop sufficient background for understanding the proof of this result, followed by a walkthrough of the proof itself.
">
    

    <!-- end of Twitter cards -->
  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Fan PuÂ </span>Zeng</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/courses/">CMU Course Reviews</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cmu-online/">CMU Online</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/summaries/">ML Paper Summaries</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">Repositories</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        
        <div class="row">
          <!-- sidebar, which will move to the top on a small screen -->
          <div class="col-sm-3">
            <nav id="toc-sidebar" class="sticky-top"></nav>
          </div>
          <!-- main content area -->
          <div class="col-sm-9">
            <!-- _layouts/post.html -->

<div class="post">

  <div style="display:none">
    $$
    \newcommand{\bone}{\mathbf{1}}
    \newcommand{\bbeta}{\mathbf{\beta}}
    \newcommand{\bdelta}{\mathbf{\delta}}
    \newcommand{\bepsilon}{\mathbf{\epsilon}}
    \newcommand{\blambda}{\mathbf{\lambda}}
    \newcommand{\bomega}{\mathbf{\omega}}
    \newcommand{\bpi}{\mathbf{\pi}}
    \newcommand{\bphi}{\mathbf{\phi}}
    \newcommand{\bvphi}{\mathbf{\varphi}}
    \newcommand{\bpsi}{\mathbf{\psi}}
    \newcommand{\bsigma}{\mathbf{\sigma}}
    \newcommand{\btheta}{\mathbf{\theta}}
    \newcommand{\btau}{\mathbf{\tau}}
    \newcommand{\ba}{\mathbf{a}}
    \newcommand{\bb}{\mathbf{b}}
    \newcommand{\bc}{\mathbf{c}}
    \newcommand{\bd}{\mathbf{d}}
    \newcommand{\be}{\mathbf{e}}
    \newcommand{\boldf}{\mathbf{f}}
    \newcommand{\bg}{\mathbf{g}}
    \newcommand{\bh}{\mathbf{h}}
    \newcommand{\bi}{\mathbf{i}}
    \newcommand{\bj}{\mathbf{j}}
    \newcommand{\bk}{\mathbf{k}}
    \newcommand{\bell}{\mathbf{\ell}}
    \newcommand{\bm}{\mathbf{m}}
    \newcommand{\bn}{\mathbf{n}}
    \newcommand{\bo}{\mathbf{o}}
    \newcommand{\bp}{\mathbf{p}}
    \newcommand{\bq}{\mathbf{q}}
    \newcommand{\br}{\mathbf{r}}
    \newcommand{\bs}{\mathbf{s}}
    \newcommand{\bt}{\mathbf{t}}
    \newcommand{\bu}{\mathbf{u}}
    \newcommand{\bv}{\mathbf{v}}
    \newcommand{\bw}{\mathbf{w}}
    \newcommand{\bx}{\mathbf{x}}
    \newcommand{\by}{\mathbf{y}}
    \newcommand{\bz}{\mathbf{z}}
    \newcommand{\bA}{\mathbf{A}}
    \newcommand{\bB}{\mathbf{B}}
    \newcommand{\bC}{\mathbf{C}}
    \newcommand{\bD}{\mathbf{D}}
    \newcommand{\bE}{\mathbf{E}}
    \newcommand{\bF}{\mathbf{F}}
    \newcommand{\bG}{\mathbf{G}}
    \newcommand{\bH}{\mathbf{H}}
    \newcommand{\bI}{\mathbf{I}}
    \newcommand{\bJ}{\mathbf{J}}
    \newcommand{\bK}{\mathbf{K}}
    \newcommand{\bL}{\mathbf{L}}
    \newcommand{\bM}{\mathbf{M}}
    \newcommand{\bN}{\mathbf{N}}
    \newcommand{\bP}{\mathbf{P}}
    \newcommand{\bQ}{\mathbf{Q}}
    \newcommand{\bR}{\mathbf{R}}
    \newcommand{\bS}{\mathbf{S}}
    \newcommand{\bT}{\mathbf{T}}
    \newcommand{\bU}{\mathbf{U}}
    \newcommand{\bV}{\mathbf{V}}
    \newcommand{\bW}{\mathbf{W}}
    \newcommand{\bX}{\mathbf{X}}
    \newcommand{\bY}{\mathbf{Y}}
    \newcommand{\bZ}{\mathbf{Z}}

    \newcommand{\bsa}{\boldsymbol{a}}
    \newcommand{\bsb}{\boldsymbol{b}}
    \newcommand{\bsc}{\boldsymbol{c}}
    \newcommand{\bsd}{\boldsymbol{d}}
    \newcommand{\bse}{\boldsymbol{e}}
    \newcommand{\bsoldf}{\boldsymbol{f}}
    \newcommand{\bsg}{\boldsymbol{g}}
    \newcommand{\bsh}{\boldsymbol{h}}
    \newcommand{\bsi}{\boldsymbol{i}}
    \newcommand{\bsj}{\boldsymbol{j}}
    \newcommand{\bsk}{\boldsymbol{k}}
    \newcommand{\bsell}{\boldsymbol{\ell}}
    \newcommand{\bsm}{\boldsymbol{m}}
    \newcommand{\bsn}{\boldsymbol{n}}
    \newcommand{\bso}{\boldsymbol{o}}
    \newcommand{\bsp}{\boldsymbol{p}}
    \newcommand{\bsq}{\boldsymbol{q}}
    \newcommand{\bsr}{\boldsymbol{r}}
    \newcommand{\bss}{\boldsymbol{s}}
    \newcommand{\bst}{\boldsymbol{t}}
    \newcommand{\bsu}{\boldsymbol{u}}
    \newcommand{\bsv}{\boldsymbol{v}}
    \newcommand{\bsw}{\boldsymbol{w}}
    \newcommand{\bsx}{\boldsymbol{x}}
    \newcommand{\bsy}{\boldsymbol{y}}
    \newcommand{\bsz}{\boldsymbol{z}}
    \newcommand{\bsA}{\boldsymbol{A}}
    \newcommand{\bsB}{\boldsymbol{B}}
    \newcommand{\bsC}{\boldsymbol{C}}
    \newcommand{\bsD}{\boldsymbol{D}}
    \newcommand{\bsE}{\boldsymbol{E}}
    \newcommand{\bsF}{\boldsymbol{F}}
    \newcommand{\bsG}{\boldsymbol{G}}
    \newcommand{\bsH}{\boldsymbol{H}}
    \newcommand{\bsI}{\boldsymbol{I}}
    \newcommand{\bsJ}{\boldsymbol{J}}
    \newcommand{\bsK}{\boldsymbol{K}}
    \newcommand{\bsL}{\boldsymbol{L}}
    \newcommand{\bsM}{\boldsymbol{M}}
    \newcommand{\bsN}{\boldsymbol{N}}
    \newcommand{\bsP}{\boldsymbol{P}}
    \newcommand{\bsQ}{\boldsymbol{Q}}
    \newcommand{\bsR}{\boldsymbol{R}}
    \newcommand{\bsS}{\boldsymbol{S}}
    \newcommand{\bsT}{\boldsymbol{T}}
    \newcommand{\bsU}{\boldsymbol{U}}
    \newcommand{\bsV}{\boldsymbol{V}}
    \newcommand{\bsW}{\boldsymbol{W}}
    \newcommand{\bsX}{\boldsymbol{X}}
    \newcommand{\bsY}{\boldsymbol{Y}}
    \newcommand{\bsZ}{\boldsymbol{Z}}

    \newcommand{\calA}{\mathcal{A}}
    \newcommand{\calB}{\mathcal{B}}
    \newcommand{\calC}{\mathcal{C}}
    \newcommand{\calD}{\mathcal{D}}
    \newcommand{\calE}{\mathcal{E}}
    \newcommand{\calF}{\mathcal{F}}
    \newcommand{\calG}{\mathcal{G}}
    \newcommand{\calH}{\mathcal{H}}
    \newcommand{\calI}{\mathcal{I}}
    \newcommand{\calJ}{\mathcal{J}}
    \newcommand{\calK}{\mathcal{K}}
    \newcommand{\calL}{\mathcal{L}}
    \newcommand{\calM}{\mathcal{M}}
    \newcommand{\calN}{\mathcal{N}}
    \newcommand{\calO}{\mathcal{O}}
    \newcommand{\calP}{\mathcal{P}}
    \newcommand{\calQ}{\mathcal{Q}}
    \newcommand{\calR}{\mathcal{R}}
    \newcommand{\calS}{\mathcal{S}}
    \newcommand{\calT}{\mathcal{T}}
    \newcommand{\calU}{\mathcal{U}}
    \newcommand{\calV}{\mathcal{V}}
    \newcommand{\calW}{\mathcal{W}}
    \newcommand{\calX}{\mathcal{X}}
    \newcommand{\calY}{\mathcal{Y}}
    \newcommand{\calZ}{\mathcal{Z}}

    \newcommand{\R}{\mathbb{R}}
    \newcommand{\C}{\mathbb{C}}
    \newcommand{\N}{\mathbb{N}}
    \newcommand{\Z}{\mathbb{Z}}
    \newcommand{\F}{\mathbb{F}}
    \newcommand{\Q}{\mathbb{Q}}

    \DeclareMathOperator*{\argmax}{arg\,max}
    \DeclareMathOperator*{\argmin}{arg\,min}
    \newcommand{\nnz}[1]{\mbox{nnz}(#1)}
    \newcommand{\dotprod}[2]{\langle #1, #2 \rangle}

    \newcommand{\ignore}[1]{}

    \let\Pr\relax
    \DeclareMathOperator*{\Pr}{\mathbf{Pr}}
    \newcommand{\E}{\mathbb{E}}
    \DeclareMathOperator*{\Ex}{\mathbf{E}}
    \DeclareMathOperator*{\Var}{\mathbf{Var}}
    \DeclareMathOperator*{\Cov}{\mathbf{Cov}}
    \DeclareMathOperator*{\stddev}{\mathbf{stddev}}
    \DeclareMathOperator*{\avg}{avg}

    \DeclareMathOperator{\poly}{poly}
    \DeclareMathOperator{\polylog}{polylog}
    \DeclareMathOperator{\size}{size}
    \DeclareMathOperator{\sgn}{sgn}
    \DeclareMathOperator{\dist}{dist}
    \DeclareMathOperator{\vol}{vol}
    \DeclareMathOperator{\spn}{span}
    \DeclareMathOperator{\supp}{supp}
    \DeclareMathOperator{\tr}{tr}
    \DeclareMathOperator{\Tr}{Tr}
    \DeclareMathOperator{\codim}{codim}
    \DeclareMathOperator{\diag}{diag}

    \newcommand{\PTIME}{\mathsf{P}}
    \newcommand{\LOGSPACE}{\mathsf{L}}
    \newcommand{\ZPP}{\mathsf{ZPP}}
    \newcommand{\RP}{\mathsf{RP}}
    \newcommand{\BPP}{\mathsf{BPP}}
    \newcommand{\P}{\mathsf{P}}
    \newcommand{\NP}{\mathsf{NP}}
    \newcommand{\TC}{\mathsf{TC}}
    \newcommand{\AC}{\mathsf{AC}}
    \newcommand{\SC}{\mathsf{SC}}
    \newcommand{\SZK}{\mathsf{SZK}}
    \newcommand{\AM}{\mathsf{AM}}
    \newcommand{\IP}{\mathsf{IP}}
    \newcommand{\PSPACE}{\mathsf{PSPACE}}
    \newcommand{\EXP}{\mathsf{EXP}}
    \newcommand{\MIP}{\mathsf{MIP}}
    \newcommand{\NEXP}{\mathsf{NEXP}}
    \newcommand{\BQP}{\mathsf{BQP}}
    \newcommand{\distP}{\mathsf{dist\textbf{P}}}
    \newcommand{\distNP}{\mathsf{dist\textbf{NP}}}

    \newcommand{\eps}{\epsilon}
    \newcommand{\lam}{\lambda}
    \newcommand{\dleta}{\delta}
    \newcommand{\simga}{\sigma}
    \newcommand{\vphi}{\varphi}
    \newcommand{\la}{\langle}
    \newcommand{\ra}{\rangle}
    \newcommand{\wt}[1]{\widetilde{#1}}
    \newcommand{\wh}[1]{\widehat{#1}}
    \newcommand{\ol}[1]{\overline{#1}}
    \newcommand{\ul}[1]{\underline{#1}}
    \newcommand{\ot}{\otimes}
    \newcommand{\zo}{\{0,1\}}
    \newcommand{\co}{:} %\newcommand{\co}{\colon}
    \newcommand{\bdry}{\partial}
    \newcommand{\grad}{\nabla}
    \newcommand{\transp}{^\intercal}
    \newcommand{\inv}{^{-1}}
    \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff}
    \newcommand{\half}{\tfrac{1}{2}}
    \newcommand{\bbone}{\mathbbm 1}
    \newcommand{\Id}{\bbone}

    \newcommand{\SAT}{\mathsf{SAT}}

    \newcommand{\bcalG}{\boldsymbol{\calG}}
    \newcommand{\calbG}{\bcalG}
    \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX}
    \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY}
    \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ}
    $$
</div>

  <header class="post-header">
    <h1 class="post-title">A Unified Framework for High-Dimensional Analysis of M-Estimators with Decomposable Regularizers: A Guided Walkthrough</h1>
    <p class="post-meta">July 14, 2023â¢ fanpu</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
      Â  Â· Â 
        <a href="/blog/tag/statistics">
          <i class="fas fa-hashtag fa-sm"></i> statistics</a> Â 
          <a href="/blog/tag/machine-learning">
          <i class="fas fa-hashtag fa-sm"></i> machine-learning</a> Â 
          

    </p>
  </header>

  
      <figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/dionysus_delphi.webp" class="preview z-depth-1 rounded center" width="100%" height="450px" alt="post.cover" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>
</figure>

    <div class="caption">
        Theatre of Dionysus at Delphi, Greece
    </div>
  

  <article class="post-content">
    
    <div id="markdown-content">
      \[\newcommand{\rcal}{\mathcal{R}}
    \newcommand{\lcal}{\mathcal{L}}
    \newcommand{\mcal}{\mathcal{M}}
    \newcommand{\mocal}{\overline{\mathcal{M}}}
    \newcommand{\mocalp}{\overline{\mathcal{M}}^\perp}
    \newcommand{\mcalp}{\mathcal{M}^\perp}
    \newcommand{\sse}{\subseteq}
    \newcommand{\kl}{\kappa_{\lcal}}
    \newcommand{\tl}{\tau_{\lcal}}
    \newcommand{\ts}{\theta^*}
    \newcommand{\hd}{\widehat{\Delta}}
    \newcommand{\thatn}{\hat{\theta}_n}
    \newcommand{\that}{\hat{\theta}}
    \newcommand{\thatlambda}{\widehat{\theta}_{\lambda_n}}
    \newcommand{\thatl}{\thatlambda}
    \newcommand{\rs}{\rcal^*}
    \newcommand{\ctriplet}{ \C(\mcal, \mocalp; \ts) }
    \newcommand{\fcal}{\mathcal{F}}
    \newcommand{\kbb}{\mathbb{K}}
\newcommand{\dotprod}[2]{\langle #1, #2 \rangle}
    \DeclareMathOperator*{\argmin}{arg\,min}\]

<h1 id="introduction">Introduction</h1>

<p>In high-dimensional statistical inference, it is common for the number of
parameters \(p\) to be comparable to or greater than the sample size \(n\).
However, for an estimator \(\thatn\) to be consistent in such a regime,
meaning that it converges to the true parameter \(\theta\),
it is necessary to make additional low-dimensional
assumptions on the model.
Examples of such constraints that have been well-studied include
linear regression with sparsity constraints, estimation of structured covariance
or inverse covariance matrices, graphical model selection, sparse principal
component analysis (PCA), low-rank matrix estimation, matrix decomposition problems
and estimation of sparse additive nonparametric models <a href="https://arxiv.org/abs/1010.2731" rel="external nofollow noopener" target="_blank">(Negahban et al., 2009)</a>.</p>

<p>In recent years, there has been a flurry of work on each of these individual specific cases.
However, the authors of the paper in discussion poses the question of whether there is a way
of unifying these analysis to understand all of such estimators in a common framework,
and answers it in the affirmative. They showed that it is possible to bound
the squared difference between any regularized \(M\)-estimator and its true
parameter by (1) the decomposability of the regularization function, and (2)
restricted strong convexity of the loss function. We will call this the âmain theoremâ
in the remainder of the blog post, and this is referred to as âTheorem 1â in <a href="https://arxiv.org/abs/1010.2731" rel="external nofollow noopener" target="_blank">(Negahban et al., 2009)</a>.</p>

<p>In the remainder of the paper, we will develop the tools necessary to deeply
understand and prove the result. Notation used will be consistent with the
original paper for expositional clarity.</p>

<h1 id="background">Background</h1>

<p>In this section, we develop some of the necessary background and notation to build up to the proof.</p>

<h2 id="regularized-m-estimators">Regularized \(M\)-estimators</h2>

<p>\(M\)-estimators (\(M\) for âmaximum likelihood-typeâ) are solutions that minimize the sum of loss functions \(\rho\):
\begin{align}
    \that \in \argmin_\theta \sum_{i=1}^n \rho(x_i, \theta).
\end{align}</p>

<p>If we add a regularization term \(\rcal\) to penalize complexity of the model, scaled by weights \(\lambda\), the method is known as a regularized \(M\)-estimator:
\begin{align}
    \that \in \argmin_\theta \sum_{i=1}^n \rho(x_i, \theta) + \lambda \rcal(\theta).
\end{align}</p>

<div class="example">
    <div class="theorem-title">Example
        
        
            (Lasso Program)
        
    </div>
    <div class="theorem-contents">
        
    The Lasso program is an example of a regularized \( M \)-estimator, where a
    \( \ell_1 \) regularization penalty is applied:
    $$
        \that \in \argmin_{\theta \in \mathbb{R}^d} \left\{
        \frac{1}{2n} \| y - \bX \theta \|_2^2 + \lambda_n \| \theta \|_1
        \right\}.
    $$
  
    </div>
</div>

<h2 id="dual-norms">Dual Norms</h2>

<div class="definition">
    <div class="theorem-title">Definition
        
        
            (Dual Norms)
        
    </div>
    <div class="theorem-contents">
        
    Let \(\rcal\) be a norm induced by an inner product
    \(\dotprod{\cdot}{\cdot}\). Then the dual norm of \(\rcal\)
    is defined as
    $$
        \rs(v) \coloneqq \sup_{u \in \mathbb{R}^p \setminus \left\{ 0 \right\}}
        \frac{ \dotprod{u}{v} }{\rcal (u)} = \sup_{\rcal(u) \leq 1} \dotprod{u}{v}.
    $$
  
    </div>
</div>

<div class="example">
    <div class="theorem-title">Example
        
        
            (\(\ell_1\) and \(\ell_\infty\) norms are dual norms)
        
    </div>
    <div class="theorem-contents">
        
  We will show that the dual of the \( \ell_1 \) norm is the \( \ell_\infty \) norm.

Well, to see that \( \rs(v) \leq \| v \|_\infty \), observe that
        \begin{align*}
            \rs(v)
             &amp; = \sup_{\| u \|_1 \leq 1} \dotprod{u}{v}                                        \\
             &amp; = \sup_{\| u \|_1 \leq 1} \sum_{k=1}^p | u_k | | v_k |                          \\
             &amp; \leq \sup_{\| u \|_1 \leq 1} \left( \sum_{k=1}^p | u_k | \right) \| v \|_\infty \\
             &amp; = | v |_\infty   \tag{since \( \| u \|_1 \leq 1 \) }.
        \end{align*}
        For the opposite direction,

        \begin{align*}
            \sup_{\| u \|_1 \leq 1} \dotprod{u}{v}
             &amp; = \sup_{\| u \|_1 \leq 1} \sum_{k=1}^p |u_k| |v_k|                               \\
             &amp; \geq 1 \cdot |v_j| \tag{
             set \( j = \argmax_j |v_j|, u = \be_j \)
             } \\
             &amp; = \| v \|_\infty,
        \end{align*}
        hence we have equality.

  
    </div>
</div>

<h2 id="subspace-compatibility-constant">Subspace Compatibility Constant</h2>

<p>The subspace compatibility constant measures how much the regularizer \(\rcal\) can change
with respect to the error norm  \(\| \cdot \|\) restricted to the subspace \(\mcal\).
This concept will show up later in showing that the restricted strong convexity
condition will hold with certain parameters.</p>

<p>The subspace compatibility constant is defined as follows:</p>

<div class="definition">
    <div class="theorem-title">Definition
        
        
            (Subspace Compatibility Constant)
        
    </div>
    <div class="theorem-contents">
        
    For any subspace \( \mcal \) of \( \mathbb{R}^p \), the <i>subspace compatibility constant</i>
    with respect to the pair \( (\rcal, \| \cdot \|) \) is given by

    $$
        \varPsi (\mcal) \coloneqq \sup_{u \in \mcal \setminus \left\{ 0 \right\}} \frac{\rcal(u)}{\| u \|}.
    $$
    
    </div>
</div>

<p>It can be thought of as the Lipschitz constant of the regularizer with respect to the error norm
restricted to values in \(\mcal\),
by considering the point where it can vary the most.</p>

<h2 id="projections">Projections</h2>
<p>Define the projection operator
\begin{align}
    \Pi_{\mcal}(u) \coloneqq \argmin_{v \in \mcal} | u - v |
\end{align}
to be the projection of \(u\) onto the subspace \(\mcal\).
For notational brevity, we will use the shorthand \(u_{\mcal} = \Pi_{\mcal}(u)\).</p>

<p>One property of the projection operator is that it is non-expansive, meaning that
\begin{align}
    | \Pi(u) - \Pi(v) | \leq | u - v | \label{eq:non-expansive}
\end{align}
for some error norm \(\| \cdot \|\). In other words, it has Lipschitz constant 1.</p>

<h1 id="problem-formulation">Problem Formulation</h1>

<p>In our setup, we define the following quantities:</p>

<ul>
  <li>\(Z_1^n \coloneqq \left\{ Z_1, \cdots, Z_n \right\}\) \(n\) i.i.d observations
      drawn from distribution \(\mathbb{P}\) with some parameter \(\theta^*\),</li>
  <li>\(\mathcal{L}: \mathbb{R}^p \times \mathcal{Z}^n \to \mathbb{R}\) a convex and differentiable loss function, such that \(\mathcal{L}(\theta; Z_1^n)\) returns the loss of \(\theta\) on observations \(Z_1^n\),</li>
  <li>\(\lambda_n &gt; 0\): a user-defined regularization penalty,</li>
  <li>\(\mathcal{R} : \mathbb{R}^p \to \mathbb{R}_+\) a norm-based regularizer.</li>
</ul>

<p>The purpose of the regularized \(M\)-estimator is then to solve for the convex optimization problem</p>

\[\begin{align} \label{eq:opt}
    \widehat{\theta}_{\lambda_n} \in \argmin_{\theta \in \mathbb{R}^p} \left\{
    \mathcal{L}(\theta; Z_1^n) + \lambda_n \mathcal{R} (\theta) \right\},
\end{align}\]

<p>and we are interested in deriving bounds on
\(\begin{align}
    \| \thatlambda - \theta^* \|
\end{align}\)
for some error norm \(\| \cdot \|\) induced by an inner product \(\langle \cdot, \cdot \rangle\) in \(\mathbb{R}^p\).</p>

<h1 id="decomposability-of-the-regularizer-mathcalr">Decomposability of the Regularizer \(\mathcal{R}\)</h1>

<p>The first key property in the result is decomposability of our norm-based regularizer \(\rcal\).
Working in the ambient \(\mathbb{R}^p\), define \(\mcal \sse \mathbb{R}^p\) to be the model subspace that captures
the constraints of the model that we are working with (i.e \(k\)-sparse vectors),
and denote \(\mocal\) to be its closure, i.e the union of \(\mcal\) and all of its limit points.
In addition, denote \(\mocalp\) to be the orthogonal complement of \(\mocal\), namely</p>

\[\begin{align}
    \mocalp \coloneqq \left\{ v \in \mathbb{R}^p \mid \langle u, v \rangle = 0 \text{ for all \( u \in \mocal \) }
    \right\}.
\end{align}\]

<p>We call this the perturbation subspace, as they represent perturbations away from the model subspace \(\mocal\).
The reason why we need to consider \(\mocal\) instead of \(\mcal\) is because there are some special cases
of low-rank matrices and nuclear norms where it could be possible that \(\mcal\) is strictly contained in \(\mocal\).</p>

<p>Now we can introduce the property of decomposability:</p>

<div class="definition">
    <div class="theorem-title">Definition
        
        
            (Regularizer Decomposability)
        
    </div>
    <div class="theorem-contents">
        
      Given a pair of subspaces \( \mcal \sse \mocal \), a norm-based regularizer
    \( \rcal \) is <i>decomposable</i> with respect to \( (\mocal, \mocalp) \) if

    $$
        \rcal(\theta + \gamma) = \rcal(\theta) + \rcal(\gamma)
    $$

    for all \( \theta \in \mcal \) and \( \gamma \in \mocalp \).
  
    </div>
</div>

<p>Since \(\rcal\) is a norm-based regularizer, by the triangle inequality property of norms we know that always
\begin{align}
    \rcal(\theta + \gamma) \leq \rcal(\theta) + \rcal(\gamma),
\end{align}
and hence this is a stronger condition which requires tightness in the
inequality when we are specifically considering elements in the closure of the
model subspace and its orthogonal complement.</p>

<p>Decomposability of the regularizer is important as it allows us to penalize deviations \(\gamma\)
away from the model subspace in \(\mcal\) to the maximum extent possible.
We are usually interested to find model subspaces that are small, with a large orthogonal complement.
We will see in the main theorem that when this is the case, we will obtain better rates for estimating
\(\theta^*\).</p>

<p>There are many natural contexts that admit regularizers which are decomposable with respect to subspaces,
and the following example highlights one such case.</p>

<div class="example">
    <div class="theorem-title">Example
        
        
            (\( s \)-sparse Vectors)
        
    </div>
    <div class="theorem-contents">
        
      Consider estimating the parameters \( \that \) with \( \ell_1 \)-regularization in \( \mathbb{R}^p \) where we assume that
    the model is \( s \)-sparse. Then for any set \( S \sse [p] \) where \( |S| = s \),
    we can define our model subspace \( \mcal \) as

    \[
    \begin{align*}
        \mcal(S) \coloneqq \left\{ \theta \in \mathbb{R}^p \mid \theta_j = 0 \quad \forall j \not\in S \right\},
    \end{align*}
    \]

    i.e all the vectors in \( \mathbb{R}^p \) that only has support in \( S \). In this case, \( \mcal = \mocal \),
    and our orthogonal complement \( \mocalp \) is just

    \[
    \begin{align*}
        \mocalp(S) \coloneqq \left\{ \gamma \in \mathbb{R}^p \mid \gamma_j = 0 \quad \forall j \in S \right\}.
    \end{align*}
    \]

    Then this setup is decomposable:

    \[
    \begin{align*}
        \| \theta + \gamma \|_1 = \| \theta_S + \gamma_{S^c} \|_1 = \| \theta_S \|_1 + \| \gamma_{S^c} \| = \| \theta \|_1 + \| \gamma \|_1
    \end{align*}
    \] 

    by the Pythagorean theorem.
  
    </div>
</div>

<h2 id="role-of-decomposability">Role of Decomposability</h2>

<figure id="fig-1">
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/high-dimensional-analysis-of-m-estimators/c_illust.webp" class="z-depth-1 center" width="500px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption"><i>Figure 1.</i>
      
        A visualization of \( \ctriplet \).  The shaded area represents the set
        \( \ctriplet \), i.e all values of \( \theta \) that satisfies the inequality of
        the set in Lemma 1. 
    
    </figcaption>
</figure>

<p>Decomposability is important because it allows us to bound the error of the estimator.
This is given in the following result, which is known as Lemma 1 in <a href="https://arxiv.org/abs/1010.2731" rel="external nofollow noopener" target="_blank">(Negahban et al., 2009)</a>:</p>

<div class="lemma" id="lemma-1">
    <div class="theorem-title">Lemma
        
        
            (Lemma 1 in 
  <a href="https://arxiv.org/abs/1010.2731" rel="external nofollow noopener" target="_blank">
    (Negahban et al., 2009)
  </a>)
        
    </div>
    <div class="theorem-contents">
        
    Suppose that \( \lcal \) is a convex and differentiable function, and consider
    any optimal solution \( \that \) to the optimization problem
    with a strictly positive regularization parameter satisfying

    $$
    \begin{align*}
        \lambda_n \geq 2 \rcal^* (\nabla \lcal (\ts; Z_1^n)).
    \end{align*}
    $$

    Then for any pair \( (\mcal, \mocalp) \) over which \( \rcal \) is decomposable,
    the error \( \hd = \thatlambda - \ts  \) belongs to the set

    $$
    \begin{align*} \label{eq:c}
        \C(\mcal, \mocalp; \ts) \coloneqq \left\{  \Delta \in \mathbb{R}^p \mid
        \rcal(\Delta_{\mocalp}) \leq 3 \rcal (\Delta_{\mocal}) + 4 \rcal (\ts_{\mcalp})
        \right\}.
    \end{align*}
    $$
  
    </div>
</div>

<p>Recall from the <a href="#projections">Projections Section</a> that
\(\Delta_{\mocalp}\) represents the projection of \(\Delta\) onto \(\mocalp\), and similarly
for the other quantities.
Due to space constraints, we are unable to prove Lemma <a href="#lemma-1">Lemma 1</a> in this survey,
but it is very important in the formulation of restricted strong convexity, and in proving
<a href="#thm-1">Theorem 1</a>.</p>

<p><a href="#fig-1">Figure 1</a> provides a visualization of \(\ctriplet\) in \(\mathbb{R}^3\) in the
sparse vectors setting. In this case, \(S = \left\{ 3 \right\}\) with \(|S|=1\),
and so the projection of \(\Delta\) onto the model subspace only has non-zero
values on the third coordinate, and its orthogonal complement is where the third
coordinate is zero. Formally,</p>

\[\begin{align}
    \mcal(S) = \mocal(S) &amp; = \left\{ \Delta \in \mathbb{R}^3 \mid \Delta_1 = \Delta_2 = 0 \right\}, \\
    \mocalp(S)           &amp; = \left\{ \Delta \in \mathbb{R}^3 \mid \Delta_3 = 0 \right\}.
\end{align}\]

<p>The vertical axis of <a href="#fig-1">Figure 1</a> denotes the third coordinate,
and the horizontal plane denotes the first two coordinates.
The shaded area
represents the set \(\ctriplet\), i.e all values of \(\theta\) that satisfies the inequality
of the set in <a href="#lemma-1">Lemma 1</a>.</p>

<p><a href="#fig-1">Figure 1(a)</a> shows the special case
when \(\ts \in \mcal\). In this scenario, \(\rcal (\ts_{\mcalp}) = 0\), and so</p>

\[\begin{align*}
    \C(\mcal, \mocalp; \ts) = \left\{  \Delta \in \mathbb{R}^p \mid
    \rcal(\Delta_{\mocalp}) \leq 3 \rcal (\Delta_{\mocal}) \right\},
\end{align*}\]

<p>which is a cone.</p>

<p>However, in the general setting where \(\ts \not\in \mcal\),
then \(\rcal (\ts_{\mcalp}) &gt; 0\), and the set \(\ctriplet\) will become a star-shaped set
like what is shown in <a href="#fig-1">Figure 1(b)</a>.</p>

<h1 id="restricted-strong-convexity-rsc-of-the-loss-function">Restricted Strong Convexity (RSC) of the Loss Function</h1>

<figure id="fig-2">
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/high-dimensional-analysis-of-m-estimators/curvature.webp" class="z-depth-1 center" width="500px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption"><i>Figure 2.</i>
      
        An illustration of the role of curvature in guaranteeing that
        \( \hd = \thatlambda - \ts \) is small when \( \lcal(\thatlambda) - \lcal(\ts) \) is small.
    
    </figcaption>
</figure>

<p>In a classical setting, as the number of samples \(n\) increases, the difference
in loss \(d \lcal = |\lcal(\thatlambda) - \lcal(\ts)|\) will converge to zero.
However, the convergence in loss by itself is insufficient to also ensure
the convergence in parameters, \(\hd = \thatlambda - \ts\). Instead, it also
depends on the curvature of the loss function \(\lcal\).</p>

<p><a href="#fig-2">Figure 2</a> illustrates the importance of curvature.
In <a href="#fig-2">Figure 2(a)</a>, \(\lcal\) has high curvature, and so
having a small \(d\lcal\) also implies a small \(\hd\). On the other hand,
in <a href="#fig-2">Figure 2(b)</a>, \(\lcal\) has an almost flat landscape
near \(\thatlambda\), and hence even when \(d \lcal\) is small,
\(\hd\) could still be large.</p>

<p>Consider performing a Taylor expansion of \(\lcal\) around \(\ts\):</p>

\[\begin{align}
    \lcal(\ts + \Delta)
     &amp; = \lcal(\ts) + \dotprod{\nabla \lcal(\ts)}{\Delta}
    + \underbrace{\frac{1}{2} \Delta^T \nabla^2 \lcal(\ts) \Delta + \dots}_{\delta \lcal(\Delta, \ts)}.
\end{align}\]

<p>Then we can rearrange and write the error of the first-order Taylor series expansion at \(\ts\) as</p>

\[\begin{align*}
    \delta \lcal(\Delta, \ts) = \lcal(\ts + \Delta) - \lcal(\ts) -
    \dotprod{\nabla \lcal(\ts)}{\Delta}.
\end{align*}\]

<p>The first-order Taylor approximation is a linear approximation, and hence the error
\(\delta \lcal(\Delta, \ts)\), which is dominated by the quadratic term, can capture the curvature
about \(\ts\).</p>

<p>As such, one way to show that \(\lcal\) has good curvature about \(\ts\) is to show that
\(\delta \lcal(\Delta, \ts) \geq \kappa \|\Delta \|^2\) holds for all \(\Delta\) in
a neighborhood of \(\ts\). This is because we are enforcing a lower bound on its quadratic growth.</p>

<p>This leads us to the definition of restricted strong convexity:</p>

<div class="definition">
    <div class="theorem-title">Definition
        
        
            (Restricted Strong Convexity)
        
    </div>
    <div class="theorem-contents">
        
    The loss function satisfies a <i>restricted strong convexity</i> (RSC)
    condition with curvature \( \kl &gt; 0 \) and tolerance function \( \tl \) if
    \begin{align*}
        \delta \lcal(\Delta, \ts) \geq \kl \| \Delta \|^2 - \tl^2(\ts)
    \end{align*}
    for all \( \Delta \in \ctriplet \).
  
    </div>
</div>

<p>We only need to consider error terms \(\Delta \in \ctriplet\), since Lemma \ref{lemma:1}
guarantees us that the error term will only lie in that set.</p>

<p>In many statistical models, restricted strong convexity holds with \(\tl = 0\), however, it is required in more general settings, such as generalized linear models.</p>

<h1 id="proof-of-theorem-1">Proof of Theorem 1</h1>
<p>We can now state and prove the main result of the paper.
This will hold under the decomposability of the regularizer (G1), and the
restricted strong convexity of the loss function (G2).</p>

<ul>
  <li>
    <p><strong>(G1)</strong>
      The regularizer \(\rcal\) is a norm and is decomposable
      with respect to the subspace pair \((\mcal, \mocalp)\), where \(\mcal \sse \mocalp\).</p>
  </li>
  <li>
    <p><strong>(G2)</strong>
      The loss function \(\lcal\) is convex and differentiable, and satisfies restricted strong convexity
      with curvature \(\kl\) and tolerance \(\tl\).</p>
  </li>
</ul>

<div class="theorem" id="thm-1">
    <div class="theorem-title">Theorem
        
            1 in (Negahban et al., 2009)
        
        
            (Bounds for General Models)
        
    </div>
    <div class="theorem-contents">
        
      Under conditions (G1) and (G2),
    consider the convex optimization problem (\ref{eq:opt})
    based on a strictly positive positive regularization constant
    \( \lambda_n \geq 2 \rs (\nabla \lcal (\ts)) \). Then any optimal solution
    \( \thatlambda \) to the convex program (\ref{eq:opt}) satisfies the bound
    \begin{align*}
        \| \thatlambda - \ts \|^2 \leq 9 \frac{\lambda_n^2}{\kl^2} \Psi^2(\mocal)
        + \frac{\lambda_n}{\kl} \left( 2 \tl^2 (\ts) + 4 \rcal (\ts_{\mcal^{\perp}}) \right).
    \end{align*}
  
    </div>
</div>

<p>We will rely on the following lemmas that will be stated without proof due to space constraints:</p>

<div class="lemma">
    <div class="theorem-title">Lemma
        
            3 in (Negahban et al., 2009)
        
        
            (Deviation Inequalities)
        
    </div>
    <div class="theorem-contents">
        
    For any decomposable regularizer and \( p \)-dimensional
    vectors \( \ts \) and \( \Delta \), we have
    \begin{align*}
        \rcal(\ts + \Delta) - \rcal(\ts) \geq
        \rcal(\Delta_{\mocalp}) - \rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}}).
    \end{align*}
    Moreover, as long as \( \lambda_n \geq 2 \rs (\nabla \lcal(\ts)) \) and \( \lcal \) is convex, we have
    \begin{align*}
        \lcal(\ts + \Delta) - \lcal(\ts) \geq - \frac{\lambda_n}{2} [\rcal(\Delta_{\mocal}) + \rcal(\Delta_{\mocalp})].
    \end{align*}
  
    </div>
</div>

<div class="lemma">
    <div class="theorem-title">Lemma
        
            4 in (Negahban et al., 2009)
        
        
    </div>
    <div class="theorem-contents">
        
    If \( \fcal(\Delta) &gt; 0 \) for all vectors \( \Delta \in \mathbb{K}(\delta) \), then
    \( \| \hd \| \leq \delta \).
  
    </div>
</div>

<p>Note that this was similar to our previous analysis on restricted strong
convexity where we only really need to consider error terms restricted to
\(\ctriplet\) due to <a href="#lemma-1">Lemma 1</a>.  Therefore, it suffices to show
\(\fcal(\Delta) &gt; 0\) to obtain a bound on \(\| \hd \| = \| \thatlambda - \ts\|\), 
which completes the proof of Theorem 1.</p>

<p>Define \(\fcal : \mathbb{R}^p \to \mathbb{R}\) by</p>

\[\begin{align}
    \fcal(\Delta) \coloneqq \lcal(\ts + \Delta) - \lcal(\ts) + \lambda_n \left\{
    \rcal(\ts + \Delta) - \rcal(\ts)
    \right\},
\end{align}\]

<p>and define the set</p>

\[\begin{align}
    \mathbb{K}(\delta) \coloneqq \ctriplet \cap \left\{ \| \Delta \| = \delta \right\}.
\end{align}\]

<p>Take any \(\Delta \in \kbb\). Then</p>

\[\begin{align}
    \fcal(\Delta)
    =    &amp; \lcal(\ts + \Delta) - \lcal(\ts) + \lambda_n \left\{
    \rcal(\ts + \Delta) - \rcal(\ts) \right\} \tag{by definition} \\
    \geq &amp; \langle  \nabla \lcal (\ts), \Delta \rangle + \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{
    \rcal(\ts + \Delta) - \rcal(\ts) \right\} \\
    &amp; \qquad \text{(by restricted strong convexity:
        \(\delta \lcal(\Delta, \ts) \geq \kl \| \Delta \|^2 - \tl^2(\ts)\),} \\
    &amp; \qquad \text{ and
    \( \delta \lcal(\Delta, \ts) = \lcal(\ts + \Delta) - \lcal(\ts) -
        \dotprod{\nabla \lcal(\ts)}{\Delta} \) ) } \\
    \geq &amp; \langle  \nabla \lcal (\ts), \Delta \rangle + \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{
    \rcal(\Delta_{\mocalp}) - \rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}})
    \right\} \\
    &amp; \qquad \text{(by Lemma 3)}.
    \label{thm-deriv:1}
\end{align}\]

<p>We lower bound the first term as
\(\langle  \nabla \lcal (\ts), \Delta \rangle \geq - \frac{\lambda_n}{2}
    \rcal(\Delta)\):</p>

\[\begin{align}
    | \langle  \nabla \lcal (\ts), \Delta \rangle |
    \leq                                             &amp; \rs(\nabla \lcal(\ts)) \rcal(\Delta) &amp; \text{(Cauchy-Schwarz using dual norms \( \rcal \) and \( \rs \))} \\
    \leq                                             &amp; \frac{\lambda_n}{2} \rcal(\Delta) &amp; \text{Theorem 1 assumption: \( \lambda_n \geq 2 \rs (\nabla \lcal(\ts)) \))},
\end{align}\]

<p>and hence,</p>

\[\begin{align}
    \langle  \nabla \lcal (\ts), \Delta \rangle \geq &amp; - \frac{\lambda_n}{2}
    \rcal(\Delta).
\end{align}\]

<p>So applying to (\ref{thm-deriv:1}),</p>

\[\begin{align}
    \fcal(\Delta)
    \geq &amp; \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{
    \rcal(\Delta_{\mocalp}) - \rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}})
    \right\} - \frac{\lambda_n}{2} \rcal(\Delta)                                                                                                                                       \\
    \geq &amp; \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{
    \rcal(\Delta_{\mocalp}) - \rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}})
    \right\} - \frac{\lambda_n}{2} (\rcal(\Delta_{\mocalp}) + \rcal(\Delta_{\mocal})) \\
    &amp; \qquad \text{(Triangle inequality: \( \rcal(\Delta) \leq \rcal(\Delta_{\mocalp}) + \rcal(\Delta_{\mocal}) \))} \\
    =    &amp; \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{
    \frac{1}{2}\rcal(\Delta_{\mocalp}) -
    \frac{3}{2}\rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}})
    \right\} \\
    &amp; \qquad \text{(Moving terms in)} \\
    \geq &amp; \kl \| \Delta \|^2 - \tl^2(\ts) + \lambda_n \left\{
    -
    \frac{3}{2}\rcal(\Delta_{\mocal}) - 2 \rcal(\ts_{\mcal^{\perp}})
    \right\} \\
    &amp; \qquad \text{(Norms always non-negative)} \\
    = &amp; \kl \| \Delta \|^2 - \tl^2(\ts) - \frac{\lambda_n }{2} \left\{
    3 \rcal(\Delta_{\mocal}) + 4 \rcal(\ts_{\mcal^{\perp}})
    \right\} \label{eq:r-delta-lb} .
\end{align}\]

<p>To bound the term \(\rcal(\Delta_{\mocal})\),
recall the definition of subspace compatibility:</p>

\[\begin{align}
    \varPsi (\mcal) \coloneqq \sup_{u \in \mcal \setminus \left\{ 0 \right\}} \frac{\rcal(u)}{\| u \|}, \label{eq:r-delta-ub}
\end{align}\]

<p>and hence</p>

\[\begin{align}
    \rcal(\Delta_{\mocal}) \leq \varPsi(\mocal) \| \Delta_{\mocal} \|.
\end{align}\]

<p>To upper bound \(\| \Delta_{\mocal} \|\), we have</p>

\[\begin{align}
    \| \Delta_{\mocal} \|
        &amp; = \| \Pi_{\mocal} (\Delta) - \Pi_{\mocal}(0) \| &amp; \text{(Since \(0 \in \mocal \), \( \Pi_{\mocal}(0) = 0 \)) }     \\
        &amp; \leq \| \Delta - 0 \| &amp; \text{(Projection operator is non-expansive, see Equation \ref{eq:non-expansive})} \\
        &amp; = \| \Delta \|,
\end{align}\]

<p>which substituting into Equation (\ref{eq:r-delta-ub}) gives</p>

\[\begin{align}
    \rcal(\Delta_{\mocal}) \leq \varPsi(\mocal) \| \Delta \|.
\end{align}\]

<p>Now we can use this result to lower bound
Equation \ref{eq:r-delta-lb}:</p>

\[\begin{align}
    \fcal (\Delta)
    \geq &amp; \kl \| \Delta \|^2 - \tl^2(\ts) - \frac{\lambda_n }{2} \left\{
    3 \varPsi(\mocal) \| \Delta \|
    + 4 \rcal(\ts_{\mcal^{\perp}})
    \right\}. \label{eq:strict-psd}
\end{align}\]

<p>The RHS of the inequality in Equation \ref{eq:strict-psd} has a strictly
positive definite quadratic form in \(\| \Delta \|\), and hence by taking
\(\| \Delta \|\) large, it will be strictly positive.
To find such a sufficiently large \(\| \Delta \|\), write</p>

\[\begin{align}
    a &amp; = \kl,                                                     \\
    b &amp; = \frac{3\lambda_n}{2} \varPsi (\mocal),                  \\
    c &amp; = \tau_{\lcal}^2 (\ts) + 2 \lambda_n \rcal(\ts_{\mcalp}), \\
\end{align}\]

<p>such that we have</p>

\[\begin{align}
    \fcal (\Delta)
        &amp; \geq a \| \Delta \|^2 - b \| \Delta \| - c.
\end{align}\]

<p>Then the square of the rightmost intercept is given by the squared quadratic formula</p>

\[\begin{align}
    \| \Delta \|^2
        &amp; = \left( \frac{-(-b) + \sqrt{b^2 - 4a(-c)}}{2a} \right)^2                                                 \\
        &amp; = \left( \frac{b + \sqrt{b^2 + 4ac}}{2a} \right)^2                                                 \\
        &amp; \leq \left( \frac{\sqrt{b^2 + 4ac}}{a} \right)^2 &amp; \text{($b \leq \sqrt{b^2 + 4ac}$)}                                              \label{eq:coarse-bound}  \\
        &amp; = \frac{b^2 + 4ac}{a^2}                                                 \\
        &amp; = \frac{9 \lambda_n^2 \varPsi^2 (\mocal)}{4 \kl^2}
    + \frac{ 4 \tau_{\lcal}^2 (\ts) + 8 \lambda_n \rcal(\ts_{\mcalp}) }{\kl}. &amp; \text{(Substituting in \(a, b, c\))} \\
\end{align}\]

<p>In <a href="https://arxiv.org/abs/1010.2731" rel="external nofollow noopener" target="_blank">(Negahban et al., 2009)</a>, they were able to show an upper bound of</p>

\[\begin{align}
    \| \Delta \|^2
        &amp; \leq \frac{9 \lambda_n^2 \varPsi^2 (\mocal)}{\kl^2} +
    \frac{\lambda_n}{\kl} \left\{
    2\tau_{\lcal}^2 (\ts) + 4 \rcal(\ts_{\mcalp})
    \right\}, \label{eq:ub}
\end{align}\]

<p>but I did not manage to figure out how they managed to produce a \(\lambda_n\)
term beside the \(\tl^2(\ts)\) term. All other differences are just 
constant factors. It may be due to an overly coarse
bound on my end applied in Equation \ref{eq:coarse-bound}, but it
is unclear to me how the \(\lambda_n\) term can be applied on only
the \(\tl^2(\ts)\) term without affecting the \(\rcal(\ts_{\mcalp})\) term.</p>

<p>With Equation \ref{eq:ub}, we can hence apply Lemma 4 in <a href="https://arxiv.org/abs/1010.2731" rel="external nofollow noopener" target="_blank">(Negahban et al., 2009)</a>
to obtain the desired result that</p>

\[\begin{align}
    \| \thatlambda - \ts \|^2 \leq 9 \frac{\lambda_n^2}{\kl^2} \Psi^2(\mocal)
    + \frac{\lambda_n}{\kl} \left( 2 \tl^2 (\ts) + 4 \rcal (\ts_{\mcal^{\perp}}) \right).
\end{align}\]

<p>This concludes the proof.</p>

<h1 id="conclusion">Conclusion</h1>
<p>In the <a href="#proof-of-theorem-1">proof of Theorem 1</a>, we saw how
the bound is derived from the two key ingredients of the decomposability
of the regularizer, and restricted strong convexity of the loss function.
The decomposability of the regularizer allowed us to ensure that the
error vector \(\hd\) will stay in the set \(\ctriplet\). This condition
is then required in Lemma 4 of <a href="https://arxiv.org/abs/1010.2731" rel="external nofollow noopener" target="_blank">(Negahban et al., 2009)</a>, which allows us
to bound \(\| \hd \|\) given that \(\fcal(\Delta) &gt; 0\). In one of the
steps where we were lower bounding \(\fcal(\Delta)\) in the proof,
we made use of the properties of restricted strong convexity.</p>

<p><a href="#thm-1">Theorem 1</a> provides a family of bounds for each decomposable
regularizer under the choice of \((\mcal, \mocalp)\).
The authors of <a href="https://arxiv.org/abs/1010.2731" rel="external nofollow noopener" target="_blank">(Negahban et al., 2009)</a> were able to use
<a href="#thm-1">Theorem 1</a> to rederive both existing known results,
and also derive new results on low-rank matrix estimation using the nuclear
norm, minimax-optimal rates for noisy matrix completion, and noisy matrix
decomposition. The reader is encouraged to refer to <a href="https://arxiv.org/abs/1010.2731" rel="external nofollow noopener" target="_blank">(Negahban et al., 2009)</a>
for more details on the large number of corrollaries of <a href="#thm-1">Theorem 1</a>.</p>

<h1 id="acknowledgments">Acknowledgments</h1>
<p>I would like to thank my dear friend <a href="https://www.linkedin.com/in/josh-abrams-78a4a6134/" rel="external nofollow noopener" target="_blank">Josh
Abrams</a>
for helping to review and provide valuable suggestions for this post!</p>

<h1 id="citations">Citations</h1>
<ol>
  <li>Negahban, S., Yu, B., Wainwright, M. J., and Ravikumar, P. <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf" rel="external nofollow noopener" target="_blank">A unified
framework for high-dimensional analysis of m-estimators with decomposable
regularizers</a>.
In Bengio, Y., Schuurmans, D., Lafferty, J., Williams, C., and Culotta, A.
(eds.), Advances in Neural Information Processing Systems, volume 22. Curran
Associates, Inc., 2009. 
URL https://proceedings.neurips.cc/paper_files/paper/2009/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf.</li>
</ol>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <!-- <p class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</p> -->
    <p class="mb-2">Related Posts:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/llama-3.1-technical-report-notes/">Notes on 'The Llama 3 Herd of Models'</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/setting-up-yuancon-controller-sound-voltex/">Playing Sound Voltex at Home: Setting Up Unnamed SDVX Clone with the Yuancon SDVX Controller</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/creating-trackback-requests/">Creating Trackback Requests for Static Sites</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cmu-steam-tunnels/">The CMU Steam Tunnels and Wean 9</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/advanced-operating-systems-course-review/">CMU 15712 Advanced Operating Systems and Distributed Systems Course Review</a>
  </li>

<div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "fanpu/website",
        "data-repo-id": "R_kgDOIpOodA",
        "data-category": "General",
        "data-category-id": "DIC_kwDOIpOodM4CTKDC",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

          </div>
        </div>
        
      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2024 Fan Pu  Zeng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.
Last updated: August 09, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Enable Tooltips -->
  <script type="text/javascript">
  $(function () {$('[data-toggle="tooltip"]').tooltip()})
  </script>
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>
  <!-- Sidebar Table of Contents -->
  <script defer src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>


  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      loader: {load: ['[tex]/mathtools']},
      tex: {
        packages: {'[+]': ['mathtools', 'ams']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          addMenu: []
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
