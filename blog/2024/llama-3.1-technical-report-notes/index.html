<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Notes on 'The Llama 3 Herd of Models' | Fan Pu Zeng </title> <meta name="author" content="Fan Pu Zeng"> <meta name="description" content="Notes on the new Llama 3.1 technical report. It's a long paper, but one that's well-written with lots of interesting technical details and design choices. "> <meta name="keywords" content="fanpu, fan pu, fanpu zeng, fan pu zeng, zengfanpu, fanpuzeng, fzeng, CMU, cmu, carnegie mellon, cmu courses, school of computer science, scs, machine learning, computer science, ml, theory, courses, course reviews, CS, Jane Street"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon_new.ico?426605099301e95aedae716cc398b951"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://fanpu.io/blog/2024/llama-3.1-technical-report-notes/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Fan Pu</span> Zeng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/courses/">CMU Course Reviews </a> </li> <li class="nav-item "> <a class="nav-link" href="/cmu-online/">CMU Online </a> </li> <li class="nav-item "> <a class="nav-link" href="/summaries/">ML Paper Summaries </a> </li> <li class="nav-item "> <a class="nav-link" href="/reading-list/">ML Reading List </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <div style="display: none"> $$ \newcommand{\bone}{\mathbf{1}} \newcommand{\bbeta}{\mathbf{\beta}} \newcommand{\bdelta}{\mathbf{\delta}} \newcommand{\bepsilon}{\mathbf{\epsilon}} \newcommand{\blambda}{\mathbf{\lambda}} \newcommand{\bomega}{\mathbf{\omega}} \newcommand{\bpi}{\mathbf{\pi}} \newcommand{\bphi}{\mathbf{\phi}} \newcommand{\bvphi}{\mathbf{\varphi}} \newcommand{\bpsi}{\mathbf{\psi}} \newcommand{\bsigma}{\mathbf{\sigma}} \newcommand{\btheta}{\mathbf{\theta}} \newcommand{\btau}{\mathbf{\tau}} \newcommand{\ba}{\mathbf{a}} \newcommand{\bb}{\mathbf{b}} \newcommand{\bc}{\mathbf{c}} \newcommand{\bd}{\mathbf{d}} \newcommand{\be}{\mathbf{e}} \newcommand{\boldf}{\mathbf{f}} \newcommand{\bg}{\mathbf{g}} \newcommand{\bh}{\mathbf{h}} \newcommand{\bi}{\mathbf{i}} \newcommand{\bj}{\mathbf{j}} \newcommand{\bk}{\mathbf{k}} \newcommand{\bell}{\mathbf{\ell}} \newcommand{\bm}{\mathbf{m}} \newcommand{\bn}{\mathbf{n}} \newcommand{\bo}{\mathbf{o}} \newcommand{\bp}{\mathbf{p}} \newcommand{\bq}{\mathbf{q}} \newcommand{\br}{\mathbf{r}} \newcommand{\bs}{\mathbf{s}} \newcommand{\bt}{\mathbf{t}} \newcommand{\bu}{\mathbf{u}} \newcommand{\bv}{\mathbf{v}} \newcommand{\bw}{\mathbf{w}} \newcommand{\bx}{\mathbf{x}} \newcommand{\by}{\mathbf{y}} \newcommand{\bz}{\mathbf{z}} \newcommand{\bA}{\mathbf{A}} \newcommand{\bB}{\mathbf{B}} \newcommand{\bC}{\mathbf{C}} \newcommand{\bD}{\mathbf{D}} \newcommand{\bE}{\mathbf{E}} \newcommand{\bF}{\mathbf{F}} \newcommand{\bG}{\mathbf{G}} \newcommand{\bH}{\mathbf{H}} \newcommand{\bI}{\mathbf{I}} \newcommand{\bJ}{\mathbf{J}} \newcommand{\bK}{\mathbf{K}} \newcommand{\bL}{\mathbf{L}} \newcommand{\bM}{\mathbf{M}} \newcommand{\bN}{\mathbf{N}} \newcommand{\bP}{\mathbf{P}} \newcommand{\bQ}{\mathbf{Q}} \newcommand{\bR}{\mathbf{R}} \newcommand{\bS}{\mathbf{S}} \newcommand{\bT}{\mathbf{T}} \newcommand{\bU}{\mathbf{U}} \newcommand{\bV}{\mathbf{V}} \newcommand{\bW}{\mathbf{W}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bY}{\mathbf{Y}} \newcommand{\bZ}{\mathbf{Z}} \newcommand{\bsa}{\boldsymbol{a}} \newcommand{\bsb}{\boldsymbol{b}} \newcommand{\bsc}{\boldsymbol{c}} \newcommand{\bsd}{\boldsymbol{d}} \newcommand{\bse}{\boldsymbol{e}} \newcommand{\bsoldf}{\boldsymbol{f}} \newcommand{\bsg}{\boldsymbol{g}} \newcommand{\bsh}{\boldsymbol{h}} \newcommand{\bsi}{\boldsymbol{i}} \newcommand{\bsj}{\boldsymbol{j}} \newcommand{\bsk}{\boldsymbol{k}} \newcommand{\bsell}{\boldsymbol{\ell}} \newcommand{\bsm}{\boldsymbol{m}} \newcommand{\bsn}{\boldsymbol{n}} \newcommand{\bso}{\boldsymbol{o}} \newcommand{\bsp}{\boldsymbol{p}} \newcommand{\bsq}{\boldsymbol{q}} \newcommand{\bsr}{\boldsymbol{r}} \newcommand{\bss}{\boldsymbol{s}} \newcommand{\bst}{\boldsymbol{t}} \newcommand{\bsu}{\boldsymbol{u}} \newcommand{\bsv}{\boldsymbol{v}} \newcommand{\bsw}{\boldsymbol{w}} \newcommand{\bsx}{\boldsymbol{x}} \newcommand{\bsy}{\boldsymbol{y}} \newcommand{\bsz}{\boldsymbol{z}} \newcommand{\bsA}{\boldsymbol{A}} \newcommand{\bsB}{\boldsymbol{B}} \newcommand{\bsC}{\boldsymbol{C}} \newcommand{\bsD}{\boldsymbol{D}} \newcommand{\bsE}{\boldsymbol{E}} \newcommand{\bsF}{\boldsymbol{F}} \newcommand{\bsG}{\boldsymbol{G}} \newcommand{\bsH}{\boldsymbol{H}} \newcommand{\bsI}{\boldsymbol{I}} \newcommand{\bsJ}{\boldsymbol{J}} \newcommand{\bsK}{\boldsymbol{K}} \newcommand{\bsL}{\boldsymbol{L}} \newcommand{\bsM}{\boldsymbol{M}} \newcommand{\bsN}{\boldsymbol{N}} \newcommand{\bsP}{\boldsymbol{P}} \newcommand{\bsQ}{\boldsymbol{Q}} \newcommand{\bsR}{\boldsymbol{R}} \newcommand{\bsS}{\boldsymbol{S}} \newcommand{\bsT}{\boldsymbol{T}} \newcommand{\bsU}{\boldsymbol{U}} \newcommand{\bsV}{\boldsymbol{V}} \newcommand{\bsW}{\boldsymbol{W}} \newcommand{\bsX}{\boldsymbol{X}} \newcommand{\bsY}{\boldsymbol{Y}} \newcommand{\bsZ}{\boldsymbol{Z}} \newcommand{\calA}{\mathcal{A}} \newcommand{\calB}{\mathcal{B}} \newcommand{\calC}{\mathcal{C}} \newcommand{\calD}{\mathcal{D}} \newcommand{\calE}{\mathcal{E}} \newcommand{\calF}{\mathcal{F}} \newcommand{\calG}{\mathcal{G}} \newcommand{\calH}{\mathcal{H}} \newcommand{\calI}{\mathcal{I}} \newcommand{\calJ}{\mathcal{J}} \newcommand{\calK}{\mathcal{K}} \newcommand{\calL}{\mathcal{L}} \newcommand{\calM}{\mathcal{M}} \newcommand{\calN}{\mathcal{N}} \newcommand{\calO}{\mathcal{O}} \newcommand{\calP}{\mathcal{P}} \newcommand{\calQ}{\mathcal{Q}} \newcommand{\calR}{\mathcal{R}} \newcommand{\calS}{\mathcal{S}} \newcommand{\calT}{\mathcal{T}} \newcommand{\calU}{\mathcal{U}} \newcommand{\calV}{\mathcal{V}} \newcommand{\calW}{\mathcal{W}} \newcommand{\calX}{\mathcal{X}} \newcommand{\calY}{\mathcal{Y}} \newcommand{\calZ}{\mathcal{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\N}{\mathbb{N}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\F}{\mathbb{F}} \newcommand{\Q}{\mathbb{Q}} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \newcommand{\nnz}[1]{\mbox{nnz}(#1)} \newcommand{\dotprod}[2]{\langle #1, #2 \rangle} \newcommand{\ignore}[1]{} \let\Pr\relax \DeclareMathOperator*{\Pr}{\mathbf{Pr}} \newcommand{\E}{\mathbb{E}} \DeclareMathOperator*{\Ex}{\mathbf{E}} \DeclareMathOperator*{\Var}{\mathbf{Var}} \DeclareMathOperator*{\Cov}{\mathbf{Cov}} \DeclareMathOperator*{\stddev}{\mathbf{stddev}} \DeclareMathOperator*{\avg}{avg} \DeclareMathOperator{\poly}{poly} \DeclareMathOperator{\polylog}{polylog} \DeclareMathOperator{\size}{size} \DeclareMathOperator{\sgn}{sgn} \DeclareMathOperator{\dist}{dist} \DeclareMathOperator{\vol}{vol} \DeclareMathOperator{\spn}{span} \DeclareMathOperator{\supp}{supp} \DeclareMathOperator{\tr}{tr} \DeclareMathOperator{\Tr}{Tr} \DeclareMathOperator{\codim}{codim} \DeclareMathOperator{\diag}{diag} \newcommand{\PTIME}{\mathsf{P}} \newcommand{\LOGSPACE}{\mathsf{L}} \newcommand{\ZPP}{\mathsf{ZPP}} \newcommand{\RP}{\mathsf{RP}} \newcommand{\BPP}{\mathsf{BPP}} \newcommand{\P}{\mathsf{P}} \newcommand{\NP}{\mathsf{NP}} \newcommand{\TC}{\mathsf{TC}} \newcommand{\AC}{\mathsf{AC}} \newcommand{\SC}{\mathsf{SC}} \newcommand{\SZK}{\mathsf{SZK}} \newcommand{\AM}{\mathsf{AM}} \newcommand{\IP}{\mathsf{IP}} \newcommand{\PSPACE}{\mathsf{PSPACE}} \newcommand{\EXP}{\mathsf{EXP}} \newcommand{\MIP}{\mathsf{MIP}} \newcommand{\NEXP}{\mathsf{NEXP}} \newcommand{\BQP}{\mathsf{BQP}} \newcommand{\distP}{\mathsf{dist\textbf{P}}} \newcommand{\distNP}{\mathsf{dist\textbf{NP}}} \newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda} \newcommand{\dleta}{\delta} \newcommand{\simga}{\sigma} \newcommand{\vphi}{\varphi} \newcommand{\la}{\langle} \newcommand{\ra}{\rangle} \newcommand{\wt}[1]{\widetilde{#1}} \newcommand{\wh}[1]{\widehat{#1}} \newcommand{\ol}[1]{\overline{#1}} \newcommand{\ul}[1]{\underline{#1}} \newcommand{\ot}{\otimes} \newcommand{\zo}{\{0,1\}} \newcommand{\co}{:} %\newcommand{\co}{\colon} \newcommand{\bdry}{\partial} \newcommand{\grad}{\nabla} \newcommand{\transp}{^\intercal} \newcommand{\inv}{^{-1}} \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff} \newcommand{\half}{\tfrac{1}{2}} \newcommand{\mathbbm}{\Bbb} \newcommand{\bbone}{\mathbbm 1} \newcommand{\Id}{\bbone} \newcommand{\SAT}{\mathsf{SAT}} \newcommand{\bcalG}{\boldsymbol{\calG}} \newcommand{\calbG}{\bcalG} \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX} \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY} \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ} $$ </div> <header class="post-header"> <h1 class="post-title">Notes on 'The Llama 3 Herd of Models'</h1> <p class="post-meta"> Created in August 07, 2024 by fanpu </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a> </p> </header> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/posts/furano.webp-480.webp 480w, /assets/img/posts/furano.webp-800.webp 800w, /assets/img/posts/furano.webp-1400.webp 1400w, " sizes="95vw" type="image/webp"> <img src="/assets/img/posts/furano.webp" class="preview z-depth-1 rounded center" width="100%" height="450px" alt="post.cover" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="caption"> Lavender Fields in Biei, Kamikawa Subprefecture, Hokkaido, Japan </div> <article class="post-content"> <div id="markdown-content"> <h1 id="reading-recommendations">Reading Recommendations</h1> <p>This is a long paper, but it’s full of gems. Here’s a reading recommendation guide:</p> <ul> <li>Strapped on time: sections 1 (Introduction), 2 (General Overview). It’s just a couple of pages and provides a good overview.</li> <li>Love ML systems: 3.3 (Infrastructure, Scaling, Efficiency). Talks about on hardware, architecture, training challenges, parallelism optimizations</li> <li>How to train a coding model: 4.3.1 (Code). Covers how they targeted specific coding abilities and generated synthetic datasets to bootstrap the model</li> <li>Training model to perform tool use: 4.3.5 (Tool Use)</li> <li>Post-training framework: 4.1 (Modeling). Covers their pipeline for reward modeling, supervised finetuning, and direct preference optimization</li> <li>Extending to 128K context: 3.4.2 (Long Context Pre-Training) and 4.3.4 (Long Context)</li> <li>Why a 405B model: 3.2.1 (Scaling Laws)</li> <li>Optimizations for inference: 6 (Inference) on both pipeline parallelism and FP8 quantization, this is a short section</li> <li>Results and benchmarks: 5.1, 5.2 (Pre and Post-trained Language Model), 5.3 (Human Evaluations)</li> <li>Red teaming: 5.4.6 (Red Teaming)</li> <li>Multi-modality: 7 (Vision Experiments), 8 (Speech Experiments), 9.2 (Multimodality)</li> </ul> <h1 id="introduction">Introduction</h1> <p>Introduces new set of models (8/70/405 B) that supports:</p> <ul> <li>multilinguality</li> <li>coding</li> <li>reasoning</li> <li>tool usage</li> </ul> <p>Largest model:</p> <ul> <li>405B parameters</li> <li>128k context window</li> <li>Has instruction fine-tuned version</li> <li>Pre-trained on 3.8 x \(10^{25}\) FLOPS</li> </ul> <p>Also introduced Llama Guard 3 model for input/output safety.</p> <h1 id="pre-training">Pre-training</h1> <h2 id="pre-training-data">Pre-Training Data</h2> <h3 id="data-cleaning">Data Cleaning</h3> <p>Knowledge cutoff end of 2023. To ensure high-quality tokens, performed: de-duplication, data cleaning, removed domains known to contain large amounts of PII, adult content.</p> <p>Data cleaning:</p> <ul> <li>extracts HTML content from web documents</li> <li>done carefully for pages with math &amp; code content to preserve structure</li> <li>Markdown markers also removed</li> </ul> <p>De-duplication:</p> <ul> <li>on the URL, duplication across documents, line-level de-duplication (common in boilerplate)</li> </ul> <p>Used heuristics to filter other low-quality documents: logs/error messages, other adult websites, websites with excessive numbers of outlier tokens</p> <p>Built a model-based classifier to sub-select high-quality tokens.</p> <p>Built domain-specific pipelines to extract code &amp; math-relevant web pages, including pages containing math deduction, pages containing code interleaved with natural language.</p> <p>Used similar approaches as the above for other languages.</p> <h3 id="data-mix">Data Mix</h3> <p>This ensures they have the right proportion of different data sources. They ended up with:</p> <ul> <li>50% general knowledge</li> <li>25% math &amp; reasoning</li> <li>17% code</li> <li>8% multilingual</li> </ul> <p>Knowledge classification: categorizes data to determine the data mix. Used this to downsample data over-represented on the web like arts &amp; entertainment.</p> <p>Scaling laws for data mix: trained several small models on data mix &amp; use that to predict the performance of large models on mix</p> <p>Overview</p> <ul> <li>15.6T multilingual tokens (compare 1.8T for Llama 2)</li> <li>Use 8K token context window initially, followed by continued pre-training stage which increases supported context window to 128K tokens</li> </ul> <h3 id="multi-modality">Multi-modality</h3> <h4 id="encoders">Encoders</h4> <p>Separate encoders trained for images and speech.</p> <p>Image encoder:</p> <ul> <li>Trained on image-text pairs</li> </ul> <p>Speech encoder:</p> <ul> <li>Self-supervised learning via masking</li> <li>Masked part reconstructed by discrete-token representation</li> </ul> <h4 id="adapters">Adapters</h4> <p>TBD</p> <h3 id="annealing-data">Annealing Data</h3> <p>Performed annealing on small amounts of high-quality code and mathematical data. Annealing here means increasingly upsampling these high-quality data over time.</p> <p>Found improvements for Llama 3 8B on GSM8k and MATH, but not 405B.</p> <h2 id="model-architecture">Model Architecture</h2> <p>Architecture</p> <ul> <li>Uses dense Transformer architecture instead of MoE for training stability</li> <li>Similar to Llama and Llama 2, performance gains mostly from improvements in data quality &amp; diversity, and training scale</li> <li>Grouped query attention with 8 KV heads: improves inference speed, reduce size of KV cache during decoding</li> <li>Attention mask to prevent self-attention between different documents (why not just put them in different sequences? maybe to take advantage of parallelism?). Limited impact during pre-training, helpful for continued pre-training on long sequences</li> <li>RoPE for positional embeddings (500,000 base frequency hyperparameter instead of 10k in original paper, this is helpful for longer context), SwiGLU activation</li> <li>128K token vocabulary, based off <code class="language-plaintext highlighter-rouge">tiktoken</code> tokenizer and extra 28K non-English tokens. Tokenizer improves compression rate from 3.17 to 3.94 characters per token compared to Llama 2 tokenizer.</li> <li>Llama 3 405B: 126 layers (!!), model dimension 16,382, 128 attention heads</li> </ul> <h3 id="scaling-laws">Scaling Laws</h3> <p>Scaling laws are nice for predicting loss, but not helpful for understanding impact on downstream task performance.</p> <p>To find relationship with downstream task performance they did:</p> <ol> <li>Find correlation between compute-optimal model’s loss on downstream tasks and training FLOPs</li> <li>Find correlation between loss and downstream task accuracy, using scaling law models</li> </ol> <p>The scaling laws suggest that given their compute budget of \(3.8 \times 10^{25}\) FLOPs, a 402B model with 16.55T tokens is optimal, which led to their 405B model.</p> <p>They also found their predictions to be quite accurate for the final downstream performance of their models.</p> <h3 id="infrastructure-scaling-and-efficiency">Infrastructure, Scaling, and Efficiency</h3> <p>Compute:</p> <ul> <li>16K H100 GPUs, 700W TDP (thermal design power) with 80GB HBM3 (high bandwidth memory that allows for faster data transfer between CPU and GPU)</li> <li>Trained on Meta’s Grand Teton AI server platform, scheduling using MAST (Meta’s global-scale training scheduler)</li> <li>Each server: 8 GPUs connected by NVLink, 2 CPUs</li> </ul> <p>Storage:</p> <ul> <li>Tectonic, Meta’s distributed file system</li> <li>240 PB storage over 7500 servers, 2TB/s sustainable throughput, 7TB/s peak throughput</li> </ul> <p>Network:</p> <ul> <li>Llama 3 405B used RDMA over Converged Ethernet (RoCE) fabric</li> <li>Smaller models uses Nvidia Quantum2 Infiniband</li> <li>Both 400 Gbps interconnect</li> </ul> <h3 id="parallelism-for-model-scaling">Parallelism for Model Scaling</h3> <p>Scaled parallelism as much as possible, so all of GPU’s model parameters, optimizer states, gradients, and activations fit in HBM.</p> <p>4D parallelism:</p> <ul> <li>tensor parallelism</li> <li>pipeline parallelism</li> <li>context parallelism</li> <li>data parallelism</li> </ul> <p>Parallelism achieved BF16 Model FLOPs Utilization (MFU) of 38-43%</p> <h3 id="reliability-and-operational-challenges">Reliability and Operational Challenges</h3> <ul> <li> <blockquote> <p>90% effective training time, even while supporting automated cluster maintenance (i.e Linux kernel upgrades)</p> </blockquote> </li> <li>At least one training interruption daily</li> </ul> <p>466 job interruptions</p> <ul> <li>47 planned interruptions (i.e maintenance)</li> <li>419 unexpected: mostly GPU/host component failures, suspected data corruption, unplanned maintenance</li> <li>Significant manual intervention only required 3 times, rest handled by automation</li> </ul> <p>Debugging</p> <ul> <li>PyTorch’s built-in NCCL flight recorder helped diagnose issues quickly at scale</li> <li>Mixed use of NVLink and RoCE complicated things</li> </ul> <p>Others</p> <ul> <li>Higher mid-day temperatures impacted GPU dynamic voltage and frequency scaling, causing diurnal 1-2% throughput variation throughout the day</li> <li>~10ks of GPUs with correlated increase/decrease in power consumption (i.e waiting for checkpointing) causes fluctuation of power consumption on the order of ~10s megawatts, stretching limits of power grid</li> </ul> <h2 id="training-recipe">Training Recipe</h2> <p>Initial pre-training:</p> <ul> <li>AdamW optimizer</li> <li>Linear warm up, cosine LR schedule</li> <li>Start with lower batch size for training stability, increase subsequently for efficiency</li> <li>Few loss spikes, no interventions needed to correct for training divergence</li> <li>Upsampled non-English and math data, downsampled low-quality data</li> <li>Added recent web data in final stages of pre-training to advance model knowledge cut-off</li> </ul> <p>Long context pre-training:</p> <ul> <li>To support 128K context window</li> <li>Don’t do long-context training earlier because of quadratic self-attention, too expensive</li> <li>Increased context length by successive adaptation over 6 stages from 8K to 128K, 800B training tokens</li> </ul> <p>Annealing:</p> <ul> <li>On final 40M tokens, linearly annealed LR to 0, kept 128K context window</li> <li>Upsampled data source of very high quality</li> <li>Averaged model checkpoints during annealing to get final pre-trained model</li> </ul> <h1 id="post-training">Post-Training</h1> <ul> <li>Several rounds of post-training, each starts with SFT followed by DPO</li> <li>Examples collected by human annotations or generated synthetically</li> <li>Custom</li> </ul> <h2 id="modeling">Modeling</h2> <ul> <li>Uses reward model (RM) and language model (LM)</li> <li>RM trained by human-annotated preference data</li> <li> <p>Checkpoints aligned with DPO</p> </li> <li>Model supports tool use, which required designing multi-message chat protocol with special header and termination tokens</li> </ul> <h3 id="reward-modeling">Reward Modeling</h3> <ul> <li>RM trained on top of pre-trained checkpoint</li> <li>Preference pairs of either (chosen, rejected) or (chosen, rejected, edited), where edited &gt; chosen &gt; rejected</li> <li>Filtered out preference data with similar responses</li> </ul> <h3 id="supervised-finetuning">Supervised Finetuning</h3> <ul> <li>RM performs rejection sampling on human annotation prompts</li> <li>Fine-tune pre-trained LM on the model-generated samples that are accepted</li> </ul> <h3 id="direct-preference-optimization">Direct Preference Optimization</h3> <ul> <li>Why not on-policy algorithms like PPO? DPO required less compute, performed better on instruction-following benchmarks</li> <li>Used most recent batches of preference data from best-performing models during previous alignment rounds, ensures training data conforms better to distribution of policy model being optimized</li> </ul> <p>Modified DPO:</p> <ul> <li>Masked out formatting tokens (including header and termination tokens) in DPO loss, helps with stability. These tokens caused tail repetition or random termination tokens. Hypothesized due to these tokens being common in both chosen and rejected responses causes conflicting optimization objectives</li> <li>Added regularization with negative log-likelihood (NLL) loss</li> </ul> <h2 id="post-training-data">Post-training Data</h2> <h3 id="preference-data">Preference Data</h3> <ul> <li>Sample two responses from two different models for each user prompt, labelled by human annotators</li> <li>Annotators state strength of preference by 4 levels: significantly better, better, slightly better, marginally better</li> <li>Allow editing step after annotation to further improve response</li> <li>Only used responses significantly better or better for training</li> </ul> <h3 id="sft-data">SFT Data</h3> <p>Finetuning data contains:</p> <ul> <li>Prompts from human annotation collection with rejection-sampled (RS) responses</li> <li>Synthetic data targeting specific capabilities</li> <li>Small amounts of human-curated data</li> </ul> <p>Datasets:</p> <ul> <li>General English</li> <li>Code</li> <li>Multilingual</li> <li>Exam-like</li> <li>Reasoning and tools</li> <li>Long context</li> </ul> <p>Rejection sampling:</p> <ul> <li>Choose prompt from human annotation collection</li> <li>Sample 10-30 outputs from latest chat model policy</li> <li>Use RM to choose best candidate</li> <li>For later rounds of post-training, use system prompt to steer RS responses to conform with tone/style/formatting</li> <li>Uses PagedAttention to make RS efficient</li> </ul> <h3 id="data-processing-and-quality-control">Data Processing and Quality Control</h3> <p>Most of training data is model-generated, requires careful cleaning and quality control</p> <p>Data cleaning:</p> <ul> <li>Rule-based data removal or modification strategies</li> <li>Balance proportion of such samples in dataset</li> </ul> <p>Data pruning:</p> <ul> <li>Topic classification: Fine-tuned Llama 3 8B to a topic classifier</li> <li>Quality scoring: Use RM and Llama 3 checkpoint to rate content, keep examples marked as high quality by either RM or Llama. Both signals have high disagreement rates, and combining signals gives best recall on test set.</li> <li>Difficulty scoring: used Llama 3 70B to perform intention-tagging, where more intentions implies more complexity. Also used it to measure difficulty of dialogs</li> <li>Semantic deduplication: clustering using RoBERTa, sort by quality score \(\times\) difficulty score, go through sorted examples by best and take only ones with maximum cosine similarity less than threshold</li> </ul> <h2 id="capabilities">Capabilities</h2> <h3 id="code">Code</h3> <p>Capabilities:</p> <ul> <li>Code generation</li> <li>Documentation</li> <li>Debugging</li> <li>Review</li> </ul> <p>Targeted languages: Python, Java, Javascript, C/C++, Typescript, Rust, PHP, HTML/CSS, SQL, bash/shell</p> <p>Improved capabilities via:</p> <ul> <li>Training a code expert</li> <li>Generate synthetic data for SFT</li> <li>Improve formatting with system prompt steering</li> <li>Create quality filters to remove bad examples</li> </ul> <p>Expert training:</p> <ul> <li>Train code expert to obtain high quality human annotations for code</li> <li>Approach similar to CodeLlama (scant on details, should probably check this paper)</li> </ul> <p>Synthetic data generation:</p> <ul> <li>Faced issues in code generation: following instructions, code syntax errors, incorrect code generation, difficulty in fixing bugs</li> <li>Use Llama 3 and code expert to generate synthetic 2.7M dialogs for SFT</li> </ul> <p>During RS, used code specific system prompts to improve:</p> <ul> <li>code readability</li> <li>documentation</li> <li>thoroughness</li> <li>specificity</li> </ul> <h4 id="synthetic-data-generation-execution-feedback">Synthetic data generation: execution feedback</h4> <ul> <li>Distillation to smaller models helped, but not for 405B on its own inputs</li> <li>Use execution feedback as source of truth, allow model to learn from own mistakes <ol> <li>Problem description generation: generate programming problem descriptions, use random code snippets as inspiration</li> <li>Solution generation: Prompt Llama 3 to solve problem, use CoT in comments, add programming guidelines in system prompt</li> <li>Correctness analysis: use static analysis (parser and linters), and unit test generation (also by the model) and execution</li> <li>Error feedback and iterative self-correction: prompt model to revise solutions that fail, includes feedback from parser/linter/tester. Can modify code and unit test to accomodate new code. 20% of solutions that were incorrect could be self-corrected this way.</li> <li>Fine-tuning and iterative improvement: process iterated over multiple rounds, higher-quality synthetic data generated in each subsequent rounds</li> </ol> </li> </ul> <h4 id="synthetic-data-generation-programming-language-translation">Synthetic data generation: programming language translation</h4> <ul> <li>Noted performance gap between popular vs less common programming languages, due to difference in dataset size</li> <li>Translate data from more common to less common languages</li> <li>Ensure quality via syntax parsing, compilation, execution</li> </ul> <h4 id="synthetic-data-generation-backtranslation">Synthetic data generation: backtranslation</h4> <ul> <li>Some coding capabilities don’t benefit as much from execution feedback, i.e documentation &amp; explanation</li> <li>Generated 1.2M synthetic dialogs for code explanation, generation, documentation, debugging</li> <li>Done as follows: <ol> <li>Generate: Prompt Llama 3 to generate data corresponding to desired capability (i.e add comments to code)</li> <li>Backtranslate: Ask model to backtranslate synthetically generated data to original code (i.e generate code based on only comments)</li> <li>Filter: ask Llama 3 to determine quality of generated code with original code as reference. This self-verification step acts as a filter for good examples, only those with high scores are used for SFT</li> </ol> </li> </ul> <h3 id="tool-use">Tool Use</h3> <p>Trained Llama 3 to use search engine (Brave), Python interpreter, Wolfram Alpha API.</p> <p>To train on tool use:</p> <ul> <li>Start with training single-turn tool use, then tool use in dialog, and then multi-step tool use and data analysis</li> <li>All synthetically generated: first synthetic user prompts which require calling out to tools, then the corresponding tool calls which are then executed, and then the final answer to user prompt</li> <li>Multi-step tool use trained in a similar way synthetically</li> <li>User prompts are based on a provided file, and ask to summarize the contents of the file, find and fix bugs, optimize a piece of code, perform data analysis or visualization</li> <li>Augmented synthetic data with different system prompts to teach model to use tools only when activated</li> <li>To avoid model using tools for simple queries, added dataset containing queries of simple math/reasoning questions with tool use activated but without using tools in response</li> </ul> <h3 id="factuality">Factuality</h3> <p>To train the model to guard against hallucinations, they used a knowledge probe to find out what the model knows, and to generate training data of refusals for the things it doesn’t:</p> <ol> <li>Extract a data snippet from the pre-training data.</li> <li>Generate a factual question about these snippets (context) by prompting Llama 3.</li> <li>Sample responses from Llama 3 to the question.</li> <li>Score the correctness of the generations using the original context as a reference and Llama 3 as a judge.</li> <li>Score the informativeness of the generations using Llama 3 as a judge.</li> <li>Generate a refusal for responses which are consistently informative and incorrect across the generations, using Llama 3.</li> </ol> <p>But because pre-training data is not always factually correct, they also did this for sensitive topics where contradictory/incorrect statements are prevalent</p> <h3 id="steerability">Steerability</h3> <p>Remainder to be continued…</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">Related Posts:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/gaussian-processes/">An Intuitive Introduction to Gaussian Processes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/bounding-markov-chain-mixing-times-by-spectral-gap/">Bounding Mixing Times of Markov Chains via the Spectral Gap</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/setting-up-yuancon-controller-sound-voltex/">Playing Sound Voltex at Home: Setting Up Unnamed SDVX Clone with the Yuancon SDVX Controller</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/creating-trackback-requests/">Creating Trackback Requests for Static Sites</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/high-dimensional-analysis-of-m-estimators/">A Unified Framework for High-Dimensional Analysis of M-Estimators with Decomposable Regularizers: A Guided Walkthrough</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'fanpu/fanpu.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Fan Pu Zeng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: January 22, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?5fdb75708acb79bd79b0cacb0ed0237d"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-S3VHEYH05S"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-S3VHEYH05S');
  </script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>