<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Bounding Mixing Times of Markov Chains via the Spectral Gap | Fan Pu  Zeng</title>
    <meta name="author" content="Fan Pu  Zeng">
    <meta name="description" content="">
    <meta name="keywords" content="fanpu, fan pu, fanpu zeng, fan pu zeng, zengfanpu, fanpuzeng, fzeng, CMU, cmu, carnegie mellon, cmu courses, school of computer science, scs, machine learning, computer science, ml, theory, courses, course reviews, CS, Jane Street">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    
    <!-- Sidebar Table of Contents -->
    <link href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css" rel="stylesheet">
    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon_new.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fanpu.io/blog/2022/bounding-mixing-time-by-spectral-gap/">
    <!-- Dark Mode -->
    

    <!-- Twitter cards -->
    <meta name="twitter:site" content="@FanPu_Zeng">
    <meta name="twitter:creator" content="@fanpu">
    <meta name="og:title" content="Bounding Mixing Times of Markov Chains via the Spectral Gap">

    
    <meta name="twitter:card" content="summary_large_image">
    
    <meta name="og:image" content="https://fanpu.io/assets/img/posts/fallingwater.webp">

    

    
    <meta name="og:description" content="">
    

    <!-- end of Twitter cards -->
  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Fan PuÂ </span>Zeng</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/courses/">CMU Course Reviews</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cmu-online/">CMU Online</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/summaries/">ML Paper Summaries</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">Repositories</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        
        <div class="row">
          <!-- sidebar, which will move to the top on a small screen -->
          <div class="col-sm-3">
            <nav id="toc-sidebar" class="sticky-top"></nav>
          </div>
          <!-- main content area -->
          <div class="col-sm-9">
            <!-- _layouts/post.html -->

<div class="post">

  <div style="display:none">
    $$
    \newcommand{\bone}{\mathbf{1}}
    \newcommand{\bbeta}{\mathbf{\beta}}
    \newcommand{\bdelta}{\mathbf{\delta}}
    \newcommand{\bepsilon}{\mathbf{\epsilon}}
    \newcommand{\blambda}{\mathbf{\lambda}}
    \newcommand{\bomega}{\mathbf{\omega}}
    \newcommand{\bpi}{\mathbf{\pi}}
    \newcommand{\bphi}{\mathbf{\phi}}
    \newcommand{\bvphi}{\mathbf{\varphi}}
    \newcommand{\bpsi}{\mathbf{\psi}}
    \newcommand{\bsigma}{\mathbf{\sigma}}
    \newcommand{\btheta}{\mathbf{\theta}}
    \newcommand{\btau}{\mathbf{\tau}}
    \newcommand{\ba}{\mathbf{a}}
    \newcommand{\bb}{\mathbf{b}}
    \newcommand{\bc}{\mathbf{c}}
    \newcommand{\bd}{\mathbf{d}}
    \newcommand{\be}{\mathbf{e}}
    \newcommand{\boldf}{\mathbf{f}}
    \newcommand{\bg}{\mathbf{g}}
    \newcommand{\bh}{\mathbf{h}}
    \newcommand{\bi}{\mathbf{i}}
    \newcommand{\bj}{\mathbf{j}}
    \newcommand{\bk}{\mathbf{k}}
    \newcommand{\bell}{\mathbf{\ell}}
    \newcommand{\bm}{\mathbf{m}}
    \newcommand{\bn}{\mathbf{n}}
    \newcommand{\bo}{\mathbf{o}}
    \newcommand{\bp}{\mathbf{p}}
    \newcommand{\bq}{\mathbf{q}}
    \newcommand{\br}{\mathbf{r}}
    \newcommand{\bs}{\mathbf{s}}
    \newcommand{\bt}{\mathbf{t}}
    \newcommand{\bu}{\mathbf{u}}
    \newcommand{\bv}{\mathbf{v}}
    \newcommand{\bw}{\mathbf{w}}
    \newcommand{\bx}{\mathbf{x}}
    \newcommand{\by}{\mathbf{y}}
    \newcommand{\bz}{\mathbf{z}}
    \newcommand{\bA}{\mathbf{A}}
    \newcommand{\bB}{\mathbf{B}}
    \newcommand{\bC}{\mathbf{C}}
    \newcommand{\bD}{\mathbf{D}}
    \newcommand{\bE}{\mathbf{E}}
    \newcommand{\bF}{\mathbf{F}}
    \newcommand{\bG}{\mathbf{G}}
    \newcommand{\bH}{\mathbf{H}}
    \newcommand{\bI}{\mathbf{I}}
    \newcommand{\bJ}{\mathbf{J}}
    \newcommand{\bK}{\mathbf{K}}
    \newcommand{\bL}{\mathbf{L}}
    \newcommand{\bM}{\mathbf{M}}
    \newcommand{\bN}{\mathbf{N}}
    \newcommand{\bP}{\mathbf{P}}
    \newcommand{\bQ}{\mathbf{Q}}
    \newcommand{\bR}{\mathbf{R}}
    \newcommand{\bS}{\mathbf{S}}
    \newcommand{\bT}{\mathbf{T}}
    \newcommand{\bU}{\mathbf{U}}
    \newcommand{\bV}{\mathbf{V}}
    \newcommand{\bW}{\mathbf{W}}
    \newcommand{\bX}{\mathbf{X}}
    \newcommand{\bY}{\mathbf{Y}}
    \newcommand{\bZ}{\mathbf{Z}}

    \newcommand{\bsa}{\boldsymbol{a}}
    \newcommand{\bsb}{\boldsymbol{b}}
    \newcommand{\bsc}{\boldsymbol{c}}
    \newcommand{\bsd}{\boldsymbol{d}}
    \newcommand{\bse}{\boldsymbol{e}}
    \newcommand{\bsoldf}{\boldsymbol{f}}
    \newcommand{\bsg}{\boldsymbol{g}}
    \newcommand{\bsh}{\boldsymbol{h}}
    \newcommand{\bsi}{\boldsymbol{i}}
    \newcommand{\bsj}{\boldsymbol{j}}
    \newcommand{\bsk}{\boldsymbol{k}}
    \newcommand{\bsell}{\boldsymbol{\ell}}
    \newcommand{\bsm}{\boldsymbol{m}}
    \newcommand{\bsn}{\boldsymbol{n}}
    \newcommand{\bso}{\boldsymbol{o}}
    \newcommand{\bsp}{\boldsymbol{p}}
    \newcommand{\bsq}{\boldsymbol{q}}
    \newcommand{\bsr}{\boldsymbol{r}}
    \newcommand{\bss}{\boldsymbol{s}}
    \newcommand{\bst}{\boldsymbol{t}}
    \newcommand{\bsu}{\boldsymbol{u}}
    \newcommand{\bsv}{\boldsymbol{v}}
    \newcommand{\bsw}{\boldsymbol{w}}
    \newcommand{\bsx}{\boldsymbol{x}}
    \newcommand{\bsy}{\boldsymbol{y}}
    \newcommand{\bsz}{\boldsymbol{z}}
    \newcommand{\bsA}{\boldsymbol{A}}
    \newcommand{\bsB}{\boldsymbol{B}}
    \newcommand{\bsC}{\boldsymbol{C}}
    \newcommand{\bsD}{\boldsymbol{D}}
    \newcommand{\bsE}{\boldsymbol{E}}
    \newcommand{\bsF}{\boldsymbol{F}}
    \newcommand{\bsG}{\boldsymbol{G}}
    \newcommand{\bsH}{\boldsymbol{H}}
    \newcommand{\bsI}{\boldsymbol{I}}
    \newcommand{\bsJ}{\boldsymbol{J}}
    \newcommand{\bsK}{\boldsymbol{K}}
    \newcommand{\bsL}{\boldsymbol{L}}
    \newcommand{\bsM}{\boldsymbol{M}}
    \newcommand{\bsN}{\boldsymbol{N}}
    \newcommand{\bsP}{\boldsymbol{P}}
    \newcommand{\bsQ}{\boldsymbol{Q}}
    \newcommand{\bsR}{\boldsymbol{R}}
    \newcommand{\bsS}{\boldsymbol{S}}
    \newcommand{\bsT}{\boldsymbol{T}}
    \newcommand{\bsU}{\boldsymbol{U}}
    \newcommand{\bsV}{\boldsymbol{V}}
    \newcommand{\bsW}{\boldsymbol{W}}
    \newcommand{\bsX}{\boldsymbol{X}}
    \newcommand{\bsY}{\boldsymbol{Y}}
    \newcommand{\bsZ}{\boldsymbol{Z}}

    \newcommand{\calA}{\mathcal{A}}
    \newcommand{\calB}{\mathcal{B}}
    \newcommand{\calC}{\mathcal{C}}
    \newcommand{\calD}{\mathcal{D}}
    \newcommand{\calE}{\mathcal{E}}
    \newcommand{\calF}{\mathcal{F}}
    \newcommand{\calG}{\mathcal{G}}
    \newcommand{\calH}{\mathcal{H}}
    \newcommand{\calI}{\mathcal{I}}
    \newcommand{\calJ}{\mathcal{J}}
    \newcommand{\calK}{\mathcal{K}}
    \newcommand{\calL}{\mathcal{L}}
    \newcommand{\calM}{\mathcal{M}}
    \newcommand{\calN}{\mathcal{N}}
    \newcommand{\calO}{\mathcal{O}}
    \newcommand{\calP}{\mathcal{P}}
    \newcommand{\calQ}{\mathcal{Q}}
    \newcommand{\calR}{\mathcal{R}}
    \newcommand{\calS}{\mathcal{S}}
    \newcommand{\calT}{\mathcal{T}}
    \newcommand{\calU}{\mathcal{U}}
    \newcommand{\calV}{\mathcal{V}}
    \newcommand{\calW}{\mathcal{W}}
    \newcommand{\calX}{\mathcal{X}}
    \newcommand{\calY}{\mathcal{Y}}
    \newcommand{\calZ}{\mathcal{Z}}

    \newcommand{\R}{\mathbb{R}}
    \newcommand{\C}{\mathbb{C}}
    \newcommand{\N}{\mathbb{N}}
    \newcommand{\Z}{\mathbb{Z}}
    \newcommand{\F}{\mathbb{F}}
    \newcommand{\Q}{\mathbb{Q}}

    \DeclareMathOperator*{\argmax}{arg\,max}
    \DeclareMathOperator*{\argmin}{arg\,min}
    \newcommand{\nnz}[1]{\mbox{nnz}(#1)}
    \newcommand{\dotprod}[2]{\langle #1, #2 \rangle}

    \newcommand{\ignore}[1]{}

    \let\Pr\relax
    \DeclareMathOperator*{\Pr}{\mathbf{Pr}}
    \newcommand{\E}{\mathbb{E}}
    \DeclareMathOperator*{\Ex}{\mathbf{E}}
    \DeclareMathOperator*{\Var}{\mathbf{Var}}
    \DeclareMathOperator*{\Cov}{\mathbf{Cov}}
    \DeclareMathOperator*{\stddev}{\mathbf{stddev}}
    \DeclareMathOperator*{\avg}{avg}

    \DeclareMathOperator{\poly}{poly}
    \DeclareMathOperator{\polylog}{polylog}
    \DeclareMathOperator{\size}{size}
    \DeclareMathOperator{\sgn}{sgn}
    \DeclareMathOperator{\dist}{dist}
    \DeclareMathOperator{\vol}{vol}
    \DeclareMathOperator{\spn}{span}
    \DeclareMathOperator{\supp}{supp}
    \DeclareMathOperator{\tr}{tr}
    \DeclareMathOperator{\Tr}{Tr}
    \DeclareMathOperator{\codim}{codim}
    \DeclareMathOperator{\diag}{diag}

    \newcommand{\PTIME}{\mathsf{P}}
    \newcommand{\LOGSPACE}{\mathsf{L}}
    \newcommand{\ZPP}{\mathsf{ZPP}}
    \newcommand{\RP}{\mathsf{RP}}
    \newcommand{\BPP}{\mathsf{BPP}}
    \newcommand{\P}{\mathsf{P}}
    \newcommand{\NP}{\mathsf{NP}}
    \newcommand{\TC}{\mathsf{TC}}
    \newcommand{\AC}{\mathsf{AC}}
    \newcommand{\SC}{\mathsf{SC}}
    \newcommand{\SZK}{\mathsf{SZK}}
    \newcommand{\AM}{\mathsf{AM}}
    \newcommand{\IP}{\mathsf{IP}}
    \newcommand{\PSPACE}{\mathsf{PSPACE}}
    \newcommand{\EXP}{\mathsf{EXP}}
    \newcommand{\MIP}{\mathsf{MIP}}
    \newcommand{\NEXP}{\mathsf{NEXP}}
    \newcommand{\BQP}{\mathsf{BQP}}
    \newcommand{\distP}{\mathsf{dist\textbf{P}}}
    \newcommand{\distNP}{\mathsf{dist\textbf{NP}}}

    \newcommand{\eps}{\epsilon}
    \newcommand{\lam}{\lambda}
    \newcommand{\dleta}{\delta}
    \newcommand{\simga}{\sigma}
    \newcommand{\vphi}{\varphi}
    \newcommand{\la}{\langle}
    \newcommand{\ra}{\rangle}
    \newcommand{\wt}[1]{\widetilde{#1}}
    \newcommand{\wh}[1]{\widehat{#1}}
    \newcommand{\ol}[1]{\overline{#1}}
    \newcommand{\ul}[1]{\underline{#1}}
    \newcommand{\ot}{\otimes}
    \newcommand{\zo}{\{0,1\}}
    \newcommand{\co}{:} %\newcommand{\co}{\colon}
    \newcommand{\bdry}{\partial}
    \newcommand{\grad}{\nabla}
    \newcommand{\transp}{^\intercal}
    \newcommand{\inv}{^{-1}}
    \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff}
    \newcommand{\half}{\tfrac{1}{2}}
    \newcommand{\bbone}{\mathbbm 1}
    \newcommand{\Id}{\bbone}

    \newcommand{\SAT}{\mathsf{SAT}}

    \newcommand{\bcalG}{\boldsymbol{\calG}}
    \newcommand{\calbG}{\bcalG}
    \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX}
    \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY}
    \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ}
    $$
</div>

  <header class="post-header">
    <h1 class="post-title">Bounding Mixing Times of Markov Chains via the Spectral Gap</h1>
    <p class="post-meta">September 27, 2022â¢ fanpu</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
      Â  Â· Â 
        <a href="/blog/tag/math">
          <i class="fas fa-hashtag fa-sm"></i> math</a> Â 
          <a href="/blog/tag/machine-learning">
          <i class="fas fa-hashtag fa-sm"></i> machine-learning</a> Â 
          

    </p>
  </header>

  
      <figure>
  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/fallingwater.webp" class="preview z-depth-1 rounded center" width="100%" height="450px" alt="post.cover" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>
</figure>

    <div class="caption">
        Fallingwater, designed by Frank Lloyd Wright. Laurel Highlands, Pennsylvania, USA
    </div>
  

  <article class="post-content">
    
    <div id="markdown-content">
      <p>A Markov chain that is aperiodic and irreducible will eventually converge to a
stationary distribution. This is widely used in many applications in machine
learning, such as in Markov Chain Monte Carlo (MCMC) methods,
where random walks on Markov chains are used to obtain a good estimate of the
log likelihood of the partition function of a model, which is hard to compute
directly as it is #P-hard (this is even harder than NP-hardness).
However, one common issue is that it is unclear how many steps we should take
before we are guaranteed that the Markov chain has converged to the true
stationary distribution. In this post, we will see how the spectral gap
of the</p>

<p>\begin{definition}[Total Variation Distance]
    Let $\mathcal{D}<em>1, \mathcal{D}_2$ be distributions on $\Omega$.
    Then
    \begin{align}
        | \mathcal{D}_1 - \mathcal{D}_2 |</em>{TV}
        = &amp; \frac{1}{2}
        \sum\limits_{\omega \in \Omega} \Big| \mathcal{D}<em>1(\omega) -
        \mathcal{D}_2(\omega) \Big|                              <br>
        = &amp; \max</em>{A \subseteq \Omega} \sum\limits_{\omega \in A}
        \mathcal{D}<em>1(\omega) - \sum\limits</em>{\omega \in A} \mathcal{D}_2(\omega).
    \end{align}
\end{definition}</p>

<p>\lecture{4: Markov Chains, Random Walks, Applications (Part 2)}{September 27, 2022}{Lecturer: Pravesh K. Kothari}{Fan Pu Zeng}</p>

<p>\section{Mixing Times}</p>

<p>We now try to develop methods to understand how long it takes to approximate the
stationary distribution $\pi$ of a Markov Chain. Our goal is to eventually show
that the mixing time is in $O\left(\frac{\log (n)}{1 - \beta}\right)$, where
$\beta$ is the second largest eigenvalue of the transition matrix of the Markov
Chain.</p>

<p>\section{Coupling}
Coupling is one general technique that allows us to bound how long it takes for
a Markov Chain to converge to its stationary distribution based. It is based on
having two copies of the original Markov Chain running simultaneously, with one
being at stationarity, and showing how they can be made to coincide (i.e have
bounded variation distance) after some time (known as the ``coupling timeââ).</p>

<p>We will not discuss coupling in this class, but will instead develop how
spectral gaps can be used, as this is more useful for future classes.</p>

<p>\section{The <code class="language-plaintext highlighter-rouge">Spectral Gap'' Method}
The main idea of the</code>Spectral Gapââ method is that the mixing time is bounded by the inverse of the spectral
gap, which is the difference between the largest and second largest eigenvalues
of the transition matrix.</p>

<p>Before we can talk about one distribution approximating another, we need to
first introduce what ``closenessââ between two distributions means
The formulation that we will use is via the Total Variation Distance.</p>

<p>\begin{definition}[Total Variation Distance]
    Let $\mathcal{D}<em>1, \mathcal{D}_2$ be distributions on $\Omega$.
    Then
    \begin{align}
        | \mathcal{D}_1 - \mathcal{D}_2 |</em>{TV}
        = &amp; \frac{1}{2}
        \sum\limits_{\omega \in \Omega} \Big| \mathcal{D}<em>1(\omega) -
        \mathcal{D}_2(\omega) \Big|                              <br>
        = &amp; \max</em>{A \subseteq \Omega} \sum\limits_{\omega \in A}
        \mathcal{D}<em>1(\omega) - \sum\limits</em>{\omega \in A} \mathcal{D}_2(\omega).
    \end{align}
\end{definition}</p>

<p>The equality between the two lines can be observed from the fact that
\begin{equation}
    \max_{A \subseteq \Omega} \sum\limits_{\omega \in A}
    \mathcal{D}<em>1(\omega) - \sum\limits</em>{\omega \in A} \mathcal{D}<em>2(\omega)=
    \max</em>{B \subseteq \Omega} \sum\limits_{\omega \in B}
    \mathcal{D}<em>2(\omega) - \sum\limits</em>{\omega \in B} \mathcal{D}_1(\omega),
\end{equation}
since both $\mathcal{D}_1, \mathcal{D}_2$ are probability distributions and integrate to 1. See Figure \ref{fig:tv}
for an illustration.</p>

<p>\begin{figure}[h]
    \label{fig:tv}
    \centering
    \includegraphics[width=0.7\textwidth]{tv.webp}
    \caption{Total Variation distance between some sample $\mathcal{D}_1, \mathcal{D}_2$ illustrated by the sum of the shaded green regions.}
\end{figure}</p>

<p>\section{Intuition for Mixing Times}
We consider how long it takes to converge on some special graphs to build up intuition.</p>

<p>\subsection{Random Walks on Path Graphs}
The path graph is a line graph on $n$ vertices.
We claim that the mixing time of the path graph is at least $n$:
this is because it takes at least $n$ steps to even reach the rightmost vertex from the leftmost vertex.</p>

<p>\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \node[circle,fill=blue!20,draw, minimum size=1cm] (1) {};
        \node[circle,fill=blue!20,draw, minimum size=1cm] (2) [right = 2cm of 1] {};
        \node[circle,fill=blue!20,draw, minimum size=1cm] (3) [right = 2cm of 2] {};
        \node[circle,fill=blue!20,draw, minimum size=1cm] (4) [right = 2cm of 3] {};</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    \path[draw,thick]
    (1) edge node {} (2)
    (2) edge node {} (3)
    (3) edge node {} (4);
\end{tikzpicture}
\caption{The path graph, $n=4$.} \end{figure}
</code></pre></div></div>

<p>\subsection{Random Walks on the Complete Graph}
The complete graph $K_n$ on $n$ vertices is one where each vertex has an edge to every other vertex.</p>

<p>This only takes 1 step to mix, since after a single step we can reach any vertex.</p>

<p>\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \graph[circular placement, radius=3cm,
            empty nodes, nodes={circle,fill=blue!17,draw, minimum size=0.8cm, inner sep=0pt}] {
            \foreach \x in {a,â¦,f} {
                    \foreach \y in {\x,â¦,f} {
                            \x â \y;
                        };
                };
        };
        \foreach \x [count=\idx from 0] in {a,â¦,f} {
                \pgfmathparse{90 + \idx * (360 / 6)}
                \node at (\pgfmathresult:4cm) {};
            };
    \end{tikzpicture}
    \caption{The complete graph $K_6$}
\end{figure}</p>

<p>This short analysis tells us that if our graph looks like a line graph then we should expect poor mixing times; whereas if it looks more like a complete graph then we can expect the opposite.</p>

<p>\newcommand{\tmix}{\tau_{\mathsf{mix}}}
\newcommand{\sgap}[1]{\mathsf{spectral_gap}(#1)}</p>

<p>\section{Mixing Times}
We now formally introduce the concept of mixing times.</p>

<p>\begin{definition}[Mixing Time]
    Let $\left{  X_t \right}$ be a finite, irreducible, aperiodic Markov
    Chain, $\pi$ be the stationary distribution, and $T$ to be the transition matrix. Then define
    \begin{equation}
        \Delta(t) = \max_{\omega \in \Omega} | \pi - T_\omega^t |<em>{TV},
    \end{equation}
    where $T</em>\omega^t$ is the distribution of $X_t$ given $X_o = \omega$.
    In words, $\Delta(t)$ is the maximum time to converge to stationary
    distribution over all the starting points, where convergence is defined on
    total variation distance.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Then the mixing time $\tmix$ is defined to be the smallest $t$ such that $\Delta(t) \leq \frac{1}{4}$. \end{definition}
</code></pre></div></div>

<p>We claim that the choice of $\frac{1}{4}$ in defining $\tmix$ does not matter.</p>

<p>\begin{proposition}[Constants Donât Matter]
    The choice of constant $\frac{1}{4}$ does not matter.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>\begin{proof}
    This is because for all $c \geq 1$, $\Delta(c \cdot \tmix) \leq \frac{1}{4^c}$. In other words, we can increase
    the mixing time by a linear amount to get an exponential decrease in total variation distance.
\end{proof} \end{proposition}
</code></pre></div></div>

<p>To bound mixing times, we consider random walks on undirected, regular graphs $G$. The same analysis can be extended to
directed, weighted, irregular graphs, but it causes the notation to become more
cumbersome and distracts from the key ideas.</p>

<p>Consider random walks on an undirected, regular graph $G(V, E)$, $|V| = n$.
Define the transition matrix $T$ of the graph to be</p>

<p>\begin{equation}
    T =
    \begin{blockarray}{cccccc}
        &amp;   &amp;  &amp; j  &amp; &amp;<br>
        \begin{block}{c[ccccc]}
            &amp; &amp; &amp; &amp; &amp;   <br>
            &amp; &amp; &amp; &amp; &amp;   <br>
            &amp; &amp; &amp; &amp; &amp;   <br>
            i &amp; &amp; &amp;
            \begin{cases}
                \frac{1}{\deg(j)} &amp; \text{if $j \sim i$} <br>
                0                 &amp; \text{ otherwise}
            \end{cases}
            &amp; &amp;   <br>
            &amp; &amp; &amp; &amp; &amp;   <br>
            &amp; &amp; &amp; &amp; &amp;   <br>
            &amp; &amp; &amp; &amp; &amp;   <br>
        \end{block}
    \end{blockarray},
\end{equation}</p>

<p>where $j \sim i$ means that $j$ shares an edge with $i$.</p>

<p>The stationary distribution for $T$ is given by
\begin{equation}
    \pi = \left(  \frac{\deg (1)}{2|E|} , \dots, \frac{\deg (n)}{2|E|}  \right).
\end{equation}
This can be seen from the following:
\begin{align}
    (T \pi)<em>i &amp; =
    \sum\limits</em>{j \in [n]} \frac{\deg (j)}{2 |E| } \mathbbm{1}
    \begin{rcases}
        \begin{dcases}
            \frac{1}{\deg(j)} &amp; \text{ if $j \sim i$}, <br>
            0                 &amp; \text{ otherwise. }    <br>
        \end{dcases}
    \end{rcases}                 <br>
              &amp; = \sum\limits_{j \sim i}
    \frac{\deg(j)}{2 |E| } \frac{1}{\deg (j)} <br>
              &amp; = \frac{
        \deg (i)
    }{2|E|}.
\end{align}</p>

<p>If $G$ is $d$-regular, then</p>

<p>\begin{equation}
    T = \frac{1}{d} \cdot A,
\end{equation}
where $A$ is the adjacency matrix of the graph.</p>

<p>\section{Spectral Graph Theory}
Spectral graph theory is the study of how the eigenvalues and eigenvectors of
the matrix of a graph reveals certain properties about the graph, for instance,
how well-connected it is.</p>

<p>\begin{lemma}[Properties of the Adjacency Matrix of a $d$-regular Graph]\label{laplacian-prop}
    Let $T = \frac{1}{d} A$.
    Let ${\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n}$ to be the eigenvalues of $T$.
    Then the following properties hold:
    \begin{enumerate}
        \item $|\lambda_i| \leq 1$ for all $i$, and $\lambda_1 = 1$</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    \item $\lambda_2 &lt; 1$ if and only if $G$ is connected

    \item $\lambda_n &gt; -1$ if and only if $G$ does not have a bipartite connected component
\end{enumerate} \end{lemma}
</code></pre></div></div>

<p>We prove each of the claims in Lemma \ref{laplacian-prop} in order.</p>

<p>Claim 1: $|\lambda_i| \leq 1$ for all $i$, and $\lambda_1 = 1$.
\begin{proof}</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Choose any eigenvector $v$.
Let $v_i$ be the maximum magnitude entry of $v$.  Observe that $v$
is an eigenvector of $T$ only if $Tv = \lambda
    v$ for some $\lambda$.  Then
\begin{align}
    \lambda v_i \label{eq:max_entry}
     &amp; = (Tv)_i                                                                                        \\
     &amp; = \sum\limits_{j \in N(i)} \frac{1}{d} \cdot v_j &amp; \text{(Multiplying $i$th row of $T$ by $v$)} \\
     &amp; \leq | v_i |
\end{align}
The last step comes from the fact that since each $|v_j| \leq
    |v_i|$, so at most we have $d \times \frac{1}{d}|v_i| = |v_i|$,
recalling that $|N(i)| = d$ since the graph is $d$-regular.

This shows that $|\lambda v_i| \leq |v_i|$ for all $i$, and so $|\lambda| \leq 1$.

It remains to show that $\lambda_1=1$. To see this, consider
the vectors where all entries are 1, i.e $\mathbbm{1}$.  Then $T
    \cdot \mathbbm{1} = \mathbbm{1}$. So $\mathbbm{1}$ is an
eigenvector of $T$ with eigenvalue 1. \qedhere \end{proof}
</code></pre></div></div>

<p>Claim 2: $\lambda_2 &lt; 1$ if and only if $G$ is connected.
\begin{proof}
    $(\Longleftarrow)$ Suppose that $G$ is disconnected, we show that its second largest eigenvalue $\lambda_2$ is 1.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WLOG, assume that the graph has two distinct connected components; the proof
easily extends to having more components.
Let $S_1, S_2$ be connected components of $G$. Recall that the
connected components of $G$ are the equivalence class of components where
in each component, all vertices are reachable from any other vertex.

Define $v^1, v^2$ via
\begin{align*}
    v^1_i =
    \begin{cases}
        1 &amp; \text{if $i \in S_1$,} \\
        0 &amp; \text{otherwise,}      \\
    \end{cases} \\
    v^2_i =
    \begin{cases}
        1 &amp; \text{if $i \in S_2$,} \\
        0 &amp; \text{otherwise.}      \\
    \end{cases} \\
\end{align*}

Then
\begin{align}
    (T \cdot v^1)_i
     &amp; = \sum\limits_{j \in N(i)} \frac{1}{d} v^1_j                                  &amp; \text{(multiplying row $i$ of $T$ by $v^1$)} \\
     &amp; = \sum\limits_{j \in N(i)} \frac{1}{d} \mathbbm{1} \left\{ j \in S_1 \right\}                                                \\
     &amp; = \begin{cases}
             1 &amp; \text{if $i \in S_1$,} \\
             0 &amp; \text{otherwise.}      \\
         \end{cases}
\end{align}
This shows that $T \cdot v^1 = v^1$. Similarly, we can perform the same
sequence of steps to derive that $T \cdot v^2 = v^2$.


We can show the same for $v^2$ to get $T \cdot v^2 = v^2$. which shows that $\lambda_2 = 1$.
Since by our disconnected assumption $v^1, v^2 \neq \mathbbm{1}$, the
all-ones eigenvector corresponding to eigenvalue $\lambda_1$, it means $\lambda_2 = 1$.
This shows the backwards direction.

$(\implies)$ For the other direction, suppose that $G$ is connected, we want to show that $\lambda_2 &lt; 1$.

We will show that for any eigenvector $v$ with eigenvalue $1$, then it must be a scaling of $\mathbbm{1}$.

Let $v$ be any eigenvector with eigenvalue $1$. Then let $v_i$ be its maximum entry. From Equation \ref{eq:max_entry}, we must have that
\begin{align}
    \lambda v_i
     &amp; = v_i                                            \\
     &amp; = (Tv)_i                                         \\
     &amp; = \sum\limits_{j \in N(i)} \frac{1}{d} \cdot v_j \\
     &amp; = v_i.
\end{align}
But since $v_i$ is the largest entry, it must be the case that $v_j = v_i$
for all $j \sim i$.  We then repeat this argument to observe that all the
neighbors of each $j$ must also take on the same value. Since the graph is
connected, $v$ is just the uniform vector, as desired.

Note that this lemma shows that if $G$ is disconnected, then it has a spectral gap of 0. \end{proof}
</code></pre></div></div>

<p>Claim 3: $\lambda_n &gt; -1$ if and only if $G$ does not have a bipartite connected component
\begin{proof}
    $(\implies)$
    We show the forward drection by contraposition.
    Suppose that $G$ has a bipartite component $S$. We want to show that $\lambda_n = -1$.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let $S = L \cup R$ denote the two disjoint bipartite components.

Define vector
\begin{equation}
    v_i = \begin{cases}
        1  &amp; \text{if $i \in L$,} \\
        -1 &amp; \text{if $i \in R$,} \\
        0  &amp; \text{otherwise.}    \\
    \end{cases}
\end{equation}

Again we compute $T \cdot v$, and consider its $i$th entry:
\begin{align}
    \left( T \cdot v \right)_i
     &amp; = \sum\limits_{j \in N(i)} \frac{1}{d} v_j \\
     &amp; = -v_i,
\end{align}
since the signs of its neighbors $N(i)$ are always the opposite of the sign of $v_i$ by construction.

Since $Tv = -v$, this shows that we have an eigenvector with eigenvalue $-1$.

$(\Longleftarrow)$ Now suppose that $Tv = -v$, with the goal to show that
the graph is bipartite.  Similarly as for the backwards direction of Claim
2, we can see that this can only hold on each element $v_i$ if all the signs
of the neighbors of $v_i$ have the same magnitude but opposite sign of
$v_i$. Then we can similarly expand this argument to the neighbors of its
neighbors, which shows that the graph is bipartite. \end{proof}
</code></pre></div></div>

<p>This shows how we can gleam useful information about a graph just from its eigenvalues.</p>

<p>Recall how we previously showed that a unique stationary distribution exists if the graph is connected and not bipartite. Now we have another characterization of the same property, except in terms of the eigenvalues of its
transition matrix:</p>

<p>\begin{corollary}[Corollary of the Fundamental Theorem]
    If $T$ is such that $\lambda_2 &lt; 1$, $\lambda_n &gt; -1$ then the random walk
    has a unique stationary distribution which is uniform.
\end{corollary}
Our goal now is to formulate a robust version of this corollary, where we can bound the mixing time of approaching the stationary distribution.</p>

<p>\section{Bounding the Mixing Time via the Spectral Gap}
We define the spectral gap:
\begin{definition}[Spectral Gap]
    Given $T$, define
    \begin{equation}
        beta = \max\left{ \lambda_2, | \lambda_n | \right} = \max_{2 \leq i \leq n} |\lambda_i|.
    \end{equation}</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Then the spectral gap is given by
\begin{equation}
    \sgap{T} = 1 - \beta.
\end{equation} \end{definition}
</code></pre></div></div>

<p>We now finally introduce a lemma that shows that the mixing time is proportional to the inverse of the spectral gap multiplied by a log factor:
\begin{lemma}\label{mixing}
    Suppose $T = \frac{1}{d} A$. Then
    \begin{equation}
        \tmix(T) \leq O\left(\frac{\log (n)}{1 - \beta}\right).
    \end{equation}
\end{lemma}</p>

<p>This shows that if your spectral gap is bounded by a constant, your mixing time is in $O(\log (n))$.</p>

<p>\begin{exercise}
    Verify that the path graph indeed has a small spectral gap, since we previously established that it has a large mixing time. Similarly, check that the complete graph has a large spectral gap.
\end{exercise}</p>

<p>\begin{proof}[Proof of Lemma \ref{mixing}]
    Let $T$ have eigenvalues $1 = \lambda_1 \geq \lambda_2 \geq \dots \geq
        \lambda_n$ with eigenvectors $v^1, v^2, \dots, v^n$. Assume that the
    eigenvectors are scaled to be unit vectors.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Since this is a symmetric matrix, the eigenvectors are pairwise orthogonal.

We can perform an eigenvalue decomposition of $T$ in terms of its eigenvectors via
\begin{equation}\label{eq:decomp}
    T = \sum\limits_i \lambda_i v_i v_i^\top .
\end{equation}

It follows from Equation \ref{eq:decomp} that
\begin{equation}
    T^k = \sum\limits_i \lambda_i^k v_i v_i^\top .
\end{equation}

Let $x \in [0,1]^n$ be a probability vector of $G$ where all entries are
non-negative and sum to 1.  Think of $x$ as the start state of the Markov
chain.

After $k$ steps, the state will be $T^k \cdot x$.

We can re-write $x$ in terms of the orthogonal basis of the eigenvectors of $T$, i.e
\begin{equation}
    x = \sum\limits_{i} \langle x, v_i \rangle \cdot v_i.
\end{equation}
Write $a_i = \langle x, v_i \rangle $ to be the coefficients of each eigenvector $v_i$.

$\lambda_1=1$, so $\lambda_1^k = 1$.
We also know that
\begin{equation}
    v^1 =
    \begin{pmatrix}
        \frac{1}{\sqrt{n}} \\
        \vdots             \\
        \frac{1}{\sqrt{n}} \\
    \end{pmatrix},
\end{equation}
since we previously showed that the all-ones vector is always an
eigenvector with eigenvalue 1, where here it is re-scaled to have unit norm.

Then
\begin{align}
    T^k \cdot x &amp; =
    \sum\limits_{i} \langle x, v_i \rangle  \cdot \lambda_i^k \cdot v_i                                                                                       \\
                &amp; = \langle x, v^1 \rangle \cdot v^1 + \sum\limits_{i \geq 2} \langle x, v_i \rangle  \cdot \lambda_i^k \cdot v_i                             \\
                &amp; = \frac{1}{n} \langle x, \mathbbm{1} \rangle \cdot \mathbbm{1} + \sum\limits_{i \geq 2} \langle x, v_i \rangle  \cdot \lambda_i^k \cdot v_i \\
                &amp; =
    \begin{pmatrix}
        \frac{1} \\
        \vdots        \\
        \frac{1} \\
    \end{pmatrix} +
    \sum\limits_{i \geq 2} \langle x, v_i \rangle  \cdot \lambda_i^k \cdot v_i,                                                                               \\
\end{align}
where the last step follows from the fact that $x$ is a probability distribution and thus $x \cdot \mathbbm{1} = 1$.

Rearranging and moving to work in the L2 (Euclidean) norm, we obtain
\begin{align}
    \left| \left|
    T^k \cdot x -
    \begin{pmatrix}
        \frac{1} \\
        \vdots        \\
        \frac{1} \\
    \end{pmatrix}
    \right| \right|_2
     &amp; =
    \left| \left|
    \sum\limits_{i = 2}^n \langle x, v_i \rangle  \cdot \lambda_i^{k}  v_i
    \right| \right|_2                                                                                   \\
     &amp; =
    \sqrt{
        \sum\limits_{i = 2}^n \langle x, v_i \rangle^2  \cdot \lambda_i^{2k} \cdot \| v_i \|^2_2
    } \tag{by definition of L2 norm, cross-terms cancel out since eigenvectors are pairwise orthogonal} \\
     &amp; =
    \sqrt{
        \sum\limits_{i = 2}^n \langle x, v_i \rangle^2  \cdot \lambda_i^{2k}
    } \tag{$v_i$ has unit norm}                                                                         \\
     &amp; \leq \| x \|_2 \cdot \beta^k,
\end{align}
where the last step comes from the fact that $\lambda_i \leq \beta$ for all $i \geq 2$ since $\beta$ is the second-largest eigenvalue, and
$\sum\limits_{i = 1}^n \langle x, v_i \rangle^2 = \| x \|_2^2$ .

Since $\| x \|_2 \leq 1$, we can simplify
\begin{align}
    \left| \left|
    T^k \cdot x -
    \begin{pmatrix}
        \frac{1} \\
        \vdots        \\
        \frac{1} \\
    \end{pmatrix}
    \right| \right|_2
     &amp; \leq \beta^k           \\
     &amp; = (1 - (1 - \beta))^k.
\end{align}
However, what we really care about is the total variation distance, which is the quantity
\begin{equation}
    \frac{1}{2}
    \left| \left|
    T^k \cdot x -
    \begin{pmatrix}
        \frac{1} \\
        \vdots        \\
        \frac{1} \\
    \end{pmatrix}
    \right| \right|_{TV} \\
    =
    \frac{1}{2}
    \left| \left|
    T^k \cdot x -
    \begin{pmatrix}
        \frac{1} \\
        \vdots        \\
        \frac{1} \\
    \end{pmatrix}
    \right| \right|_{1}.
\end{equation}

Recall that for any $n$-dimensional vector $x$, $\| x \|_1 = \sqrt{n} \| x \|_s$ by Cauchy-Schwarz:
\begin{align}
    \| x \|_1
     &amp; = \mathbbm{1} \cdot x                                      \\
     &amp; \leq \| \mathbbm{1} \|_2 \| x \|_2 \tag{by Cauchy-Schwarz} \\
     &amp; = \sqrt{n} \| x \|_2.
\end{align}

To relate the L2 distance to L1 distance, we can apply the above inequality to get
\begin{align}
    \frac{1}{2}
    \left| \left|
    T^k \cdot x -
    \begin{pmatrix}
        \frac{1} \\
        \vdots        \\
        \frac{1} \\
    \end{pmatrix}
    \right| \right|_1
     &amp; \leq
    \frac{1}{2}
    \sqrt{n}
    \left| \left|
    T^k \cdot x -
    \begin{pmatrix}
        \frac{1} \\
        \vdots        \\
        \frac{1} \\
    \end{pmatrix}
    \right| \right|_2                                                            \\
    \\
     &amp; \leq \frac{1}{2} \sqrt{n} \beta^k                                         \\
     &amp; \leq \frac{1}{4}, \tag{if $k &gt; O\left( \frac{\log n}{1 - \beta} \right)$}
\end{align}
as desired.

So we set $k \geq O\left( \frac{\log n}{1 - \beta} \right)$ for the total variation distance to be less than 1/4. \end{proof}
</code></pre></div></div>

<p>We say that a Markov Chain is fast mixing if $\tmix \leq \log^{O(1)}(n)$.</p>

<p>\section{Expander Graphs}
Lemma \ref{mixing}  motivates the following definition of expander graphs:
\begin{definition}[Expander Graph]
    $G$ is a $(n, d, \epsilon)$-expander graph if $G$ is a $d$-regular graph and
    $T = \frac{1}{d} A$ has spectral gap at least $\epsilon$.
\end{definition}</p>

<p>From what we have learnt so far, we know that an expander has to be well-connected in order to have a large spectral gap. Next lecture, we will see how to use expander graphs for derandomization. This helps to reduce the amount of random
bits required for algorithms.</p>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <!-- <p class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</p> -->
    <p class="mb-2">Related Posts:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/llama-3.1-technical-report-notes/">Notes on 'The Llama 3 Herd of Models'</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/setting-up-yuancon-controller-sound-voltex/">Playing Sound Voltex at Home: Setting Up Unnamed SDVX Clone with the Yuancon SDVX Controller</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/creating-trackback-requests/">Creating Trackback Requests for Static Sites</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/high-dimensional-analysis-of-m-estimators/">A Unified Framework for High-Dimensional Analysis of M-Estimators with Decomposable Regularizers: A Guided Walkthrough</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cmu-steam-tunnels/">The CMU Steam Tunnels and Wean 9</a>
  </li>

<div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "fanpu/website",
        "data-repo-id": "R_kgDOIpOodA",
        "data-category": "General",
        "data-category-id": "DIC_kwDOIpOodM4CTKDC",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

          </div>
        </div>
        
      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2024 Fan Pu  Zeng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.
Last updated: October 03, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Enable Tooltips -->
  <script type="text/javascript">
  $(function () {$('[data-toggle="tooltip"]').tooltip()})
  </script>
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>
  <!-- Sidebar Table of Contents -->
  <script defer src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>


  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      loader: {load: ['[tex]/mathtools']},
      tex: {
        packages: {'[+]': ['mathtools', 'ams']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          addMenu: []
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
