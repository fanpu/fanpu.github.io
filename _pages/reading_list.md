---
layout: page
permalink: /reading-list/
title: ML Reading List
description: Curated list of papers that I have bookmarked to read, well, someday...
nav: true
nav_order: 10
toc:
  sidebar: left
giscus_comments: true
---
This list is continously updated as I bookmark more papers to read. It's also a
good reflection of my current research interests.

There's a good chance that if I like the paper, I will write a summary about it.

1. [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
1. [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/abs/2312.08935)
1. [Scaling Data-Constrained Language Models](https://arxiv.org/abs/2305.16264)
1. [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://arxiv.org/abs/2311.03099)
1. [TIES-Merging: Resolving Interference When Merging Models](https://arxiv.org/abs/2306.01708)
1. [Editing Models with Task Arithmetic](https://arxiv.org/abs/2212.04089)
1. [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://arxiv.org/abs/2401.10774)
1. [AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification](https://arxiv.org/abs/1811.01727)
1. [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
1. [Jasper and Stella: distillation of SOTA embedding models](https://arxiv.org/abs/2412.19048)
1. [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)
1. [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071)
1. [Zero Bubble Pipeline Parallelism](https://arxiv.org/abs/2401.10241)
1. [How Does Critical Batch Size Scale in Pre-training?](https://arxiv.org/abs/2410.21676)
1. [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737)
1. [Pointer Networks](https://arxiv.org/abs/1506.03134)
1. [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://arxiv.org/abs/2404.10719)
1. [Recursive Introspection: Teaching Language Model Agents How to Self-Improve](https://arxiv.org/abs/2407.18219)
1. [WARM: On the Benefits of Weight Averaged Reward Models](https://arxiv.org/abs/2401.12187)
1. [Dual Operating Modes of In-Context Learning](https://arxiv.org/abs/2402.18819)
1. [Combining Induction and Transduction for Abstract Reasoning](https://arxiv.org/abs/2411.02272)
1. [Phi-4 Technical Report](https://arxiv.org/abs/2412.08905)
1. [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)
1. [RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning](https://arxiv.org/abs/2410.02089)
1. [Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs](https://arxiv.org/abs/2410.08020)
1. [HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation](https://arxiv.org/abs/2410.21216v1)
1. [Stream of Search (SoS): Learning to Search in Language](https://arxiv.org/abs/2404.03683)
1. [Quadratic models for understanding catapult dynamics of neural networks](https://arxiv.org/abs/2205.11787)
1. [Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning](https://arxiv.org/abs/2306.04815)
1. [$μ$LO: Compute-Efficient Meta-Generalization of Learned Optimizers](https://arxiv.org/abs/2406.00153)
1. [VeLO: Training Versatile Learned Optimizers by Scaling Up](https://arxiv.org/abs/2211.09760)
1. [Probing the Decision Boundaries of In-context Learning in Large Language Models](https://arxiv.org/abs/2406.11233)
1. [Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers](https://arxiv.org/abs/2411.15370)
1. [VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search](https://arxiv.org/abs/2402.08147)
1. [xLSTM: Extended Long Short-Term Memory](https://arxiv.org/abs/2405.04517)
1. [Memory-Efficient LLM Training with Online Subspace Descent](https://arxiv.org/abs/2408.12857)
1. [Efficient Model-Free Exploration in Low-Rank MDPs](https://arxiv.org/abs/2307.03997)
1. [On the Computational Landscape of Replicable Learning](https://arxiv.org/abs/2405.15599)
1. [TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications](https://arxiv.org/abs/2311.02971)
1. [TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second](https://arxiv.org/abs/2207.01848)
1. [Reinforcement Learning: An Overview](https://arxiv.org/abs/2412.05265)
1. [Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP](https://arxiv.org/abs/2406.01583)
1. [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/abs/2203.03466)
1. [Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices](https://arxiv.org/abs/2410.02117)
1. [Compute Better Spent: Replacing Dense Layers with Structured Matrices](https://arxiv.org/abs/2406.06248)
1. [CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra](https://arxiv.org/abs/2309.03060)
1. [Theoretical Foundations of Conformal Prediction](https://arxiv.org/abs/2411.11824)
1. [AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning](https://arxiv.org/abs/2406.11200)
1. [Mechanism of feature learning in convolutional neural networks](https://arxiv.org/abs/2309.00570)
1. [Average gradient outer product as a mechanism for deep neural collapse](https://arxiv.org/abs/2402.13728)
1. [Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel](https://arxiv.org/abs/1810.05369)
1. [Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation](https://arxiv.org/abs/2105.14368)
1. [The duality structure gradient descent algorithm: analysis and applications to neural networks](https://arxiv.org/abs/1708.00523)
1. [A Spectral Condition for Feature Learning](https://arxiv.org/abs/2310.17813)
1. [Modular Duality in Deep Learning](https://arxiv.org/abs/2410.21265)
1. [Old Optimizer, New Norm: An Anthology](https://arxiv.org/abs/2409.20325)
1. [Modular Duality in Deep Learning](https://arxiv.org/abs/2410.21265)
1. [Identifying and attacking the saddle point problem in high-dimensional non-convex optimization](https://arxiv.org/abs/1406.2572)
1. [Investigating the Limitations of Transformers with Simple Arithmetic Tasks](https://arxiv.org/abs/2102.13019)
1. [Scaling Relationship on Learning Mathematical Reasoning with Large Language Models](https://arxiv.org/abs/2308.01825)
1. [Evaluating Language Models for Mathematics through Interactions](https://arxiv.org/abs/2306.01694)
1. [Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks](https://arxiv.org/abs/2305.14201)
1. [Continual Pre-Training of Large Language Models: How to (re)warm your model?](https://arxiv.org/abs/2308.04014)
1. [Functional Data Analysis: An Introduction and Recent Developments](https://arxiv.org/abs/2312.05523)
1. [On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599)
1. [How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis](https://arxiv.org/abs/2411.04105)
1. [LoRA vs Full Fine-tuning: An Illusion of Equivalence](https://arxiv.org/abs/2410.21228v1)
1. [An Empirical Model of Large-Batch Training](https://arxiv.org/abs/1812.06162)
1. [The Description Length of Deep Learning Models](https://arxiv.org/abs/1802.07044)
1. [ADOPT: Modified Adam Can Converge with Any $β_2$ with the Optimal Rate](https://arxiv.org/abs/2411.02853)
1. [Denoising Diffusion Probabilistic Models in Six Simple Steps](https://arxiv.org/abs/2402.04384)
1. [Understanding Optimization in Deep Learning with Central Flows](https://arxiv.org/abs/2410.24206)
1. [Modular Duality in Deep Learning](https://arxiv.org/abs/2410.21265)
1. [DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data](https://arxiv.org/abs/2405.14333)
1. [Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis](https://arxiv.org/abs/2406.13762)
1. [Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective](https://arxiv.org/abs/2410.05192)
1. [Impacts of Continued Legal Pre-Training and IFT on LLMs' Latent Representations of Human-Defined Legal Concepts](https://arxiv.org/abs/2410.12001v1)
1. [A First Course in Monte Carlo Methods](https://arxiv.org/abs/2405.16359)
1. [Mixture of Parrots: Experts improve memorization more than reasoning](https://arxiv.org/abs/2410.19034)
1. [The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities](https://arxiv.org/abs/2408.13296v1)
1. [Agentic Information Retrieval](https://arxiv.org/abs/2410.09713)
1. [nGPT: Normalized Transformer with Representation Learning on the Hypersphere](https://arxiv.org/abs/2410.01131)
1. [Why Do We Need Weight Decay in Modern Deep Learning?](https://arxiv.org/abs/2310.04415)
1. [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202)
1. [Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning](https://arxiv.org/abs/1702.03118)
1. [Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning](https://arxiv.org/abs/2410.08146)
1. [Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think](https://arxiv.org/abs/2410.06940)
1. [Generative Verifiers: Reward Modeling as Next-Token Prediction](https://arxiv.org/abs/2408.15240)
1. [Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs](https://arxiv.org/abs/2402.14740)
1. [Round and Round We Go! What makes Rotary Positional Encodings useful?](https://arxiv.org/abs/2410.06205)
1. [Large Language Models as Markov Chains](https://arxiv.org/abs/2410.02724)
1. [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/abs/2407.01219)
1. [Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers](https://arxiv.org/abs/2402.01502)
1. [Reinforced Self-Training (ReST) for Language Modeling](https://arxiv.org/abs/2308.08998)
1. [eGAD! double descent is explained by Generalized Aliasing Decomposition](https://arxiv.org/abs/2408.08294)
1. [A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning](https://arxiv.org/abs/2310.18988)
1. [Classical Statistical (In-Sample) Intuitions Don't Generalize Well: A Note on Bias-Variance Tradeoffs, Overfitting and Moving from Fixed to Random Designs](https://arxiv.org/abs/2409.18842)
1. [Contextual Document Embeddings](https://arxiv.org/abs/2410.02525)
1. [Were RNNs All We Needed?](https://arxiv.org/abs/2410.01201)
1. [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254)
1. [Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision](https://arxiv.org/abs/2403.09472)
1. [From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models](https://arxiv.org/abs/2406.16838)
1. [Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://arxiv.org/abs/2402.12875)
1. [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)
1. [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)
1. [Hardware Acceleration of LLMs: A comprehensive survey and comparison](https://arxiv.org/abs/2409.03384)
1. [Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering](https://arxiv.org/abs/2409.02426)
1. [White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?](https://arxiv.org/abs/2311.13110)
1. [Just Say the Name: Online Continual Learning with Category Names Only via Data Generation](https://arxiv.org/abs/2403.10853)
1. [Magicoder: Empowering Code Generation with OSS-Instruct](https://arxiv.org/abs/2312.02120)
1. [Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950)
1. [#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models](https://arxiv.org/abs/2308.07074)
1. [Equivariant neural networks and piecewise linear representation theory](https://arxiv.org/abs/2408.00949)
1. [Does your data spark joy? Performance gains from domain upsampling at the end of training](https://arxiv.org/abs/2406.03476)
1. [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)
1. [Reconciling modern machine learning practice and the bias-variance trade-off](https://arxiv.org/abs/1812.11118)
1. [Moment Matching for Multi-Source Domain Adaptation](https://arxiv.org/abs/1812.01754)
1. [Invariant Risk Minimization](https://arxiv.org/abs/1907.02893)
1. [How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks](https://arxiv.org/abs/1808.04926)
1. [Do ImageNet Classifiers Generalize to ImageNet?](https://arxiv.org/abs/1902.10811)
1. [The Effect of Natural Distribution Shift on Question Answering Models](https://arxiv.org/abs/2004.14444)
1. [Causal inference using invariant prediction: identification and confidence intervals](https://arxiv.org/abs/1501.01332)
1. [Neural Tangent Kernel: Convergence and Generalization in Neural Networks](https://arxiv.org/abs/1806.07572)
1. [Measuring the Intrinsic Dimension of Objective Landscapes](https://arxiv.org/abs/1804.08838)
1. [Revisiting Unreasonable Effectiveness of Data in Deep Learning Era](https://arxiv.org/abs/1707.02968)
1. [No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems](https://arxiv.org/abs/2011.12945)
1. [Domain-Adjusted Regression or: ERM May Already Learn Features Sufficient for Out-of-Distribution Generalization](https://arxiv.org/abs/2202.06856)
1. [In-Context Learning Learns Label Relationships but Is Not Conventional Learning](https://arxiv.org/abs/2307.12375v4)
1. [Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://arxiv.org/abs/2404.01413)
1. [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221)
1. [A Tutorial on Bayesian Optimization](https://arxiv.org/abs/1807.02811)
1. [Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations](https://arxiv.org/abs/2204.02937)
1. [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)
1. [The Geometry of Categorical and Hierarchical Concepts in Large Language Models](https://arxiv.org/abs/2406.01506)
1. [When Representations Align: Universality in Representation Learning Dynamics](https://arxiv.org/abs/2402.09142)
1. [Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://arxiv.org/abs/2406.20094)
1. [A Theory of Interpretable Approximations](https://arxiv.org/abs/2406.10529)
1. [BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment](https://arxiv.org/abs/2406.12168)
1. [A Tutorial on Thompson Sampling](https://arxiv.org/abs/1707.02038)
1. [Beyond the Black Box: A Statistical Model for LLM Reasoning and Inference](https://arxiv.org/abs/2402.03175)
1. [Transcendence: Generative Models Can Outperform The Experts That Train Them](https://arxiv.org/abs/2406.11741v1)
1. [Step-by-Step Diffusion: An Elementary Tutorial](https://arxiv.org/abs/2406.08929)
1. [Text Embeddings Reveal (Almost) As Much As Text](https://arxiv.org/abs/2310.06816)
1. [Language Modeling with Gated Convolutional Networks](https://arxiv.org/abs/1612.08083)
1. [Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks](https://arxiv.org/abs/2312.08550)
1. [Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models](https://arxiv.org/abs/2405.05417)
1. [Customizing Text-to-Image Models with a Single Image Pair](https://arxiv.org/abs/2405.01536)
1. [Self-Play Preference Optimization for Language Model Alignment](https://arxiv.org/abs/2405.00675)
1. [Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences](https://arxiv.org/abs/2404.12272)
1. [U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models](https://arxiv.org/abs/2404.18444)
1. [On the Bottleneck of Graph Neural Networks and its Practical Implications](https://arxiv.org/abs/2006.05205)
1. [How Can We Know What Language Models Know?](https://arxiv.org/abs/1911.12543)
1. [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877)
1. [Why do tree-based models still outperform deep learning on tabular data?](https://arxiv.org/abs/2207.08815)
1. [On the Complexity of Best Arm Identification in Multi-Armed Bandit Models](https://arxiv.org/abs/1407.4443)
1. [Random Utility Theory for Social Choice](https://arxiv.org/abs/1211.2476)
1. [Black-box Dataset Ownership Verification via Backdoor Watermarking](https://arxiv.org/abs/2209.06015)
1. [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)
1. [Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2207.14255)
1. [From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function](https://arxiv.org/abs/2404.12358)
1. [Large Language Models Can Self-Improve](https://arxiv.org/abs/2210.11610)
1. [How to Train Data-Efficient LLMs](https://arxiv.org/abs/2402.09668)
1. [Co-training Improves Prompt-based Learning for Large Language Models](https://arxiv.org/abs/2202.00828)
1. [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751)
1. [Learning Transformer Programs](https://arxiv.org/abs/2306.01128)
1. [Adafactor: Adaptive Learning Rates with Sublinear Memory Cost](https://arxiv.org/abs/1804.04235)
1. [Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data](https://arxiv.org/abs/1808.01204)
1. [A Convergence Theory for Deep Learning via Over-Parameterization](https://arxiv.org/abs/1811.03962)
1. [Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian](https://arxiv.org/abs/1906.05392)
1. [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://arxiv.org/abs/2012.13255)
1. [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)
1. [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://arxiv.org/abs/2403.09636)
1. [Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks](https://arxiv.org/abs/2309.17002)
1. [The Barron Space and the Flow-induced Function Spaces for Neural Network Models](https://arxiv.org/abs/1906.08039)
1. [Deep Equilibrium Based Neural Operators for Steady-State PDEs](https://arxiv.org/abs/2312.00234)
1. [Parametric Complexity Bounds for Approximating PDEs with Neural Networks](https://arxiv.org/abs/2103.02138)
1. [Simple linear attention language models balance the recall-throughput tradeoff](https://arxiv.org/abs/2402.18668)
1. [Chain-of-Thought Reasoning Without Prompting](https://arxiv.org/abs/2402.10200)
1. [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://arxiv.org/abs/2403.09636)
1. [Evolutionary Optimization of Model Merging Recipes](https://arxiv.org/abs/2403.13187)
1. [An Invitation to Deep Reinforcement Learning](https://arxiv.org/abs/2312.08365)
1. [Deep Neural Networks Tend To Extrapolate Predictably](https://arxiv.org/abs/2310.00873)
1. [Scaling Rectified Flow Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2403.03206)
1. [Is Cosine-Similarity of Embeddings Really About Similarity?](https://arxiv.org/abs/2403.05440)
1. [Language Modeling with Gated Convolutional Networks](https://arxiv.org/abs/1612.08083)
1. [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202v1)
1. [On the Measure of Intelligence](https://arxiv.org/abs/1911.01547)
1. [Asymmetry in Low-Rank Adapters of Foundation Models](https://arxiv.org/abs/2402.16842)
1. [Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748)
1. [In-Context Learning for Extreme Multi-Label Classification](https://arxiv.org/abs/2401.12178)
1. [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147)
1. [Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization](https://arxiv.org/abs/2311.04163)
1. [Diffusion Models for Generative Artificial Intelligence: An Introduction for Applied Mathematicians](https://arxiv.org/abs/2312.14977)
1. [In-context Learning and Induction Heads](https://arxiv.org/abs/2209.11895)
1. [Characterizing Implicit Bias in Terms of Optimization Geometry](https://arxiv.org/abs/1802.08246)
1. [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)
1. [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)
1. [Data Selection for Language Models via Importance Resampling](https://arxiv.org/abs/2302.03169)
1. [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)
1. [Data Movement Is All You Need: A Case Study on Optimizing Transformers](https://arxiv.org/abs/2007.00072)
1. [Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars](https://arxiv.org/abs/2312.01429)
1. [Accelerating LLM Inference with Staged Speculative Decoding](https://arxiv.org/abs/2308.04623)
1. [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318)
1. [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
1. [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339)
1. [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)
1. [One Wide Feedforward is All You Need](https://arxiv.org/abs/2309.01826)
1. [A Mean Field View of the Landscape of Two-Layers Neural Networks](https://arxiv.org/abs/1804.06561)
1. [Sharpness-Aware Minimization for Efficiently Improving Generalization](https://arxiv.org/abs/2010.01412)
1. [Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability](https://arxiv.org/abs/2103.00065)
1. [The large learning rate phase of deep learning: the catapult mechanism](https://arxiv.org/abs/2003.02218)
1. [Label Noise SGD Provably Prefers Flat Global Minimizers](https://arxiv.org/abs/2106.06530)
1. [Gradient Descent Maximizes the Margin of Homogeneous Neural Networks](https://arxiv.org/abs/1906.05890)
1. [Deep Double Descent: Where Bigger Models and More Data Hurt](https://arxiv.org/abs/1912.02292)
1. [The generalization error of random features regression: Precise asymptotics and double descent curve](https://arxiv.org/abs/1908.05355)
1. [Exploring Generalization in Deep Learning](https://arxiv.org/abs/1706.08947)
1. [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)
1. [On Exact Computation with an Infinitely Wide Neural Net](https://arxiv.org/abs/1904.11955)
1. [Neural Tangent Kernel: Convergence and Generalization in Neural Networks](https://arxiv.org/abs/1806.07572)
1. [Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers](https://arxiv.org/abs/1811.04918)
1. [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
1. [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)
1. [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370)
1. [A Fourier Perspective on Model Robustness in Computer Vision](https://arxiv.org/abs/1906.08988)
1. [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)
1. [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
1. [Vision Transformers are Robust Learners](https://arxiv.org/abs/2105.07581)
1. [On the Adversarial Robustness of Vision Transformers](https://arxiv.org/abs/2103.15670)
1. [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)
1. [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)
1. [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396v3)