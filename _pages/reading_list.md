---
layout: page
permalink: /reading-list/
title: ML Reading List
description: Curated list of papers that I have bookmarked to read, well, someday...
nav: true
nav_order: 10
toc:
  sidebar: left
giscus_comments: true
---
This list is continously updated as I bookmark more papers to read. It's also a
good reflection of my current research interests.

There's a good chance that if I like the paper, I will write a summary about it.

- [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
- [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/abs/2312.08935)
- [Scaling Data-Constrained Language Models](https://arxiv.org/abs/2305.16264)
- [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://arxiv.org/abs/2311.03099)
- [TIES-Merging: Resolving Interference When Merging Models](https://arxiv.org/abs/2306.01708)
- [Editing Models with Task Arithmetic](https://arxiv.org/abs/2212.04089)
- [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://arxiv.org/abs/2401.10774)
- [AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification](https://arxiv.org/abs/1811.01727)
- [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
- [Jasper and Stella: distillation of SOTA embedding models](https://arxiv.org/abs/2412.19048)
- [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)
- [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071)
- [Zero Bubble Pipeline Parallelism](https://arxiv.org/abs/2401.10241)
- [How Does Critical Batch Size Scale in Pre-training?](https://arxiv.org/abs/2410.21676)
- [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737)
- [Pointer Networks](https://arxiv.org/abs/1506.03134)
- [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://arxiv.org/abs/2404.10719)
- [Recursive Introspection: Teaching Language Model Agents How to Self-Improve](https://arxiv.org/abs/2407.18219)
- [WARM: On the Benefits of Weight Averaged Reward Models](https://arxiv.org/abs/2401.12187)
- [Dual Operating Modes of In-Context Learning](https://arxiv.org/abs/2402.18819)
- [Combining Induction and Transduction for Abstract Reasoning](https://arxiv.org/abs/2411.02272)
- [Phi-4 Technical Report](https://arxiv.org/abs/2412.08905)
- [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)
- [RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning](https://arxiv.org/abs/2410.02089)
- [Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs](https://arxiv.org/abs/2410.08020)
- [HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation](https://arxiv.org/abs/2410.21216v1)
- [Stream of Search (SoS): Learning to Search in Language](https://arxiv.org/abs/2404.03683)
- [Quadratic models for understanding catapult dynamics of neural networks](https://arxiv.org/abs/2205.11787)
- [Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning](https://arxiv.org/abs/2306.04815)
- [$μ$LO: Compute-Efficient Meta-Generalization of Learned Optimizers](https://arxiv.org/abs/2406.00153)
- [VeLO: Training Versatile Learned Optimizers by Scaling Up](https://arxiv.org/abs/2211.09760)
- [Probing the Decision Boundaries of In-context Learning in Large Language Models](https://arxiv.org/abs/2406.11233)
- [Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers](https://arxiv.org/abs/2411.15370)
- [VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search](https://arxiv.org/abs/2402.08147)
- [xLSTM: Extended Long Short-Term Memory](https://arxiv.org/abs/2405.04517)
- [Memory-Efficient LLM Training with Online Subspace Descent](https://arxiv.org/abs/2408.12857)
- [Efficient Model-Free Exploration in Low-Rank MDPs](https://arxiv.org/abs/2307.03997)
- [On the Computational Landscape of Replicable Learning](https://arxiv.org/abs/2405.15599)
- [TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications](https://arxiv.org/abs/2311.02971)
- [TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second](https://arxiv.org/abs/2207.01848)
- [Reinforcement Learning: An Overview](https://arxiv.org/abs/2412.05265)
- [Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP](https://arxiv.org/abs/2406.01583)
- [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/abs/2203.03466)
- [Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices](https://arxiv.org/abs/2410.02117)
- [Compute Better Spent: Replacing Dense Layers with Structured Matrices](https://arxiv.org/abs/2406.06248)
- [CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra](https://arxiv.org/abs/2309.03060)
- [Theoretical Foundations of Conformal Prediction](https://arxiv.org/abs/2411.11824)
- [AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning](https://arxiv.org/abs/2406.11200)
- [Mechanism of feature learning in convolutional neural networks](https://arxiv.org/abs/2309.00570)
- [Average gradient outer product as a mechanism for deep neural collapse](https://arxiv.org/abs/2402.13728)
- [Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel](https://arxiv.org/abs/1810.05369)
- [Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation](https://arxiv.org/abs/2105.14368)
- [The duality structure gradient descent algorithm: analysis and applications to neural networks](https://arxiv.org/abs/1708.00523)
- [A Spectral Condition for Feature Learning](https://arxiv.org/abs/2310.17813)
- [Modular Duality in Deep Learning](https://arxiv.org/abs/2410.21265)
- [Old Optimizer, New Norm: An Anthology](https://arxiv.org/abs/2409.20325)
- [Modular Duality in Deep Learning](https://arxiv.org/abs/2410.21265)
- [Identifying and attacking the saddle point problem in high-dimensional non-convex optimization](https://arxiv.org/abs/1406.2572)
- [Investigating the Limitations of Transformers with Simple Arithmetic Tasks](https://arxiv.org/abs/2102.13019)
- [Scaling Relationship on Learning Mathematical Reasoning with Large Language Models](https://arxiv.org/abs/2308.01825)
- [Evaluating Language Models for Mathematics through Interactions](https://arxiv.org/abs/2306.01694)
- [Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks](https://arxiv.org/abs/2305.14201)
- [Continual Pre-Training of Large Language Models: How to (re)warm your model?](https://arxiv.org/abs/2308.04014)
- [Functional Data Analysis: An Introduction and Recent Developments](https://arxiv.org/abs/2312.05523)
- [On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599)
- [How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis](https://arxiv.org/abs/2411.04105)
- [LoRA vs Full Fine-tuning: An Illusion of Equivalence](https://arxiv.org/abs/2410.21228v1)
- [An Empirical Model of Large-Batch Training](https://arxiv.org/abs/1812.06162)
- [The Description Length of Deep Learning Models](https://arxiv.org/abs/1802.07044)
- [ADOPT: Modified Adam Can Converge with Any $β_2$ with the Optimal Rate](https://arxiv.org/abs/2411.02853)
- [Denoising Diffusion Probabilistic Models in Six Simple Steps](https://arxiv.org/abs/2402.04384)
- [Understanding Optimization in Deep Learning with Central Flows](https://arxiv.org/abs/2410.24206)
- [Modular Duality in Deep Learning](https://arxiv.org/abs/2410.21265)
- [DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data](https://arxiv.org/abs/2405.14333)
- [Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis](https://arxiv.org/abs/2406.13762)
- [Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective](https://arxiv.org/abs/2410.05192)
- [Impacts of Continued Legal Pre-Training and IFT on LLMs' Latent Representations of Human-Defined Legal Concepts](https://arxiv.org/abs/2410.12001v1)
- [A First Course in Monte Carlo Methods](https://arxiv.org/abs/2405.16359)
- [Mixture of Parrots: Experts improve memorization more than reasoning](https://arxiv.org/abs/2410.19034)
- [The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities](https://arxiv.org/abs/2408.13296v1)
- [Agentic Information Retrieval](https://arxiv.org/abs/2410.09713)
- [nGPT: Normalized Transformer with Representation Learning on the Hypersphere](https://arxiv.org/abs/2410.01131)
- [Why Do We Need Weight Decay in Modern Deep Learning?](https://arxiv.org/abs/2310.04415)
- [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202)
- [Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning](https://arxiv.org/abs/1702.03118)
- [Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning](https://arxiv.org/abs/2410.08146)
- [Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think](https://arxiv.org/abs/2410.06940)
- [Generative Verifiers: Reward Modeling as Next-Token Prediction](https://arxiv.org/abs/2408.15240)
- [Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs](https://arxiv.org/abs/2402.14740)
- [Round and Round We Go! What makes Rotary Positional Encodings useful?](https://arxiv.org/abs/2410.06205)
- [Large Language Models as Markov Chains](https://arxiv.org/abs/2410.02724)
- [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/abs/2407.01219)
- [Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers](https://arxiv.org/abs/2402.01502)
- [Reinforced Self-Training (ReST) for Language Modeling](https://arxiv.org/abs/2308.08998)
- [eGAD! double descent is explained by Generalized Aliasing Decomposition](https://arxiv.org/abs/2408.08294)
- [A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning](https://arxiv.org/abs/2310.18988)
- [Classical Statistical (In-Sample) Intuitions Don't Generalize Well: A Note on Bias-Variance Tradeoffs, Overfitting and Moving from Fixed to Random Designs](https://arxiv.org/abs/2409.18842)
- [Contextual Document Embeddings](https://arxiv.org/abs/2410.02525)
- [Were RNNs All We Needed?](https://arxiv.org/abs/2410.01201)
- [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254)
- [Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision](https://arxiv.org/abs/2403.09472)
- [From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models](https://arxiv.org/abs/2406.16838)
- [Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://arxiv.org/abs/2402.12875)
- [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)
- [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)
- [Hardware Acceleration of LLMs: A comprehensive survey and comparison](https://arxiv.org/abs/2409.03384)
- [Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering](https://arxiv.org/abs/2409.02426)
- [White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?](https://arxiv.org/abs/2311.13110)
- [Just Say the Name: Online Continual Learning with Category Names Only via Data Generation](https://arxiv.org/abs/2403.10853)
- [Magicoder: Empowering Code Generation with OSS-Instruct](https://arxiv.org/abs/2312.02120)
- [Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950)
- [#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models](https://arxiv.org/abs/2308.07074)
- [Equivariant neural networks and piecewise linear representation theory](https://arxiv.org/abs/2408.00949)
- [Does your data spark joy? Performance gains from domain upsampling at the end of training](https://arxiv.org/abs/2406.03476)
- [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)
- [Reconciling modern machine learning practice and the bias-variance trade-off](https://arxiv.org/abs/1812.11118)
- [Moment Matching for Multi-Source Domain Adaptation](https://arxiv.org/abs/1812.01754)
- [Invariant Risk Minimization](https://arxiv.org/abs/1907.02893)
- [How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks](https://arxiv.org/abs/1808.04926)
- [Do ImageNet Classifiers Generalize to ImageNet?](https://arxiv.org/abs/1902.10811)
- [The Effect of Natural Distribution Shift on Question Answering Models](https://arxiv.org/abs/2004.14444)
- [Causal inference using invariant prediction: identification and confidence intervals](https://arxiv.org/abs/1501.01332)
- [Neural Tangent Kernel: Convergence and Generalization in Neural Networks](https://arxiv.org/abs/1806.07572)
- [Measuring the Intrinsic Dimension of Objective Landscapes](https://arxiv.org/abs/1804.08838)
- [Revisiting Unreasonable Effectiveness of Data in Deep Learning Era](https://arxiv.org/abs/1707.02968)
- [No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems](https://arxiv.org/abs/2011.12945)
- [Domain-Adjusted Regression or: ERM May Already Learn Features Sufficient for Out-of-Distribution Generalization](https://arxiv.org/abs/2202.06856)
- [In-Context Learning Learns Label Relationships but Is Not Conventional Learning](https://arxiv.org/abs/2307.12375v4)
- [Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://arxiv.org/abs/2404.01413)
- [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221)
- [A Tutorial on Bayesian Optimization](https://arxiv.org/abs/1807.02811)
- [Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations](https://arxiv.org/abs/2204.02937)
- [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)
- [The Geometry of Categorical and Hierarchical Concepts in Large Language Models](https://arxiv.org/abs/2406.01506)
- [When Representations Align: Universality in Representation Learning Dynamics](https://arxiv.org/abs/2402.09142)
- [Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://arxiv.org/abs/2406.20094)
- [A Theory of Interpretable Approximations](https://arxiv.org/abs/2406.10529)
- [BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment](https://arxiv.org/abs/2406.12168)
- [A Tutorial on Thompson Sampling](https://arxiv.org/abs/1707.02038)
- [Beyond the Black Box: A Statistical Model for LLM Reasoning and Inference](https://arxiv.org/abs/2402.03175)
- [Transcendence: Generative Models Can Outperform The Experts That Train Them](https://arxiv.org/abs/2406.11741v1)
- [Step-by-Step Diffusion: An Elementary Tutorial](https://arxiv.org/abs/2406.08929)
- [Text Embeddings Reveal (Almost) As Much As Text](https://arxiv.org/abs/2310.06816)
- [Language Modeling with Gated Convolutional Networks](https://arxiv.org/abs/1612.08083)
- [Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks](https://arxiv.org/abs/2312.08550)
- [Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models](https://arxiv.org/abs/2405.05417)
- [Customizing Text-to-Image Models with a Single Image Pair](https://arxiv.org/abs/2405.01536)
- [Self-Play Preference Optimization for Language Model Alignment](https://arxiv.org/abs/2405.00675)
- [Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences](https://arxiv.org/abs/2404.12272)
- [U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models](https://arxiv.org/abs/2404.18444)
- [On the Bottleneck of Graph Neural Networks and its Practical Implications](https://arxiv.org/abs/2006.05205)
- [How Can We Know What Language Models Know?](https://arxiv.org/abs/1911.12543)
- [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877)
- [Why do tree-based models still outperform deep learning on tabular data?](https://arxiv.org/abs/2207.08815)
- [On the Complexity of Best Arm Identification in Multi-Armed Bandit Models](https://arxiv.org/abs/1407.4443)
- [Random Utility Theory for Social Choice](https://arxiv.org/abs/1211.2476)
- [Black-box Dataset Ownership Verification via Backdoor Watermarking](https://arxiv.org/abs/2209.06015)
- [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)
- [Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2207.14255)
- [From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function](https://arxiv.org/abs/2404.12358)
- [Large Language Models Can Self-Improve](https://arxiv.org/abs/2210.11610)
- [How to Train Data-Efficient LLMs](https://arxiv.org/abs/2402.09668)
- [Co-training Improves Prompt-based Learning for Large Language Models](https://arxiv.org/abs/2202.00828)
- [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751)
- [Learning Transformer Programs](https://arxiv.org/abs/2306.01128)
- [Adafactor: Adaptive Learning Rates with Sublinear Memory Cost](https://arxiv.org/abs/1804.04235)
- [Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data](https://arxiv.org/abs/1808.01204)
- [A Convergence Theory for Deep Learning via Over-Parameterization](https://arxiv.org/abs/1811.03962)
- [Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian](https://arxiv.org/abs/1906.05392)
- [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://arxiv.org/abs/2012.13255)
- [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)
- [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://arxiv.org/abs/2403.09636)
- [Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks](https://arxiv.org/abs/2309.17002)
- [The Barron Space and the Flow-induced Function Spaces for Neural Network Models](https://arxiv.org/abs/1906.08039)
- [Deep Equilibrium Based Neural Operators for Steady-State PDEs](https://arxiv.org/abs/2312.00234)
- [Parametric Complexity Bounds for Approximating PDEs with Neural Networks](https://arxiv.org/abs/2103.02138)
- [Simple linear attention language models balance the recall-throughput tradeoff](https://arxiv.org/abs/2402.18668)
- [Chain-of-Thought Reasoning Without Prompting](https://arxiv.org/abs/2402.10200)
- [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://arxiv.org/abs/2403.09636)
- [Evolutionary Optimization of Model Merging Recipes](https://arxiv.org/abs/2403.13187)
- [An Invitation to Deep Reinforcement Learning](https://arxiv.org/abs/2312.08365)
- [Deep Neural Networks Tend To Extrapolate Predictably](https://arxiv.org/abs/2310.00873)
- [Scaling Rectified Flow Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2403.03206)
- [Is Cosine-Similarity of Embeddings Really About Similarity?](https://arxiv.org/abs/2403.05440)
- [Language Modeling with Gated Convolutional Networks](https://arxiv.org/abs/1612.08083)
- [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202v1)
- [On the Measure of Intelligence](https://arxiv.org/abs/1911.01547)
- [Asymmetry in Low-Rank Adapters of Foundation Models](https://arxiv.org/abs/2402.16842)
- [Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748)
- [In-Context Learning for Extreme Multi-Label Classification](https://arxiv.org/abs/2401.12178)
- [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147)
- [Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization](https://arxiv.org/abs/2311.04163)
- [Diffusion Models for Generative Artificial Intelligence: An Introduction for Applied Mathematicians](https://arxiv.org/abs/2312.14977)
- [In-context Learning and Induction Heads](https://arxiv.org/abs/2209.11895)
- [Characterizing Implicit Bias in Terms of Optimization Geometry](https://arxiv.org/abs/1802.08246)
- [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)
- [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)
- [Data Selection for Language Models via Importance Resampling](https://arxiv.org/abs/2302.03169)
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)
- [Data Movement Is All You Need: A Case Study on Optimizing Transformers](https://arxiv.org/abs/2007.00072)
- [Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars](https://arxiv.org/abs/2312.01429)
- [Accelerating LLM Inference with Staged Speculative Decoding](https://arxiv.org/abs/2308.04623)
- [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318)
- [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339)
- [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)
- [One Wide Feedforward is All You Need](https://arxiv.org/abs/2309.01826)
- [A Mean Field View of the Landscape of Two-Layers Neural Networks](https://arxiv.org/abs/1804.06561)
- [Sharpness-Aware Minimization for Efficiently Improving Generalization](https://arxiv.org/abs/2010.01412)
- [Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability](https://arxiv.org/abs/2103.00065)
- [The large learning rate phase of deep learning: the catapult mechanism](https://arxiv.org/abs/2003.02218)
- [Label Noise SGD Provably Prefers Flat Global Minimizers](https://arxiv.org/abs/2106.06530)
- [Gradient Descent Maximizes the Margin of Homogeneous Neural Networks](https://arxiv.org/abs/1906.05890)
- [Deep Double Descent: Where Bigger Models and More Data Hurt](https://arxiv.org/abs/1912.02292)
- [The generalization error of random features regression: Precise asymptotics and double descent curve](https://arxiv.org/abs/1908.05355)
- [Exploring Generalization in Deep Learning](https://arxiv.org/abs/1706.08947)
- [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)
- [On Exact Computation with an Infinitely Wide Neural Net](https://arxiv.org/abs/1904.11955)
- [Neural Tangent Kernel: Convergence and Generalization in Neural Networks](https://arxiv.org/abs/1806.07572)
- [Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers](https://arxiv.org/abs/1811.04918)
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
- [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)
- [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370)
- [A Fourier Perspective on Model Robustness in Computer Vision](https://arxiv.org/abs/1906.08988)
- [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
- [Vision Transformers are Robust Learners](https://arxiv.org/abs/2105.07581)
- [On the Adversarial Robustness of Vision Transformers](https://arxiv.org/abs/2103.15670)
- [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)
- [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)
- [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396v3)
