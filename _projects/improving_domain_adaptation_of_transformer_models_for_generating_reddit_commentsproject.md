---
layout: page
title: Improving Domain Adaptation of Transformer Models For Generating Reddit Comments
description: >
  We improve upon the recent success of large language models based on the trans-
  former architecture by investigating and showing several methods that have em-
  pirically improved its performance in domain adaptation. We use a pre-trained
  GPT-2 model and perform fine-tuning on 5 different subreddits, and use different
  methods of ordering the training data based on our priors about the input to see how
  this affects the prediction quality of the trained model. We propose a new metric
  for evaluating causal language modeling tasks called APES (Average Perplexity
  Evaluation for Sentences) to address the limitations of existing metrics, and apply
  them to our results. Our results are evaluated against both LSTM and GPT-2
  baselines.
img: assets/img/posts/rainier_view.avif
importance: 2
category: machine-learning
redirect_internal: _posts/2022-12-05-improving-domain-adaptation-of-transformer-models-for-reddit-comments.markdown
---
