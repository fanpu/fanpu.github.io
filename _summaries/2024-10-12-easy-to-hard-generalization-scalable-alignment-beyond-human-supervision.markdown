---
layout: summary
title: "Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision"
giscus_comments: true
bib_id: 2403.09472v1
published: false
---

### Three Important Things

#### 1. Easy-to-Hard Generalization

How can we continue to supervise AI systems when they eventually attain
superhuman-level status and we can no longer provide reliable labels?

This paper finds that reward models can be trained on easier
tasks can be used to provide supervision signal for harder tasks.

Note that this differs from [weak-to-strong generalization]({% link _summaries/2024-07-21-weak-to-strong-generalization-eliciting-strong-capabilities-with-weak-supervision.markdown %}),
where they found that strong learners trained on labels from weak teachers could
achieve moderate performance gap recovered (PGR) in some settings like NLP
tasks, but fails in others like reward modeling.

#### 2. Bar

#### 3. Baz

### Most Glaring Deficiency

### Conclusions for Future Work
