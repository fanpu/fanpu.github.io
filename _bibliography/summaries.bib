---
---

@article{2209.03430v2,
Author        = {Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency},
Title         = {Foundations and Trends in Multimodal Machine Learning: Principles,
  Challenges, and Open Questions},
Eprint        = {2209.03430v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Multimodal machine learning is a vibrant multi-disciplinary research field
that aims to design computer agents with intelligent capabilities such as
understanding, reasoning, and learning through integrating multiple
communicative modalities, including linguistic, acoustic, visual, tactile, and
physiological messages. With the recent interest in video understanding,
embodied autonomous agents, text-to-image generation, and multisensor fusion in
application domains such as healthcare and robotics, multimodal machine
learning has brought unique computational and theoretical challenges to the
machine learning community given the heterogeneity of data sources and the
interconnections often found between modalities. However, the breadth of
progress in multimodal research has made it difficult to identify the common
themes and open questions in the field. By synthesizing a broad range of
application domains and theoretical frameworks from both historical and recent
perspectives, this paper is designed to provide an overview of the
computational and theoretical foundations of multimodal machine learning. We
start by defining three key principles of modality heterogeneity, connections,
and interactions that have driven subsequent innovations, and propose a
taxonomy of six core technical challenges: representation, alignment,
reasoning, generation, transference, and quantification covering historical and
recent trends. Recent technical achievements will be presented through the lens
of this taxonomy, allowing researchers to understand the similarities and
differences across new approaches. We end by motivating several open problems
for future research as identified by our taxonomy.},
Year          = {2022},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2209.03430v2},
File          = {2209.03430v2.pdf},
EprintNoVer   = {2209.03430}
}

@article{1802.05365v2,
Author        = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
Title         = {Deep contextualized word representations},
Eprint        = {1802.05365v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce a new type of deep contextualized word representation that
models both (1) complex characteristics of word use (e.g., syntax and
semantics), and (2) how these uses vary across linguistic contexts (i.e., to
model polysemy). Our word vectors are learned functions of the internal states
of a deep bidirectional language model (biLM), which is pre-trained on a large
text corpus. We show that these representations can be easily added to existing
models and significantly improve the state of the art across six challenging
NLP problems, including question answering, textual entailment and sentiment
analysis. We also present an analysis showing that exposing the deep internals
of the pre-trained network is crucial, allowing downstream models to mix
different types of semi-supervision signals.},
Year          = {2018},
Month         = {Feb},
Url           = {http://arxiv.org/abs/1802.05365v2},
File          = {1802.05365v2.pdf},
EprintNoVer   = {1802.05365}
}

@article{2201.11903v6,
Author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
Title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
Eprint        = {2201.11903v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.},
Year          = {2022},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2201.11903v6},
File          = {2201.11903v6.pdf},
EprintNoVer   = {2201.11903}
}

@article{2005.14165v4,
Author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
Title         = {Language Models are Few-Shot Learners},
Eprint        = {2005.14165v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3's few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general.},
Year          = {2020},
Month         = {May},
Url           = {http://arxiv.org/abs/2005.14165v4},
File          = {2005.14165v4.pdf},
EprintNoVer   = {2005.14165}
}

@article{1810.04805v2,
Author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
Title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding},
Eprint        = {1810.04805v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).},
Year          = {2018},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1810.04805v2},
File          = {1810.04805v2.pdf},
EprintNoVer   = {1810.04805}
}

@article{2203.02155v1,
Author        = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
Title         = {Training language models to follow instructions with human feedback},
Eprint        = {2203.02155v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent.},
Year          = {2022},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2203.02155v1},
File          = {2203.02155v1.pdf},
EprintNoVer   = {2203.02155}
}

@article{2107.03374v2,
Author        = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
Title         = {Evaluating Large Language Models Trained on Code},
Eprint        = {2107.03374v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We introduce Codex, a GPT language model fine-tuned on publicly available
code from GitHub, and study its Python code-writing capabilities. A distinct
production version of Codex powers GitHub Copilot. On HumanEval, a new
evaluation set we release to measure functional correctness for synthesizing
programs from docstrings, our model solves 28.8% of the problems, while GPT-3
solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling
from the model is a surprisingly effective strategy for producing working
solutions to difficult prompts. Using this method, we solve 70.2% of our
problems with 100 samples per problem. Careful investigation of our model
reveals its limitations, including difficulty with docstrings describing long
chains of operations and with binding operations to variables. Finally, we
discuss the potential broader impacts of deploying powerful code generation
technologies, covering safety, security, and economics.},
Year          = {2021},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2107.03374v2},
File          = {2107.03374v2.pdf},
EprintNoVer   = {2107.03374}
}

@article{1910.13461v1,
Author        = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
Title         = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language
  Generation, Translation, and Comprehension},
Eprint        = {1910.13461v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence
models. BART is trained by (1) corrupting text with an arbitrary noising
function, and (2) learning a model to reconstruct the original text. It uses a
standard Tranformer-based neural machine translation architecture which,
despite its simplicity, can be seen as generalizing BERT (due to the
bidirectional encoder), GPT (with the left-to-right decoder), and many other
more recent pretraining schemes. We evaluate a number of noising approaches,
finding the best performance by both randomly shuffling the order of the
original sentences and using a novel in-filling scheme, where spans of text are
replaced with a single mask token. BART is particularly effective when fine
tuned for text generation but also works well for comprehension tasks. It
matches the performance of RoBERTa with comparable training resources on GLUE
and SQuAD, achieves new state-of-the-art results on a range of abstractive
dialogue, question answering, and summarization tasks, with gains of up to 6
ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system
for machine translation, with only target language pretraining. We also report
ablation experiments that replicate other pretraining schemes within the BART
framework, to better measure which factors most influence end-task performance.},
Year          = {2019},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1910.13461v1},
File          = {1910.13461v1.pdf},
EprintNoVer   = {1910.13461}
}

@article{2005.11401v4,
Author        = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
Title         = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
Eprint        = {2005.11401v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when fine-tuned on
downstream NLP tasks. However, their ability to access and precisely manipulate
knowledge is still limited, and hence on knowledge-intensive tasks, their
performance lags behind task-specific architectures. Additionally, providing
provenance for their decisions and updating their world knowledge remain open
research problems. Pre-trained models with a differentiable access mechanism to
explicit non-parametric memory can overcome this issue, but have so far been
only investigated for extractive downstream tasks. We explore a general-purpose
fine-tuning recipe for retrieval-augmented generation (RAG) -- models which
combine pre-trained parametric and non-parametric memory for language
generation. We introduce RAG models where the parametric memory is a
pre-trained seq2seq model and the non-parametric memory is a dense vector index
of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG
formulations, one which conditions on the same retrieved passages across the
whole generated sequence, the other can use different passages per token. We
fine-tune and evaluate our models on a wide range of knowledge-intensive NLP
tasks and set the state-of-the-art on three open domain QA tasks, outperforming
parametric seq2seq models and task-specific retrieve-and-extract architectures.
For language generation tasks, we find that RAG models generate more specific,
diverse and factual language than a state-of-the-art parametric-only seq2seq
baseline.},
Year          = {2020},
Month         = {May},
Url           = {http://arxiv.org/abs/2005.11401v4},
File          = {2005.11401v4.pdf},
EprintNoVer   = {2005.11401}
}

@article{2004.04906v3,
Author        = {Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
Title         = {Dense Passage Retrieval for Open-Domain Question Answering},
Eprint        = {2004.04906v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Open-domain question answering relies on efficient passage retrieval to
select candidate contexts, where traditional sparse vector space models, such
as TF-IDF or BM25, are the de facto method. In this work, we show that
retrieval can be practically implemented using dense representations alone,
where embeddings are learned from a small number of questions and passages by a
simple dual-encoder framework. When evaluated on a wide range of open-domain QA
datasets, our dense retriever outperforms a strong Lucene-BM25 system largely
by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our
end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.},
Year          = {2020},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2004.04906v3},
File          = {2004.04906v3.pdf},
EprintNoVer   = {2004.04906}
}


@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:160025533},
  abstract={Natural language processing tasks, such as question answering,
  machine translation, reading comprehension, and summarization, are typically
  approached with supervised learning on taskspecific datasets. We demonstrate
  that language models begin to learn these tasks without any explicit
  supervision when trained on a new dataset of millions of webpages called
  WebText. When conditioned on a document plus questions, the answers generated
  by the language model reach 55 F1 on the CoQA dataset matching or exceeding
  the performance of 3 out of 4 baseline systems without using the 127,000+
  training examples. The capacity of the language model is essential to the
  success of zero-shot task transfer and increasing it improves performance in a
  log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter
  Transformer that achieves state of the art results on 7 out of 8 tested
  language modeling datasets in a zero-shot setting but still underfits WebText.
  Samples from the model reflect these improvements and contain coherent
  paragraphs of text. These findings suggest a promising path towards building
  language processing systems which learn to perform tasks from their naturally
  occurring demonstrations.}
}

@article{2308.03958v1,
Author        = {Jerry Wei and Da Huang and Yifeng Lu and Denny Zhou and Quoc V. Le},
Title         = {Simple synthetic data reduces sycophancy in large language models},
Eprint        = {2308.03958v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Sycophancy is an undesirable behavior where models tailor their responses to
follow a human user's view even when that view is not objectively correct
(e.g., adapting liberal views once a user reveals that they are liberal). In
this paper, we study the prevalence of sycophancy in language models and
propose a simple synthetic-data intervention to reduce this behavior.
  First, on a set of three sycophancy tasks (Perez et al., 2022) where models
are asked for an opinion on statements with no correct answers (e.g.,
politics), we observe that both model scaling and instruction tuning
significantly increase sycophancy for PaLM models up to 540B parameters.
Second, we extend sycophancy evaluations to simple addition statements that are
objectively incorrect, finding that despite knowing that these statements are
wrong, language models will still agree with them if the user does as well.
  To reduce sycophancy, we present a straightforward synthetic-data
intervention that takes public NLP tasks and encourages models to be robust to
user opinions on these tasks. Adding these data in a lightweight finetuning
step can significantly reduce sycophantic behavior on held-out prompts. Code
for generating synthetic data for intervention can be found at
https://github.com/google/sycophancy-intervention.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.03958v1},
File          = {2308.03958v1.pdf},
EprintNoVer   = {2308.03958}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}

@article{2308.00352v3,
Author        = {Sirui Hong and Xiawu Zheng and Jonathan Chen and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu},
Title         = {MetaGPT: Meta Programming for Multi-Agent Collaborative Framework},
Eprint        = {2308.00352v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recently, remarkable progress has been made in automated task-solving through
the use of multi-agent driven by large language models (LLMs). However,
existing LLM-based multi-agent works primarily focus on solving simple dialogue
tasks, and complex tasks are rarely studied, mainly due to the LLM
hallucination problem. This type of hallucination becomes cascading when
naively chaining multiple intelligent agents, resulting in a failure to
effectively address complex problems. Therefore, we introduce MetaGPT, an
innovative framework that incorporates efficient human workflows as a meta
programming approach into LLM-based multi-agent collaboration. Specifically,
MetaGPT encodes Standardized Operating Procedures (SOPs) into prompts to
enhance structured coordination. Subsequently, it mandates modular outputs,
empowering agents with domain expertise comparable to human professionals, to
validate outputs and minimize compounded errors. In this way, MetaGPT leverages
the assembly line paradigm to assign diverse roles to various agents, thereby
establishing a framework that can effectively and cohesively deconstruct
complex multi-agent collaborative problems. Our experiments on collaborative
software engineering benchmarks demonstrate that MetaGPT generates more
coherent and correct solutions compared to existing chat-based multi-agent
systems. This highlights the potential of integrating human domain knowledge
into multi-agent systems, thereby creating new opportunities to tackle complex
real-world challenges. The GitHub repository of this project is publicly
available on:https://github.com/geekan/MetaGPT.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.00352v3},
File          = {2308.00352v3.pdf},
EprintNoVer   = {2308.00352}
}

@article{2304.03442v2,
Author        = {Joon Sung Park and Joseph C. O'Brien and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein},
Title         = {Generative Agents: Interactive Simulacra of Human Behavior},
Eprint        = {2304.03442v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Believable proxies of human behavior can empower interactive applications
ranging from immersive environments to rehearsal spaces for interpersonal
communication to prototyping tools. In this paper, we introduce generative
agents--computational software agents that simulate believable human behavior.
Generative agents wake up, cook breakfast, and head to work; artists paint,
while authors write; they form opinions, notice each other, and initiate
conversations; they remember and reflect on days past as they plan the next
day. To enable generative agents, we describe an architecture that extends a
large language model to store a complete record of the agent's experiences
using natural language, synthesize those memories over time into higher-level
reflections, and retrieve them dynamically to plan behavior. We instantiate
generative agents to populate an interactive sandbox environment inspired by
The Sims, where end users can interact with a small town of twenty five agents
using natural language. In an evaluation, these generative agents produce
believable individual and emergent social behaviors: for example, starting with
only a single user-specified notion that one agent wants to throw a Valentine's
Day party, the agents autonomously spread invitations to the party over the
next two days, make new acquaintances, ask each other out on dates to the
party, and coordinate to show up for the party together at the right time. We
demonstrate through ablation that the components of our agent
architecture--observation, planning, and reflection--each contribute critically
to the believability of agent behavior. By fusing large language models with
computational, interactive agents, this work introduces architectural and
interaction patterns for enabling believable simulations of human behavior.},
Year          = {2023},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2304.03442v2},
File          = {2304.03442v2.pdf},
EprintNoVer   = {2304.03442}
}

@article{1910.10683v3,
Author        = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
Title         = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
  Transformer},
Eprint        = {1910.10683v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Transfer learning, where a model is first pre-trained on a data-rich task
before being fine-tuned on a downstream task, has emerged as a powerful
technique in natural language processing (NLP). The effectiveness of transfer
learning has given rise to a diversity of approaches, methodology, and
practice. In this paper, we explore the landscape of transfer learning
techniques for NLP by introducing a unified framework that converts all
text-based language problems into a text-to-text format. Our systematic study
compares pre-training objectives, architectures, unlabeled data sets, transfer
approaches, and other factors on dozens of language understanding tasks. By
combining the insights from our exploration with scale and our new ``Colossal
Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks
covering summarization, question answering, text classification, and more. To
facilitate future work on transfer learning for NLP, we release our data set,
pre-trained models, and code.},
Year          = {2019},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1910.10683v3},
File          = {1910.10683v3.pdf},
EprintNoVer   = {1910.10683}
}

@article{2303.11607v1,
Author        = {Siddique Latif and Aun Zaidi and Heriberto Cuayahuitl and Fahad Shamshad and Moazzam Shoukat and Junaid Qadir},
Title         = {Transformers in Speech Processing: A Survey},
Eprint        = {2303.11607v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The remarkable success of transformers in the field of natural language
processing has sparked the interest of the speech-processing community, leading
to an exploration of their potential for modeling long-range dependencies
within speech sequences. Recently, transformers have gained prominence across
various speech-related domains, including automatic speech recognition, speech
synthesis, speech translation, speech para-linguistics, speech enhancement,
spoken dialogue systems, and numerous multimodal applications. In this paper,
we present a comprehensive survey that aims to bridge research studies from
diverse subfields within speech technology. By consolidating findings from
across the speech technology landscape, we provide a valuable resource for
researchers interested in harnessing the power of transformers to advance the
field. We identify the challenges encountered by transformers in speech
processing while also offering insights into potential solutions to address
these issues.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.11607v1},
File          = {2303.11607v1.pdf},
EprintNoVer   = {2303.11607}
}

@article{2008.03790v1,
Author        = {Christin Jose and Yuriy Mishchenko and Thibaud Senechal and Anish Shah and Alex Escott and Shiv Vitaladevuni},
Title         = {Accurate Detection of Wake Word Start and End Using a CNN},
Eprint        = {2008.03790v1},
DOI           = {10.21437/Interspeech.2020-1491},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.AS},
Abstract      = {Small footprint embedded devices require keyword spotters (KWS) with small
model size and detection latency for enabling voice assistants. Such a keyword
is often referred to as \textit{wake word} as it is used to wake up voice
assistant enabled devices. Together with wake word detection, accurate
estimation of wake word endpoints (start and end) is an important task of KWS.
In this paper, we propose two new methods for detecting the endpoints of wake
words in neural KWS that use single-stage word-level neural networks. Our
results show that the new techniques give superior accuracy for detecting wake
words' endpoints of up to 50 msec standard error versus human annotations, on
par with the conventional Acoustic Model plus HMM forced alignment. To our
knowledge, this is the first study of wake word endpoints detection methods for
single-stage neural KWS.},
Year          = {2020},
Month         = {Aug},
Note          = {Interspeech 2020},
Url           = {http://arxiv.org/abs/2008.03790v1},
File          = {2008.03790v1.pdf},
EprintNoVer   = {2008.03790}
}

@article{2111.00396v3,
Author        = {Albert Gu and Karan Goel and Christopher Ré},
Title         = {Efficiently Modeling Long Sequences with Structured State Spaces},
Eprint        = {2111.00396v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {A central goal of sequence modeling is designing a single principled model
that can address sequence data across a range of modalities and tasks,
particularly on long-range dependencies. Although conventional models including
RNNs, CNNs, and Transformers have specialized variants for capturing long
dependencies, they still struggle to scale to very long sequences of $10000$ or
more steps. A promising recent approach proposed modeling sequences by
simulating the fundamental state space model (SSM) \( x'(t) = Ax(t) + Bu(t),
y(t) = Cx(t) + Du(t) \), and showed that for appropriate choices of the state
matrix \( A \), this system could handle long-range dependencies mathematically
and empirically. However, this method has prohibitive computation and memory
requirements, rendering it infeasible as a general sequence modeling solution.
We propose the Structured State Space sequence model (S4) based on a new
parameterization for the SSM, and show that it can be computed much more
efficiently than prior approaches while preserving their theoretical strengths.
Our technique involves conditioning \( A \) with a low-rank correction,
allowing it to be diagonalized stably and reducing the SSM to the well-studied
computation of a Cauchy kernel. S4 achieves strong empirical results across a
diverse range of established benchmarks, including (i) 91\% accuracy on
sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with
a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on
image and language modeling tasks, while performing generation $60\times$
faster (iii) SoTA on every task from the Long Range Arena benchmark, including
solving the challenging Path-X task of length 16k that all prior work fails on,
while being as efficient as all competitors.},
Year          = {2021},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2111.00396v3},
File          = {2111.00396v3.pdf},
EprintNoVer   = {2111.00396}
}