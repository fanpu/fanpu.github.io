---
---

@article{2209.03430v2,
  author        = {Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency},
  title         = {Foundations and Trends in Multimodal Machine Learning: Principles,
                   Challenges, and Open Questions},
  eprint        = {2209.03430v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Multimodal machine learning is a vibrant multi-disciplinary research field
                   that aims to design computer agents with intelligent capabilities such as
                   understanding, reasoning, and learning through integrating multiple
                   communicative modalities, including linguistic, acoustic, visual, tactile, and
                   physiological messages. With the recent interest in video understanding,
                   embodied autonomous agents, text-to-image generation, and multisensor fusion in
                   application domains such as healthcare and robotics, multimodal machine
                   learning has brought unique computational and theoretical challenges to the
                   machine learning community given the heterogeneity of data sources and the
                   interconnections often found between modalities. However, the breadth of
                   progress in multimodal research has made it difficult to identify the common
                   themes and open questions in the field. By synthesizing a broad range of
                   application domains and theoretical frameworks from both historical and recent
                   perspectives, this paper is designed to provide an overview of the
                   computational and theoretical foundations of multimodal machine learning. We
                   start by defining three key principles of modality heterogeneity, connections,
                   and interactions that have driven subsequent innovations, and propose a
                   taxonomy of six core technical challenges: representation, alignment,
                   reasoning, generation, transference, and quantification covering historical and
                   recent trends. Recent technical achievements will be presented through the lens
                   of this taxonomy, allowing researchers to understand the similarities and
                   differences across new approaches. We end by motivating several open problems
                   for future research as identified by our taxonomy.},
  year          = {2022},
  month         = {Sep},
  url           = {http://arxiv.org/abs/2209.03430v2},
  file          = {2209.03430v2.pdf},
  eprintnover   = {2209.03430}
}

@article{1802.05365v2,
  author        = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  title         = {Deep contextualized word representations},
  eprint        = {1802.05365v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {We introduce a new type of deep contextualized word representation that
                   models both (1) complex characteristics of word use (e.g., syntax and
                   semantics), and (2) how these uses vary across linguistic contexts (i.e., to
                   model polysemy). Our word vectors are learned functions of the internal states
                   of a deep bidirectional language model (biLM), which is pre-trained on a large
                   text corpus. We show that these representations can be easily added to existing
                   models and significantly improve the state of the art across six challenging
                   NLP problems, including question answering, textual entailment and sentiment
                   analysis. We also present an analysis showing that exposing the deep internals
                   of the pre-trained network is crucial, allowing downstream models to mix
                   different types of semi-supervision signals.},
  year          = {2018},
  month         = {Feb},
  url           = {http://arxiv.org/abs/1802.05365v2},
  file          = {1802.05365v2.pdf},
  eprintnover   = {1802.05365}
}

@article{2201.11903v6,
  author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  eprint        = {2201.11903v6},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {We explore how generating a chain of thought -- a series of intermediate
                   reasoning steps -- significantly improves the ability of large language models
                   to perform complex reasoning. In particular, we show how such reasoning
                   abilities emerge naturally in sufficiently large language models via a simple
                   method called chain of thought prompting, where a few chain of thought
                   demonstrations are provided as exemplars in prompting. Experiments on three
                   large language models show that chain of thought prompting improves performance
                   on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
                   empirical gains can be striking. For instance, prompting a 540B-parameter
                   language model with just eight chain of thought exemplars achieves state of the
                   art accuracy on the GSM8K benchmark of math word problems, surpassing even
                   finetuned GPT-3 with a verifier.},
  year          = {2022},
  month         = {Jan},
  url           = {http://arxiv.org/abs/2201.11903v6},
  file          = {2201.11903v6.pdf},
  eprintnover   = {2201.11903}
}

@article{2005.14165v4,
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title         = {Language Models are Few-Shot Learners},
  eprint        = {2005.14165v4},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {Recent work has demonstrated substantial gains on many NLP tasks and
                   benchmarks by pre-training on a large corpus of text followed by fine-tuning on
                   a specific task. While typically task-agnostic in architecture, this method
                   still requires task-specific fine-tuning datasets of thousands or tens of
                   thousands of examples. By contrast, humans can generally perform a new language
                   task from only a few examples or from simple instructions - something which
                   current NLP systems still largely struggle to do. Here we show that scaling up
                   language models greatly improves task-agnostic, few-shot performance, sometimes
                   even reaching competitiveness with prior state-of-the-art fine-tuning
                   approaches. Specifically, we train GPT-3, an autoregressive language model with
                   175 billion parameters, 10x more than any previous non-sparse language model,
                   and test its performance in the few-shot setting. For all tasks, GPT-3 is
                   applied without any gradient updates or fine-tuning, with tasks and few-shot
                   demonstrations specified purely via text interaction with the model. GPT-3
                   achieves strong performance on many NLP datasets, including translation,
                   question-answering, and cloze tasks, as well as several tasks that require
                   on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
                   novel word in a sentence, or performing 3-digit arithmetic. At the same time,
                   we also identify some datasets where GPT-3's few-shot learning still struggles,
                   as well as some datasets where GPT-3 faces methodological issues related to
                   training on large web corpora. Finally, we find that GPT-3 can generate samples
                   of news articles which human evaluators have difficulty distinguishing from
                   articles written by humans. We discuss broader societal impacts of this finding
                   and of GPT-3 in general.},
  year          = {2020},
  month         = {May},
  url           = {http://arxiv.org/abs/2005.14165v4},
  file          = {2005.14165v4.pdf},
  eprintnover   = {2005.14165}
}

@article{1810.04805v2,
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language
                   Understanding},
  eprint        = {1810.04805v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {We introduce a new language representation model called BERT, which stands
                   for Bidirectional Encoder Representations from Transformers. Unlike recent
                   language representation models, BERT is designed to pre-train deep
                   bidirectional representations from unlabeled text by jointly conditioning on
                   both left and right context in all layers. As a result, the pre-trained BERT
                   model can be fine-tuned with just one additional output layer to create
                   state-of-the-art models for a wide range of tasks, such as question answering
                   and language inference, without substantial task-specific architecture
                   modifications.
                   BERT is conceptually simple and empirically powerful. It obtains new
                   state-of-the-art results on eleven natural language processing tasks, including
                   pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
                   accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
                   Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
                   (5.1 point absolute improvement).},
  year          = {2018},
  month         = {Oct},
  url           = {http://arxiv.org/abs/1810.04805v2},
  file          = {1810.04805v2.pdf},
  eprintnover   = {1810.04805}
}

@article{2203.02155v1,
  author        = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
  title         = {Training language models to follow instructions with human feedback},
  eprint        = {2203.02155v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {Making language models bigger does not inherently make them better at
                   following a user's intent. For example, large language models can generate
                   outputs that are untruthful, toxic, or simply not helpful to the user. In other
                   words, these models are not aligned with their users. In this paper, we show an
                   avenue for aligning language models with user intent on a wide range of tasks
                   by fine-tuning with human feedback. Starting with a set of labeler-written
                   prompts and prompts submitted through the OpenAI API, we collect a dataset of
                   labeler demonstrations of the desired model behavior, which we use to fine-tune
                   GPT-3 using supervised learning. We then collect a dataset of rankings of model
                   outputs, which we use to further fine-tune this supervised model using
                   reinforcement learning from human feedback. We call the resulting models
                   InstructGPT. In human evaluations on our prompt distribution, outputs from the
                   1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
                   despite having 100x fewer parameters. Moreover, InstructGPT models show
                   improvements in truthfulness and reductions in toxic output generation while
                   having minimal performance regressions on public NLP datasets. Even though
                   InstructGPT still makes simple mistakes, our results show that fine-tuning with
                   human feedback is a promising direction for aligning language models with human
                   intent.},
  year          = {2022},
  month         = {Mar},
  url           = {http://arxiv.org/abs/2203.02155v1},
  file          = {2203.02155v1.pdf},
  eprintnover   = {2203.02155}
}

@article{2107.03374v2,
  author        = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  title         = {Evaluating Large Language Models Trained on Code},
  eprint        = {2107.03374v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {We introduce Codex, a GPT language model fine-tuned on publicly available
                   code from GitHub, and study its Python code-writing capabilities. A distinct
                   production version of Codex powers GitHub Copilot. On HumanEval, a new
                   evaluation set we release to measure functional correctness for synthesizing
                   programs from docstrings, our model solves 28.8% of the problems, while GPT-3
                   solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling
                   from the model is a surprisingly effective strategy for producing working
                   solutions to difficult prompts. Using this method, we solve 70.2% of our
                   problems with 100 samples per problem. Careful investigation of our model
                   reveals its limitations, including difficulty with docstrings describing long
                   chains of operations and with binding operations to variables. Finally, we
                   discuss the potential broader impacts of deploying powerful code generation
                   technologies, covering safety, security, and economics.},
  year          = {2021},
  month         = {Jul},
  url           = {http://arxiv.org/abs/2107.03374v2},
  file          = {2107.03374v2.pdf},
  eprintnover   = {2107.03374}
}

@article{1910.13461v1,
  author        = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
  title         = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language
                   Generation, Translation, and Comprehension},
  eprint        = {1910.13461v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence
                   models. BART is trained by (1) corrupting text with an arbitrary noising
                   function, and (2) learning a model to reconstruct the original text. It uses a
                   standard Tranformer-based neural machine translation architecture which,
                   despite its simplicity, can be seen as generalizing BERT (due to the
                   bidirectional encoder), GPT (with the left-to-right decoder), and many other
                   more recent pretraining schemes. We evaluate a number of noising approaches,
                   finding the best performance by both randomly shuffling the order of the
                   original sentences and using a novel in-filling scheme, where spans of text are
                   replaced with a single mask token. BART is particularly effective when fine
                   tuned for text generation but also works well for comprehension tasks. It
                   matches the performance of RoBERTa with comparable training resources on GLUE
                   and SQuAD, achieves new state-of-the-art results on a range of abstractive
                   dialogue, question answering, and summarization tasks, with gains of up to 6
                   ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system
                   for machine translation, with only target language pretraining. We also report
                   ablation experiments that replicate other pretraining schemes within the BART
                   framework, to better measure which factors most influence end-task performance.},
  year          = {2019},
  month         = {Oct},
  url           = {http://arxiv.org/abs/1910.13461v1},
  file          = {1910.13461v1.pdf},
  eprintnover   = {1910.13461}
}

@article{2005.11401v4,
  author        = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
  title         = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  eprint        = {2005.11401v4},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {Large pre-trained language models have been shown to store factual knowledge
                   in their parameters, and achieve state-of-the-art results when fine-tuned on
                   downstream NLP tasks. However, their ability to access and precisely manipulate
                   knowledge is still limited, and hence on knowledge-intensive tasks, their
                   performance lags behind task-specific architectures. Additionally, providing
                   provenance for their decisions and updating their world knowledge remain open
                   research problems. Pre-trained models with a differentiable access mechanism to
                   explicit non-parametric memory can overcome this issue, but have so far been
                   only investigated for extractive downstream tasks. We explore a general-purpose
                   fine-tuning recipe for retrieval-augmented generation (RAG) -- models which
                   combine pre-trained parametric and non-parametric memory for language
                   generation. We introduce RAG models where the parametric memory is a
                   pre-trained seq2seq model and the non-parametric memory is a dense vector index
                   of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG
                   formulations, one which conditions on the same retrieved passages across the
                   whole generated sequence, the other can use different passages per token. We
                   fine-tune and evaluate our models on a wide range of knowledge-intensive NLP
                   tasks and set the state-of-the-art on three open domain QA tasks, outperforming
                   parametric seq2seq models and task-specific retrieve-and-extract architectures.
                   For language generation tasks, we find that RAG models generate more specific,
                   diverse and factual language than a state-of-the-art parametric-only seq2seq
                   baseline.},
  year          = {2020},
  month         = {May},
  url           = {http://arxiv.org/abs/2005.11401v4},
  file          = {2005.11401v4.pdf},
  eprintnover   = {2005.11401}
}

@article{2004.04906v3,
  author        = {Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
  title         = {Dense Passage Retrieval for Open-Domain Question Answering},
  eprint        = {2004.04906v3},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {Open-domain question answering relies on efficient passage retrieval to
                   select candidate contexts, where traditional sparse vector space models, such
                   as TF-IDF or BM25, are the de facto method. In this work, we show that
                   retrieval can be practically implemented using dense representations alone,
                   where embeddings are learned from a small number of questions and passages by a
                   simple dual-encoder framework. When evaluated on a wide range of open-domain QA
                   datasets, our dense retriever outperforms a strong Lucene-BM25 system largely
                   by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our
                   end-to-end QA system establish new state-of-the-art on multiple open-domain QA
                   benchmarks.},
  year          = {2020},
  month         = {Apr},
  url           = {http://arxiv.org/abs/2004.04906v3},
  file          = {2004.04906v3.pdf},
  eprintnover   = {2004.04906}
}


@inproceedings{Radford2019LanguageMA,
  title    = {Language Models are Unsupervised Multitask Learners},
  author   = {Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year     = {2019},
  url      = {https://api.semanticscholar.org/CorpusID:160025533},
  abstract = {Natural language processing tasks, such as question answering,
              machine translation, reading comprehension, and summarization, are typically
              approached with supervised learning on taskspecific datasets. We demonstrate
              that language models begin to learn these tasks without any explicit
              supervision when trained on a new dataset of millions of webpages called
              WebText. When conditioned on a document plus questions, the answers generated
              by the language model reach 55 F1 on the CoQA dataset matching or exceeding
              the performance of 3 out of 4 baseline systems without using the 127,000+
              training examples. The capacity of the language model is essential to the
              success of zero-shot task transfer and increasing it improves performance in a
              log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter
              Transformer that achieves state of the art results on 7 out of 8 tested
              language modeling datasets in a zero-shot setting but still underfits WebText.
              Samples from the model reflect these improvements and contain coherent
              paragraphs of text. These findings suggest a promising path towards building
              language processing systems which learn to perform tasks from their naturally
              occurring demonstrations.}
}

@article{2308.03958v1,
  author        = {Jerry Wei and Da Huang and Yifeng Lu and Denny Zhou and Quoc V. Le},
  title         = {Simple synthetic data reduces sycophancy in large language models},
  eprint        = {2308.03958v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {Sycophancy is an undesirable behavior where models tailor their responses to
                   follow a human user's view even when that view is not objectively correct
                   (e.g., adapting liberal views once a user reveals that they are liberal). In
                   this paper, we study the prevalence of sycophancy in language models and
                   propose a simple synthetic-data intervention to reduce this behavior.
                   First, on a set of three sycophancy tasks (Perez et al., 2022) where models
                   are asked for an opinion on statements with no correct answers (e.g.,
                   politics), we observe that both model scaling and instruction tuning
                   significantly increase sycophancy for PaLM models up to 540B parameters.
                   Second, we extend sycophancy evaluations to simple addition statements that are
                   objectively incorrect, finding that despite knowing that these statements are
                   wrong, language models will still agree with them if the user does as well.
                   To reduce sycophancy, we present a straightforward synthetic-data
                   intervention that takes public NLP tasks and encourages models to be robust to
                   user opinions on these tasks. Adding these data in a lightweight finetuning
                   step can significantly reduce sycophantic behavior on held-out prompts. Code
                   for generating synthetic data for intervention can be found at
                   https://github.com/google/sycophancy-intervention.},
  year          = {2023},
  month         = {Aug},
  url           = {http://arxiv.org/abs/2308.03958v1},
  file          = {2308.03958v1.pdf},
  eprintnover   = {2308.03958}
}

@inproceedings{Radford2018ImprovingLU,
  title  = {Improving Language Understanding by Generative Pre-Training},
  author = {Alec Radford and Karthik Narasimhan},
  year   = {2018},
  url    = {https://api.semanticscholar.org/CorpusID:49313245}
}

@article{2308.00352v3,
  author        = {Sirui Hong and Xiawu Zheng and Jonathan Chen and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu},
  title         = {MetaGPT: Meta Programming for Multi-Agent Collaborative Framework},
  eprint        = {2308.00352v3},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  abstract      = {Recently, remarkable progress has been made in automated task-solving through
                   the use of multi-agent driven by large language models (LLMs). However,
                   existing LLM-based multi-agent works primarily focus on solving simple dialogue
                   tasks, and complex tasks are rarely studied, mainly due to the LLM
                   hallucination problem. This type of hallucination becomes cascading when
                   naively chaining multiple intelligent agents, resulting in a failure to
                   effectively address complex problems. Therefore, we introduce MetaGPT, an
                   innovative framework that incorporates efficient human workflows as a meta
                   programming approach into LLM-based multi-agent collaboration. Specifically,
                   MetaGPT encodes Standardized Operating Procedures (SOPs) into prompts to
                   enhance structured coordination. Subsequently, it mandates modular outputs,
                   empowering agents with domain expertise comparable to human professionals, to
                   validate outputs and minimize compounded errors. In this way, MetaGPT leverages
                   the assembly line paradigm to assign diverse roles to various agents, thereby
                   establishing a framework that can effectively and cohesively deconstruct
                   complex multi-agent collaborative problems. Our experiments on collaborative
                   software engineering benchmarks demonstrate that MetaGPT generates more
                   coherent and correct solutions compared to existing chat-based multi-agent
                   systems. This highlights the potential of integrating human domain knowledge
                   into multi-agent systems, thereby creating new opportunities to tackle complex
                   real-world challenges. The GitHub repository of this project is publicly
                   available on:https://github.com/geekan/MetaGPT.},
  year          = {2023},
  month         = {Aug},
  url           = {http://arxiv.org/abs/2308.00352v3},
  file          = {2308.00352v3.pdf},
  eprintnover   = {2308.00352}
}

@article{2304.03442v2,
  author        = {Joon Sung Park and Joseph C. O'Brien and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein},
  title         = {Generative Agents: Interactive Simulacra of Human Behavior},
  eprint        = {2304.03442v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.HC},
  abstract      = {Believable proxies of human behavior can empower interactive applications
                   ranging from immersive environments to rehearsal spaces for interpersonal
                   communication to prototyping tools. In this paper, we introduce generative
                   agents--computational software agents that simulate believable human behavior.
                   Generative agents wake up, cook breakfast, and head to work; artists paint,
                   while authors write; they form opinions, notice each other, and initiate
                   conversations; they remember and reflect on days past as they plan the next
                   day. To enable generative agents, we describe an architecture that extends a
                   large language model to store a complete record of the agent's experiences
                   using natural language, synthesize those memories over time into higher-level
                   reflections, and retrieve them dynamically to plan behavior. We instantiate
                   generative agents to populate an interactive sandbox environment inspired by
                   The Sims, where end users can interact with a small town of twenty five agents
                   using natural language. In an evaluation, these generative agents produce
                   believable individual and emergent social behaviors: for example, starting with
                   only a single user-specified notion that one agent wants to throw a Valentine's
                   Day party, the agents autonomously spread invitations to the party over the
                   next two days, make new acquaintances, ask each other out on dates to the
                   party, and coordinate to show up for the party together at the right time. We
                   demonstrate through ablation that the components of our agent
                   architecture--observation, planning, and reflection--each contribute critically
                   to the believability of agent behavior. By fusing large language models with
                   computational, interactive agents, this work introduces architectural and
                   interaction patterns for enabling believable simulations of human behavior.},
  year          = {2023},
  month         = {Apr},
  url           = {http://arxiv.org/abs/2304.03442v2},
  file          = {2304.03442v2.pdf},
  eprintnover   = {2304.03442}
}

@article{1910.10683v3,
  author        = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title         = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
                   Transformer},
  eprint        = {1910.10683v3},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Transfer learning, where a model is first pre-trained on a data-rich task
                   before being fine-tuned on a downstream task, has emerged as a powerful
                   technique in natural language processing (NLP). The effectiveness of transfer
                   learning has given rise to a diversity of approaches, methodology, and
                   practice. In this paper, we explore the landscape of transfer learning
                   techniques for NLP by introducing a unified framework that converts all
                   text-based language problems into a text-to-text format. Our systematic study
                   compares pre-training objectives, architectures, unlabeled data sets, transfer
                   approaches, and other factors on dozens of language understanding tasks. By
                   combining the insights from our exploration with scale and our new ``Colossal
                   Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks
                   covering summarization, question answering, text classification, and more. To
                   facilitate future work on transfer learning for NLP, we release our data set,
                   pre-trained models, and code.},
  year          = {2019},
  month         = {Oct},
  url           = {http://arxiv.org/abs/1910.10683v3},
  file          = {1910.10683v3.pdf},
  eprintnover   = {1910.10683}
}

@article{2303.11607v1,
  author        = {Siddique Latif and Aun Zaidi and Heriberto Cuayahuitl and Fahad Shamshad and Moazzam Shoukat and Junaid Qadir},
  title         = {Transformers in Speech Processing: A Survey},
  eprint        = {2303.11607v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {The remarkable success of transformers in the field of natural language
                   processing has sparked the interest of the speech-processing community, leading
                   to an exploration of their potential for modeling long-range dependencies
                   within speech sequences. Recently, transformers have gained prominence across
                   various speech-related domains, including automatic speech recognition, speech
                   synthesis, speech translation, speech para-linguistics, speech enhancement,
                   spoken dialogue systems, and numerous multimodal applications. In this paper,
                   we present a comprehensive survey that aims to bridge research studies from
                   diverse subfields within speech technology. By consolidating findings from
                   across the speech technology landscape, we provide a valuable resource for
                   researchers interested in harnessing the power of transformers to advance the
                   field. We identify the challenges encountered by transformers in speech
                   processing while also offering insights into potential solutions to address
                   these issues.},
  year          = {2023},
  month         = {Mar},
  url           = {http://arxiv.org/abs/2303.11607v1},
  file          = {2303.11607v1.pdf},
  eprintnover   = {2303.11607}
}

@article{2008.03790v1,
  author        = {Christin Jose and Yuriy Mishchenko and Thibaud Senechal and Anish Shah and Alex Escott and Shiv Vitaladevuni},
  title         = {Accurate Detection of Wake Word Start and End Using a CNN},
  eprint        = {2008.03790v1},
  doi           = {10.21437/Interspeech.2020-1491},
  archiveprefix = {arXiv},
  primaryclass  = {eess.AS},
  abstract      = {Small footprint embedded devices require keyword spotters (KWS) with small
                   model size and detection latency for enabling voice assistants. Such a keyword
                   is often referred to as \textit{wake word} as it is used to wake up voice
                   assistant enabled devices. Together with wake word detection, accurate
                   estimation of wake word endpoints (start and end) is an important task of KWS.
                   In this paper, we propose two new methods for detecting the endpoints of wake
                   words in neural KWS that use single-stage word-level neural networks. Our
                   results show that the new techniques give superior accuracy for detecting wake
                   words' endpoints of up to 50 msec standard error versus human annotations, on
                   par with the conventional Acoustic Model plus HMM forced alignment. To our
                   knowledge, this is the first study of wake word endpoints detection methods for
                   single-stage neural KWS.},
  year          = {2020},
  month         = {Aug},
  note          = {Interspeech 2020},
  url           = {http://arxiv.org/abs/2008.03790v1},
  file          = {2008.03790v1.pdf},
  eprintnover   = {2008.03790}
}

@article{2111.00396v3,
  author        = {Albert Gu and Karan Goel and Christopher Ré},
  title         = {Efficiently Modeling Long Sequences with Structured State Spaces},
  eprint        = {2111.00396v3},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {A central goal of sequence modeling is designing a single principled model
                   that can address sequence data across a range of modalities and tasks,
                   particularly on long-range dependencies. Although conventional models including
                   RNNs, CNNs, and Transformers have specialized variants for capturing long
                   dependencies, they still struggle to scale to very long sequences of $10000$ or
                   more steps. A promising recent approach proposed modeling sequences by
                   simulating the fundamental state space model (SSM) \( x'(t) = Ax(t) + Bu(t),
                   y(t) = Cx(t) + Du(t) \), and showed that for appropriate choices of the state
                   matrix \( A \), this system could handle long-range dependencies mathematically
                   and empirically. However, this method has prohibitive computation and memory
                   requirements, rendering it infeasible as a general sequence modeling solution.
                   We propose the Structured State Space sequence model (S4) based on a new
                   parameterization for the SSM, and show that it can be computed much more
                   efficiently than prior approaches while preserving their theoretical strengths.
                   Our technique involves conditioning \( A \) with a low-rank correction,
                   allowing it to be diagonalized stably and reducing the SSM to the well-studied
                   computation of a Cauchy kernel. S4 achieves strong empirical results across a
                   diverse range of established benchmarks, including (i) 91\% accuracy on
                   sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with
                   a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on
                   image and language modeling tasks, while performing generation $60\times$
                   faster (iii) SoTA on every task from the Long Range Arena benchmark, including
                   solving the challenging Path-X task of length 16k that all prior work fails on,
                   while being as efficient as all competitors.},
  year          = {2021},
  month         = {Oct},
  url           = {http://arxiv.org/abs/2111.00396v3},
  file          = {2111.00396v3.pdf},
  eprintnover   = {2111.00396}
}

@article{2301.10226v3,
  author        = {John Kirchenbauer and Jonas Geiping and Yuxin Wen and Jonathan Katz and Ian Miers and Tom Goldstein},
  title         = {A Watermark for Large Language Models},
  eprint        = {2301.10226v3},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Potential harms of large language models can be mitigated by watermarking
                   model output, i.e., embedding signals into generated text that are invisible to
                   humans but algorithmically detectable from a short span of tokens. We propose a
                   watermarking framework for proprietary language models. The watermark can be
                   embedded with negligible impact on text quality, and can be detected using an
                   efficient open-source algorithm without access to the language model API or
                   parameters. The watermark works by selecting a randomized set of "green" tokens
                   before a word is generated, and then softly promoting use of green tokens
                   during sampling. We propose a statistical test for detecting the watermark with
                   interpretable p-values, and derive an information-theoretic framework for
                   analyzing the sensitivity of the watermark. We test the watermark using a
                   multi-billion parameter model from the Open Pretrained Transformer (OPT)
                   family, and discuss robustness and security.},
  year          = {2023},
  month         = {Jan},
  url           = {http://arxiv.org/abs/2301.10226v3},
  file          = {2301.10226v3.pdf},
  eprintnover   = {2301.10226}
}

@article{2012.07805v2,
  author        = {Nicholas Carlini and Florian Tramer and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and Ulfar Erlingsson and Alina Oprea and Colin Raffel},
  title         = {Extracting Training Data from Large Language Models},
  eprint        = {2012.07805v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  abstract      = {It has become common to publish large (billion parameter) language models
                   that have been trained on private datasets. This paper demonstrates that in
                   such settings, an adversary can perform a training data extraction attack to
                   recover individual training examples by querying the language model.
                   We demonstrate our attack on GPT-2, a language model trained on scrapes of
                   the public Internet, and are able to extract hundreds of verbatim text
                   sequences from the model's training data. These extracted examples include
                   (public) personally identifiable information (names, phone numbers, and email
                   addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible
                   even though each of the above sequences are included in just one document in
                   the training data.
                   We comprehensively evaluate our extraction attack to understand the factors
                   that contribute to its success. Worryingly, we find that larger models are more
                   vulnerable than smaller models. We conclude by drawing lessons and discussing
                   possible safeguards for training large language models.},
  year          = {2020},
  month         = {Dec},
  url           = {http://arxiv.org/abs/2012.07805v2},
  file          = {2012.07805v2.pdf},
  eprintnover   = {2012.07805}
}

@article{2003.00307v2,
  author        = {Chaoyue Liu and Libin Zhu and Mikhail Belkin},
  title         = {Loss landscapes and optimization in over-parameterized non-linear
                   systems and neural networks},
  eprint        = {2003.00307v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {The success of deep learning is due, to a large extent, to the remarkable
                   effectiveness of gradient-based optimization methods applied to large neural
                   networks. The purpose of this work is to propose a modern view and a general
                   mathematical framework for loss landscapes and efficient optimization in
                   over-parameterized machine learning models and systems of non-linear equations,
                   a setting that includes over-parameterized deep neural networks. Our starting
                   observation is that optimization problems corresponding to such systems are
                   generally not convex, even locally. We argue that instead they satisfy PL*,
                   a variant of the Polyak-Lojasiewicz condition on most (but not all) of the
                   parameter space, which guarantees both the existence of solutions and efficient
                   optimization by (stochastic) gradient descent (SGD/GD). The PL* condition of
                   these systems is closely related to the condition number of the tangent kernel
                   associated to a non-linear system showing how a PL*-based non-linear theory
                   parallels classical analyses of over-parameterized linear equations. We show
                   that wide neural networks satisfy the PL* condition, which explains the
                   (S)GD convergence to a global minimum. Finally we propose a relaxation of the
                   PL* condition applicable to "almost" over-parameterized systems.},
  year          = {2020},
  month         = {Feb},
  url           = {http://arxiv.org/abs/2003.00307v2},
  file          = {2003.00307v2.pdf},
  eprintnover   = {2003.00307}
}

@article{1810.02054v2,
  author        = {Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
  title         = {Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  eprint        = {1810.02054v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {One of the mysteries in the success of neural networks is randomly
                   initialized first order methods like gradient descent can achieve zero training
                   loss even though the objective function is non-convex and non-smooth. This
                   paper demystifies this surprising phenomenon for two-layer fully connected ReLU
                   activated neural networks. For an $m$ hidden node shallow neural network with
                   ReLU activation and $n$ training data, we show as long as $m$ is large enough
                   and no two inputs are parallel, randomly initialized gradient descent converges
                   to a globally optimal solution at a linear convergence rate for the quadratic
                   loss function.
                   Our analysis relies on the following observation: over-parameterization and
                   random initialization jointly restrict every weight vector to be close to its
                   initialization for all iterations, which allows us to exploit a strong
                   convexity-like property to show that gradient descent converges at a global
                   linear rate to the global optimum. We believe these insights are also useful in
                   analyzing deep models and other first order methods.},
  year          = {2018},
  month         = {Oct},
  url           = {http://arxiv.org/abs/1810.02054v2},
  file          = {1810.02054v2.pdf},
  eprintnover   = {1810.02054}
}

@article{1710.10345v5,
  author        = {Daniel Soudry and Elad Hoffer and Mor Shpigel Nacson and Suriya Gunasekar and Nathan Srebro},
  title         = {The Implicit Bias of Gradient Descent on Separable Data},
  eprint        = {1710.10345v5},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  abstract      = {We examine gradient descent on unregularized logistic regression problems,
                   with homogeneous linear predictors on linearly separable datasets. We show the
                   predictor converges to the direction of the max-margin (hard margin SVM)
                   solution. The result also generalizes to other monotone decreasing loss
                   functions with an infimum at infinity, to multi-class problems, and to training
                   a weight layer in a deep network in a certain restricted setting. Furthermore,
                   we show this convergence is very slow, and only logarithmic in the convergence
                   of the loss itself. This can help explain the benefit of continuing to optimize
                   the logistic or cross-entropy loss even after the training error is zero and
                   the training loss is extremely small, and, as we show, even if the validation
                   loss increases. Our methodology can also aid in understanding implicit
                   regularization n more complex models and with other optimization methods.},
  year          = {2017},
  month         = {Oct},
  url           = {http://arxiv.org/abs/1710.10345v5},
  file          = {1710.10345v5.pdf},
  eprintnover   = {1710.10345}
}

@article{1611.03530v2,
  author        = {Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
  title         = {Understanding deep learning requires rethinking generalization},
  eprint        = {1611.03530v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Despite their massive size, successful deep artificial neural networks can
                   exhibit a remarkably small difference between training and test performance.
                   Conventional wisdom attributes small generalization error either to properties
                   of the model family, or to the regularization techniques used during training.
                   Through extensive systematic experiments, we show how these traditional
                   approaches fail to explain why large neural networks generalize well in
                   practice. Specifically, our experiments establish that state-of-the-art
                   convolutional networks for image classification trained with stochastic
                   gradient methods easily fit a random labeling of the training data. This
                   phenomenon is qualitatively unaffected by explicit regularization, and occurs
                   even if we replace the true images by completely unstructured random noise. We
                   corroborate these experimental findings with a theoretical construction showing
                   that simple depth two neural networks already have perfect finite sample
                   expressivity as soon as the number of parameters exceeds the number of data
                   points as it usually does in practice.
                   We interpret our experimental findings by comparison with traditional models.},
  year          = {2016},
  month         = {Nov},
  url           = {http://arxiv.org/abs/1611.03530v2},
  file          = {1611.03530v2.pdf},
  eprintnover   = {1611.03530}
}

@article{2102.09690v2,
  author        = {Tony Z. Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},
  title         = {Calibrate Before Use: Improving Few-Shot Performance of Language Models},
  eprint        = {2102.09690v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {GPT-3 can perform numerous tasks when provided a natural language prompt that
                   contains a few training examples. We show that this type of few-shot learning
                   can be unstable: the choice of prompt format, training examples, and even the
                   order of the training examples can cause accuracy to vary from near chance to
                   near state-of-the-art. We demonstrate that this instability arises from the
                   bias of language models towards predicting certain answers, e.g., those that
                   are placed near the end of the prompt or are common in the pre-training data.
                   To mitigate this, we first estimate the model's bias towards each answer by
                   asking for its prediction when given the training prompt and a content-free
                   test input such as "N/A". We then fit calibration parameters that cause the
                   prediction for this input to be uniform across answers. On a diverse set of
                   tasks, this contextual calibration procedure substantially improves GPT-3 and
                   GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across
                   different choices of the prompt.},
  year          = {2021},
  month         = {Feb},
  url           = {http://arxiv.org/abs/2102.09690v2},
  file          = {2102.09690v2.pdf},
  eprintnover   = {2102.09690}
}

@article{2202.12837v2,
  author        = {Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
  title         = {Rethinking the Role of Demonstrations: What Makes In-Context Learning
                   Work?},
  eprint        = {2202.12837v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {Large language models (LMs) are able to in-context learn -- perform a new
                   task via inference alone by conditioning on a few input-label pairs
                   (demonstrations) and making predictions for new inputs. However, there has been
                   little understanding of how the model learns and which aspects of the
                   demonstrations contribute to end task performance. In this paper, we show that
                   ground truth demonstrations are in fact not required -- randomly replacing
                   labels in the demonstrations barely hurts performance on a range of
                   classification and multi-choce tasks, consistently over 12 different models
                   including GPT-3. Instead, we find that other aspects of the demonstrations are
                   the key drivers of end task performance, including the fact that they provide a
                   few examples of (1) the label space, (2) the distribution of the input text,
                   and (3) the overall format of the sequence. Together, our analysis provides a
                   new way of understanding how and why in-context learning works, while opening
                   up new questions about how much can be learned from large language models
                   through inference alone.},
  year          = {2022},
  month         = {Feb},
  url           = {http://arxiv.org/abs/2202.12837v2},
  file          = {2202.12837v2.pdf},
  eprintnover   = {2202.12837}
}

@article{2108.04106v3,
  author        = {Sewon Min and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
  title         = {Noisy Channel Language Model Prompting for Few-Shot Text Classification},
  eprint        = {2108.04106v3},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {We introduce a noisy channel approach for language model prompting in
                   few-shot text classification. Instead of computing the likelihood of the label
                   given the input (referred as direct models), channel models compute the
                   conditional probability of the input given the label, and are thereby required
                   to explain every word in the input. We use channel models for recently proposed
                   few-shot learning methods with no or very limited updates to the language model
                   parameters, via either in-context demonstration or prompt tuning. Our
                   experiments show that, for both methods, channel models significantly
                   outperform their direct counterparts, which we attribute to their stability,
                   i.e., lower variance and higher worst-case accuracy. We also present extensive
                   ablations that provide recommendations for when to use channel prompt tuning
                   instead of other competitive methods (e.g., direct head tuning): channel prompt
                   tuning is preferred when the number of training examples is small, labels in
                   the training data are imbalanced, or generalization to unseen labels is
                   required.},
  year          = {2021},
  month         = {Aug},
  url           = {http://arxiv.org/abs/2108.04106v3},
  file          = {2108.04106v3.pdf},
  eprintnover   = {2108.04106}
}

@article{2206.12839v3,
  author        = {Disha Shrivastava and Hugo Larochelle and Daniel Tarlow},
  title         = {Repository-Level Prompt Generation for Large Language Models of Code},
  eprint        = {2206.12839v3},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {With the success of large language models (LLMs) of code and their use as
                   code assistants (e.g. Codex used in GitHub Copilot), techniques for introducing
                   domain-specific knowledge in the prompt design process become important. In
                   this work, we propose a framework called Repo-Level Prompt Generator that
                   learns to generate example-specific prompts using prompt proposals. The prompt
                   proposals take context from the entire repository, thereby incorporating both
                   the structure of the repository and the context from other relevant files (e.g.
                   imports, parent class files). Our technique doesn't require any access to the
                   weights of the LLM, making it applicable in cases where we only have black-box
                   access to the LLM. We conduct experiments on the task of single-line
                   code-autocompletion using code repositories taken from Google Code archives. We
                   demonstrate that an oracle constructed from our prompt proposals gives a
                   remarkably high relative improvement of 36% over Codex, showing the quality of
                   these proposals. Further, we show that when we train a model to predict a
                   prompt proposal, we can achieve significant performance gains over Codex and
                   other baselines. We release our code, data, and trained checkpoints at:
                   \url{https://github.com/shrivastavadisha/repo_level_prompt_generation}.},
  year          = {2022},
  month         = {Jun},
  note          = {ICML, 2023},
  url           = {http://arxiv.org/abs/2206.12839v3},
  file          = {2206.12839v3.pdf},
  eprintnover   = {2206.12839}
}

@inproceedings{stochastic-parrots,
  author    = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  title     = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜},
  year      = {2021},
  isbn      = {9781450383097},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3442188.3445922},
  doi       = {10.1145/3442188.3445922},
  abstract  = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages     = {610–623},
  numpages  = {14},
  location  = {Virtual Event, Canada},
  series    = {FAccT '21}
}

@article{2208.01618v1,
  author        = {Rinon Gal and Yuval Alaluf and Yuval Atzmon and Or Patashnik and Amit H. Bermano and Gal Chechik and Daniel Cohen-Or},
  title         = {An Image is Worth One Word: Personalizing Text-to-Image Generation using
                   Textual Inversion},
  eprint        = {2208.01618v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  abstract      = {Text-to-image models offer unprecedented freedom to guide creation through
                   natural language. Yet, it is unclear how such freedom can be exercised to
                   generate images of specific unique concepts, modify their appearance, or
                   compose them in new roles and novel scenes. In other words, we ask: how can we
                   use language-guided models to turn our cat into a painting, or imagine a new
                   product based on our favorite toy? Here we present a simple approach that
                   allows such creative freedom. Using only 3-5 images of a user-provided concept,
                   like an object or a style, we learn to represent it through new "words" in the
                   embedding space of a frozen text-to-image model. These "words" can be composed
                   into natural language sentences, guiding personalized creation in an intuitive
                   way. Notably, we find evidence that a single word embedding is sufficient for
                   capturing unique and varied concepts. We compare our approach to a wide range
                   of baselines, and demonstrate that it can more faithfully portray the concepts
                   across a range of applications and tasks.
                   Our code, data and new words will be available at:
                   https://textual-inversion.github.io},
  year          = {2022},
  month         = {Aug},
  url           = {http://arxiv.org/abs/2208.01618v1},
  file          = {2208.01618v1.pdf},
  eprintnover   = {2208.01618}
}

@article{2211.09800v2,
  author        = {Tim Brooks and Aleksander Holynski and Alexei A. Efros},
  title         = {InstructPix2Pix: Learning to Follow Image Editing Instructions},
  eprint        = {2211.09800v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  abstract      = {We propose a method for editing images from human instructions: given an
                   input image and a written instruction that tells the model what to do, our
                   model follows these instructions to edit the image. To obtain training data for
                   this problem, we combine the knowledge of two large pretrained models -- a
                   language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
                   generate a large dataset of image editing examples. Our conditional diffusion
                   model, InstructPix2Pix, is trained on our generated data, and generalizes to
                   real images and user-written instructions at inference time. Since it performs
                   edits in the forward pass and does not require per example fine-tuning or
                   inversion, our model edits images quickly, in a matter of seconds. We show
                   compelling editing results for a diverse collection of input images and written
                   instructions.},
  year          = {2022},
  month         = {Nov},
  url           = {http://arxiv.org/abs/2211.09800v2},
  file          = {2211.09800v2.pdf},
  eprintnover   = {2211.09800}
}

@article{2302.03027v1,
  author        = {Gaurav Parmar and Krishna Kumar Singh and Richard Zhang and Yijun Li and Jingwan Lu and Jun-Yan Zhu},
  title         = {Zero-shot Image-to-Image Translation},
  eprint        = {2302.03027v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  abstract      = {Large-scale text-to-image generative models have shown their remarkable
                   ability to synthesize diverse and high-quality images. However, it is still
                   challenging to directly apply these models for editing real images for two
                   reasons. First, it is hard for users to come up with a perfect text prompt that
                   accurately describes every visual detail in the input image. Second, while
                   existing models can introduce desirable changes in certain regions, they often
                   dramatically alter the input content and introduce unexpected changes in
                   unwanted regions. In this work, we propose pix2pix-zero, an image-to-image
                   translation method that can preserve the content of the original image without
                   manual prompting. We first automatically discover editing directions that
                   reflect desired edits in the text embedding space. To preserve the general
                   content structure after editing, we further propose cross-attention guidance,
                   which aims to retain the cross-attention maps of the input image throughout the
                   diffusion process. In addition, our method does not need additional training
                   for these edits and can directly use the existing pre-trained text-to-image
                   diffusion model. We conduct extensive experiments and show that our method
                   outperforms existing and concurrent works for both real and synthetic image
                   editing.},
  year          = {2023},
  month         = {Feb},
  url           = {http://arxiv.org/abs/2302.03027v1},
  file          = {2302.03027v1.pdf},
  eprintnover   = {2302.03027}
}

@article{2307.15043v1,
  author        = {Andy Zou and Zifan Wang and J. Zico Kolter and Matt Fredrikson},
  title         = {Universal and Transferable Adversarial Attacks on Aligned Language
                   Models},
  eprint        = {2307.15043v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {Because "out-of-the-box" large language models are capable of generating a
                   great deal of objectionable content, recent work has focused on aligning these
                   models in an attempt to prevent undesirable generation. While there has been
                   some success at circumventing these measures -- so-called "jailbreaks" against
                   LLMs -- these attacks have required significant human ingenuity and are brittle
                   in practice. In this paper, we propose a simple and effective attack method
                   that causes aligned language models to generate objectionable behaviors.
                   Specifically, our approach finds a suffix that, when attached to a wide range
                   of queries for an LLM to produce objectionable content, aims to maximize the
                   probability that the model produces an affirmative response (rather than
                   refusing to answer). However, instead of relying on manual engineering, our
                   approach automatically produces these adversarial suffixes by a combination of
                   greedy and gradient-based search techniques, and also improves over past
                   automatic prompt generation methods.
                   Surprisingly, we find that the adversarial prompts generated by our approach
                   are quite transferable, including to black-box, publicly released LLMs.
                   Specifically, we train an adversarial attack suffix on multiple prompts (i.e.,
                   queries asking for many different types of objectionable content), as well as
                   multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting
                   attack suffix is able to induce objectionable content in the public interfaces
                   to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,
                   Pythia, Falcon, and others. In total, this work significantly advances the
                   state-of-the-art in adversarial attacks against aligned language models,
                   raising important questions about how such systems can be prevented from
                   producing objectionable information. Code is available at
                   github.com/llm-attacks/llm-attacks.},
  year          = {2023},
  month         = {Jul},
  url           = {http://arxiv.org/abs/2307.15043v1},
  file          = {2307.15043v1.pdf},
  eprintnover   = {2307.15043}
}

@article{2112.10752v2,
  author        = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
  title         = {High-Resolution Image Synthesis with Latent Diffusion Models},
  eprint        = {2112.10752v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  abstract      = {By decomposing the image formation process into a sequential application of
                   denoising autoencoders, diffusion models (DMs) achieve state-of-the-art
                   synthesis results on image data and beyond. Additionally, their formulation
                   allows for a guiding mechanism to control the image generation process without
                   retraining. However, since these models typically operate directly in pixel
                   space, optimization of powerful DMs often consumes hundreds of GPU days and
                   inference is expensive due to sequential evaluations. To enable DM training on
                   limited computational resources while retaining their quality and flexibility,
                   we apply them in the latent space of powerful pretrained autoencoders. In
                   contrast to previous work, training diffusion models on such a representation
                   allows for the first time to reach a near-optimal point between complexity
                   reduction and detail preservation, greatly boosting visual fidelity. By
                   introducing cross-attention layers into the model architecture, we turn
                   diffusion models into powerful and flexible generators for general conditioning
                   inputs such as text or bounding boxes and high-resolution synthesis becomes
                   possible in a convolutional manner. Our latent diffusion models (LDMs) achieve
                   a new state of the art for image inpainting and highly competitive performance
                   on various tasks, including unconditional image generation, semantic scene
                   synthesis, and super-resolution, while significantly reducing computational
                   requirements compared to pixel-based DMs. Code is available at
                   https://github.com/CompVis/latent-diffusion .},
  year          = {2021},
  month         = {Dec},
  url           = {http://arxiv.org/abs/2112.10752v2},
  file          = {2112.10752v2.pdf},
  eprintnover   = {2112.10752}
}

@article{2310.03533v4,
  author        = {Angela Fan and Beliz Gokkaya and Mark Harman and Mitya Lyubarskiy and Shubho Sengupta and Shin Yoo and Jie M. Zhang},
  title         = {Large Language Models for Software Engineering: Survey and Open Problems},
  eprint        = {2310.03533v4},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  abstract      = {This paper provides a survey of the emerging area of Large Language Models
                   (LLMs) for Software Engineering (SE). It also sets out open research challenges
                   for the application of LLMs to technical problems faced by software engineers.
                   LLMs' emergent properties bring novelty and creativity with applications right
                   across the spectrum of Software Engineering activities including coding,
                   design, requirements, repair, refactoring, performance improvement,
                   documentation and analytics. However, these very same emergent properties also
                   pose significant technical challenges; we need techniques that can reliably
                   weed out incorrect solutions, such as hallucinations. Our survey reveals the
                   pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in
                   the development and deployment of reliable, efficient and effective LLM-based
                   SE.},
  year          = {2023},
  month         = {Oct},
  url           = {http://arxiv.org/abs/2310.03533v4},
  file          = {2310.03533v4.pdf},
  eprintnover   = {2310.03533}
}

@article{2312.00752v1,
  author        = {Albert Gu and Tri Dao},
  title         = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  eprint        = {2312.00752v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Foundation models, now powering most of the exciting applications in deep
                   learning, are almost universally based on the Transformer architecture and its
                   core attention module. Many subquadratic-time architectures such as linear
                   attention, gated convolution and recurrent models, and structured state space
                   models (SSMs) have been developed to address Transformers' computational
                   inefficiency on long sequences, but they have not performed as well as
                   attention on important modalities such as language. We identify that a key
                   weakness of such models is their inability to perform content-based reasoning,
                   and make several improvements. First, simply letting the SSM parameters be
                   functions of the input addresses their weakness with discrete modalities,
                   allowing the model to selectively propagate or forget information along the
                   sequence length dimension depending on the current token. Second, even though
                   this change prevents the use of efficient convolutions, we design a
                   hardware-aware parallel algorithm in recurrent mode. We integrate these
                   selective SSMs into a simplified end-to-end neural network architecture without
                   attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$
                   higher throughput than Transformers) and linear scaling in sequence length, and
                   its performance improves on real data up to million-length sequences. As a
                   general sequence model backbone, Mamba achieves state-of-the-art performance
                   across several modalities such as language, audio, and genomics. On language
                   modeling, our Mamba-3B model outperforms Transformers of the same size and
                   matches Transformers twice its size, both in pretraining and downstream
                   evaluation.},
  year          = {2023},
  month         = {Dec},
  url           = {http://arxiv.org/abs/2312.00752v1},
  file          = {2312.00752v1.pdf},
  eprintnover   = {2312.00752}
}

@article{2209.11895v1,
  author        = {Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
  title         = {In-context Learning and Induction Heads},
  eprint        = {2209.11895v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {"Induction heads" are attention heads that implement a simple algorithm to
                   complete token sequences like [A][B] ... [A] -> [B]. In this work, we present
                   preliminary and indirect evidence for a hypothesis that induction heads might
                   constitute the mechanism for the majority of all "in-context learning" in large
                   transformer models (i.e. decreasing loss at increasing token indices). We find
                   that induction heads develop at precisely the same point as a sudden sharp
                   increase in in-context learning ability, visible as a bump in the training
                   loss. We present six complementary lines of evidence, arguing that induction
                   heads may be the mechanistic source of general in-context learning in
                   transformer models of any size. For small attention-only models, we present
                   strong, causal evidence; for larger models with MLPs, we present correlational
                   evidence.},
  year          = {2022},
  month         = {Sep},
  url           = {http://arxiv.org/abs/2209.11895v1},
  file          = {2209.11895v1.pdf},
  eprintnover   = {2209.11895}
}

@article{2205.13147v4,
  author        = {Aditya Kusupati and Gantavya Bhatt and Aniket Rege and Matthew Wallingford and Aditya Sinha and Vivek Ramanujan and William Howard-Snyder and Kaifeng Chen and Sham Kakade and Prateek Jain and Ali Farhadi},
  title         = {Matryoshka Representation Learning},
  eprint        = {2205.13147v4},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Learned representations are a central component in modern ML systems, serving
                   a multitude of downstream tasks. When training such representations, it is
                   often the case that computational and statistical constraints for each
                   downstream task are unknown. In this context rigid, fixed capacity
                   representations can be either over or under-accommodating to the task at hand.
                   This leads us to ask: can we design a flexible representation that can adapt to
                   multiple downstream tasks with varying computational resources? Our main
                   contribution is Matryoshka Representation Learning (MRL) which encodes
                   information at different granularities and allows a single embedding to adapt
                   to the computational constraints of downstream tasks. MRL minimally modifies
                   existing representation learning pipelines and imposes no additional cost
                   during inference and deployment. MRL learns coarse-to-fine representations that
                   are at least as accurate and rich as independently trained low-dimensional
                   representations. The flexibility within the learned Matryoshka Representations
                   offer: (a) up to 14x smaller embedding size for ImageNet-1K classification at
                   the same level of accuracy; (b) up to 14x real-world speed-ups for large-scale
                   retrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for
                   long-tail few-shot classification, all while being as robust as the original
                   representations. Finally, we show that MRL extends seamlessly to web-scale
                   datasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet),
                   vision + language (ALIGN) and language (BERT). MRL code and pretrained models
                   are open-sourced at https://github.com/RAIVNLab/MRL.},
  year          = {2022},
  month         = {May},
  url           = {http://arxiv.org/abs/2205.13147v4},
  file          = {2205.13147v4.pdf},
  eprintnover   = {2205.13147}
}

@article{2004.12832v2,
  author        = {Omar Khattab and Matei Zaharia},
  title         = {ColBERT: Efficient and Effective Passage Search via Contextualized Late
                   Interaction over BERT},
  eprint        = {2004.12832v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  abstract      = {Recent progress in Natural Language Understanding (NLU) is driving fast-paced
                   advances in Information Retrieval (IR), largely owed to fine-tuning deep
                   language models (LMs) for document ranking. While remarkably effective, the
                   ranking models based on these LMs increase computational cost by orders of
                   magnitude over prior approaches, particularly as they must feed each
                   query-document pair through a massive neural network to compute a single
                   relevance score. To tackle this, we present ColBERT, a novel ranking model that
                   adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT
                   introduces a late interaction architecture that independently encodes the query
                   and the document using BERT and then employs a cheap yet powerful interaction
                   step that models their fine-grained similarity. By delaying and yet retaining
                   this fine-granular interaction, ColBERT can leverage the expressiveness of deep
                   LMs while simultaneously gaining the ability to pre-compute document
                   representations offline, considerably speeding up query processing. Beyond
                   reducing the cost of re-ranking the documents retrieved by a traditional model,
                   ColBERT's pruning-friendly interaction mechanism enables leveraging
                   vector-similarity indexes for end-to-end retrieval directly from a large
                   document collection. We extensively evaluate ColBERT using two recent passage
                   search datasets. Results show that ColBERT's effectiveness is competitive with
                   existing BERT-based models (and outperforms every non-BERT baseline), while
                   executing two orders-of-magnitude faster and requiring four orders-of-magnitude
                   fewer FLOPs per query.},
  year          = {2020},
  month         = {Apr},
  url           = {http://arxiv.org/abs/2004.12832v2},
  file          = {2004.12832v2.pdf},
  eprintnover   = {2004.12832}
}

@article{2310.03714v1,
  author        = {Omar Khattab and Arnav Singhvi and Paridhi Maheshwari and Zhiyuan Zhang and Keshav Santhanam and Sri Vardhamanan and Saiful Haq and Ashutosh Sharma and Thomas T. Joshi and Hanna Moazam and Heather Miller and Matei Zaharia and Christopher Potts},
  title         = {DSPy: Compiling Declarative Language Model Calls into Self-Improving
                   Pipelines},
  eprint        = {2310.03714v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {The ML community is rapidly exploring techniques for prompting language
                   models (LMs) and for stacking them into pipelines that solve complex tasks.
                   Unfortunately, existing LM pipelines are typically implemented using hard-coded
                   "prompt templates", i.e. lengthy strings discovered via trial and error. Toward
                   a more systematic approach for developing and optimizing LM pipelines, we
                   introduce DSPy, a programming model that abstracts LM pipelines as text
                   transformation graphs, i.e. imperative computational graphs where LMs are
                   invoked through declarative modules. DSPy modules are parameterized, meaning
                   they can learn (by creating and collecting demonstrations) how to apply
                   compositions of prompting, finetuning, augmentation, and reasoning techniques.
                   We design a compiler that will optimize any DSPy pipeline to maximize a given
                   metric. We conduct two case studies, showing that succinct DSPy programs can
                   express and optimize sophisticated LM pipelines that reason about math word
                   problems, tackle multi-hop retrieval, answer complex questions, and control
                   agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and
                   llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot
                   prompting (generally by over 25% and 65%, respectively) and pipelines with
                   expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top
                   of that, DSPy programs compiled to open and relatively small LMs like
                   770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely
                   on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at
                   https://github.com/stanfordnlp/dspy},
  year          = {2023},
  month         = {Oct},
  url           = {http://arxiv.org/abs/2310.03714v1},
  file          = {2310.03714v1.pdf},
  eprintnover   = {2310.03714}
}

@article{2001.08361v1,
  author        = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  title         = {Scaling Laws for Neural Language Models},
  eprint        = {2001.08361v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {We study empirical scaling laws for language model performance on the
                   cross-entropy loss. The loss scales as a power-law with model size, dataset
                   size, and the amount of compute used for training, with some trends spanning
                   more than seven orders of magnitude. Other architectural details such as
                   network width or depth have minimal effects within a wide range. Simple
                   equations govern the dependence of overfitting on model/dataset size and the
                   dependence of training speed on model size. These relationships allow us to
                   determine the optimal allocation of a fixed compute budget. Larger models are
                   significantly more sample-efficient, such that optimally compute-efficient
                   training involves training very large models on a relatively modest amount of
                   data and stopping significantly before convergence.},
  year          = {2020},
  month         = {Jan},
  url           = {http://arxiv.org/abs/2001.08361v1},
  file          = {2001.08361v1.pdf},
  eprintnover   = {2001.08361}
}

@article{2203.15556v1,
  author        = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
  title         = {Training Compute-Optimal Large Language Models},
  eprint        = {2203.15556v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {We investigate the optimal model size and number of tokens for training a
                   transformer language model under a given compute budget. We find that current
                   large language models are significantly undertrained, a consequence of the
                   recent focus on scaling language models whilst keeping the amount of training
                   data constant. By training over 400 language models ranging from 70 million to
                   over 16 billion parameters on 5 to 500 billion tokens, we find that for
                   compute-optimal training, the model size and the number of training tokens
                   should be scaled equally: for every doubling of model size the number of
                   training tokens should also be doubled. We test this hypothesis by training a
                   predicted compute-optimal model, Chinchilla, that uses the same compute budget
                   as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla
                   uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1
                   (178B), and Megatron-Turing NLG (530B) on a large range of downstream
                   evaluation tasks. This also means that Chinchilla uses substantially less
                   compute for fine-tuning and inference, greatly facilitating downstream usage.
                   As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
                   on the MMLU benchmark, greater than a 7% improvement over Gopher.},
  year          = {2022},
  month         = {Mar},
  url           = {http://arxiv.org/abs/2203.15556v1},
  file          = {2203.15556v1.pdf},
  eprintnover   = {2203.15556}
}

@article{2106.09685v2,
  author        = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  eprint        = {2106.09685v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {An important paradigm of natural language processing consists of large-scale
                   pre-training on general domain data and adaptation to particular tasks or
                   domains. As we pre-train larger models, full fine-tuning, which retrains all
                   model parameters, becomes less feasible. Using GPT-3 175B as an example --
                   deploying independent instances of fine-tuned models, each with 175B
                   parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or
                   LoRA, which freezes the pre-trained model weights and injects trainable rank
                   decomposition matrices into each layer of the Transformer architecture, greatly
                   reducing the number of trainable parameters for downstream tasks. Compared to
                   GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable
                   parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA
                   performs on-par or better than fine-tuning in model quality on RoBERTa,
                   DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher
                   training throughput, and, unlike adapters, no additional inference latency. We
                   also provide an empirical investigation into rank-deficiency in language model
                   adaptation, which sheds light on the efficacy of LoRA. We release a package
                   that facilitates the integration of LoRA with PyTorch models and provide our
                   implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at
                   https://github.com/microsoft/LoRA.},
  year          = {2021},
  month         = {Jun},
  url           = {http://arxiv.org/abs/2106.09685v2},
  file          = {2106.09685v2.pdf},
  eprintnover   = {2106.09685}
}
@article{2402.13753v1,
  author        = {Yiran Ding and Li Lyna Zhang and Chengruidong Zhang and Yuanyuan Xu and Ning Shang and Jiahang Xu and Fan Yang and Mao Yang},
  title         = {LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens},
  eprint        = {2402.13753v1},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {Large context window is a desirable feature in large language models (LLMs).
                   However, due to high fine-tuning costs, scarcity of long texts, and
                   catastrophic values introduced by new token positions, current extended context
                   windows are limited to around 128k tokens. This paper introduces LongRoPE that,
                   for the first time, extends the context window of pre-trained LLMs to an
                   impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k
                   training lengths, while maintaining performance at the original short context
                   window. This is achieved by three key innovations: (i) we identify and exploit
                   two forms of non-uniformities in positional interpolation through an efficient
                   search, providing a better initialization for fine-tuning and enabling an 8x
                   extension in non-fine-tuning scenarios; (ii) we introduce a progressive
                   extension strategy that first fine-tunes a 256k length LLM and then conducts a
                   second positional interpolation on the fine-tuned extended LLM to achieve a
                   2048k context window; (iii) we readjust LongRoPE on 8k length to recover the
                   short context window performance. Extensive experiments on LLaMA2 and Mistral
                   across various tasks demonstrate the effectiveness of our method. Models
                   extended via LongRoPE retain the original architecture with minor modifications
                   to the positional embedding, and can reuse most pre-existing optimizations.},
  year          = {2024},
  month         = {Feb},
  url           = {http://arxiv.org/abs/2402.13753v1},
  file          = {2402.13753v1.pdf},
  eprintnover   = {2402.13753}
}

@article{2104.09864v5,
Author        = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
Title         = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
Eprint        = {2104.09864v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Position encoding recently has shown effective in the transformer
architecture. It enables valuable supervision for dependency modeling between
elements at different positions of the sequence. In this paper, we first
investigate various methods to integrate positional information into the
learning process of transformer-based language models. Then, we propose a novel
method named Rotary Position Embedding(RoPE) to effectively leverage the
positional information. Specifically, the proposed RoPE encodes the absolute
position with a rotation matrix and meanwhile incorporates the explicit
relative position dependency in self-attention formulation. Notably, RoPE
enables valuable properties, including the flexibility of sequence length,
decaying inter-token dependency with increasing relative distances, and the
capability of equipping the linear self-attention with relative position
encoding. Finally, we evaluate the enhanced transformer with rotary position
embedding, also called RoFormer, on various long text classification benchmark
datasets. Our experiments show that it consistently overcomes its alternatives.
Furthermore, we provide a theoretical analysis to explain some experimental
results. RoFormer is already integrated into Huggingface:
\url{https://huggingface.co/docs/transformers/model_doc/roformer}.},
Year          = {2021},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2104.09864v5},
File          = {2104.09864v5.pdf},
EprintNoVer   = {2104.09864}
}

@article{2101.00190v1,
Author        = {Xiang Lisa Li and Percy Liang},
Title         = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
Eprint        = {2101.00190v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Fine-tuning is the de facto way to leverage large pretrained language models
to perform downstream tasks. However, it modifies all the language model
parameters and therefore necessitates storing a full copy for each task. In
this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning
for natural language generation tasks, which keeps language model parameters
frozen, but optimizes a small continuous task-specific vector (called the
prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent
tokens to attend to this prefix as if it were "virtual tokens". We apply
prefix-tuning to GPT-2 for table-to-text generation and to BART for
summarization. We find that by learning only 0.1\% of the parameters,
prefix-tuning obtains comparable performance in the full data setting,
outperforms fine-tuning in low-data settings, and extrapolates better to
examples with topics unseen during training.},
Year          = {2021},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2101.00190v1},
File          = {2101.00190v1.pdf},
EprintNoVer   = {2101.00190}
}

@article{2309.03409v3,
Author        = {Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},
Title         = {Large Language Models as Optimizers},
Eprint        = {2309.03409v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Optimization is ubiquitous. While derivative-based algorithms have been
powerful tools for various problems, the absence of gradient imposes challenges
on many real-world applications. In this work, we propose Optimization by
PROmpting (OPRO), a simple and effective approach to leverage large language
models (LLMs) as optimizers, where the optimization task is described in
natural language. In each optimization step, the LLM generates new solutions
from the prompt that contains previously generated solutions with their values,
then the new solutions are evaluated and added to the prompt for the next
optimization step. We first showcase OPRO on linear regression and traveling
salesman problems, then move on to our main application in prompt optimization,
where the goal is to find instructions that maximize the task accuracy. With a
variety of LLMs, we demonstrate that the best prompts optimized by OPRO
outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on
Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.03409v3},
File          = {2309.03409v3.pdf},
EprintNoVer   = {2309.03409}
}

@article{2406.11695v1,
Author        = {Krista Opsahl-Ong and Michael J Ryan and Josh Purtell and David Broman and Christopher Potts and Matei Zaharia and Omar Khattab},
Title         = {Optimizing Instructions and Demonstrations for Multi-Stage Language
  Model Programs},
Eprint        = {2406.11695v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Language Model Programs, i.e. sophisticated pipelines of modular language
model (LM) calls, are increasingly advancing NLP tasks, but they require
crafting prompts that are jointly effective for all modules. We study prompt
optimization for LM programs, i.e. how to update these prompts to maximize a
downstream metric without access to module-level labels or gradients. To make
this tractable, we factorize our problem into optimizing the free-form
instructions and few-shot demonstrations of every module and introduce several
strategies to craft task-grounded instructions and navigate credit assignment
across modules. Our strategies include (i) program- and data-aware techniques
for proposing effective instructions, (ii) a stochastic mini-batch evaluation
function for learning a surrogate model of our objective, and (iii) a
meta-optimization procedure in which we refine how LMs construct proposals over
time. Using these insights we develop MIPRO, a novel optimizer that outperforms
baselines on five of six diverse LM programs using a best-in-class open-source
model (Llama-3-8B), by as high as 12.9% accuracy. We will release our new
optimizers and benchmark in DSPy at https://github.com/stanfordnlp/dspy},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.11695v1},
File          = {2406.11695v1.pdf},
EprintNoVer   = {2406.11695}
}

@article{2312.09390v1,
Author        = {Collin Burns and Pavel Izmailov and Jan Hendrik Kirchner and Bowen Baker and Leo Gao and Leopold Aschenbrenner and Yining Chen and Adrien Ecoffet and Manas Joglekar and Jan Leike and Ilya Sutskever and Jeff Wu},
Title         = {Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak
  Supervision},
Eprint        = {2312.09390v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Widely used alignment techniques, such as reinforcement learning from human
feedback (RLHF), rely on the ability of humans to supervise model behavior -
for example, to evaluate whether a model faithfully followed instructions or
generated safe outputs. However, future superhuman models will behave in
complex ways too difficult for humans to reliably evaluate; humans will only be
able to weakly supervise superhuman models. We study an analogy to this
problem: can weak model supervision elicit the full capabilities of a much
stronger model? We test this using a range of pretrained language models in the
GPT-4 family on natural language processing (NLP), chess, and reward modeling
tasks. We find that when we naively finetune strong pretrained models on labels
generated by a weak model, they consistently perform better than their weak
supervisors, a phenomenon we call weak-to-strong generalization. However, we
are still far from recovering the full capabilities of strong models with naive
finetuning alone, suggesting that techniques like RLHF may scale poorly to
superhuman models without further work. We find that simple methods can often
significantly improve weak-to-strong generalization: for example, when
finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence
loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our
results suggest that it is feasible to make empirical progress today on a
fundamental challenge of aligning superhuman models.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.09390v1},
File          = {2312.09390v1.pdf},
EprintNoVer   = {2312.09390}
}

@article{2212.08073v1,
Author        = {Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
Title         = {Constitutional AI: Harmlessness from AI Feedback},
Eprint        = {2212.08073v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {As AI systems become more capable, we would like to enlist their help to
supervise other AIs. We experiment with methods for training a harmless AI
assistant through self-improvement, without any human labels identifying
harmful outputs. The only human oversight is provided through a list of rules
or principles, and so we refer to the method as 'Constitutional AI'. The
process involves both a supervised learning and a reinforcement learning phase.
In the supervised phase we sample from an initial model, then generate
self-critiques and revisions, and then finetune the original model on revised
responses. In the RL phase, we sample from the finetuned model, use a model to
evaluate which of the two samples is better, and then train a preference model
from this dataset of AI preferences. We then train with RL using the preference
model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a
result we are able to train a harmless but non-evasive AI assistant that
engages with harmful queries by explaining its objections to them. Both the SL
and RL methods can leverage chain-of-thought style reasoning to improve the
human-judged performance and transparency of AI decision making. These methods
make it possible to control AI behavior more precisely and with far fewer human
labels.},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.08073v1},
File          = {2212.08073v1.pdf},
EprintNoVer   = {2212.08073}
}

@article{1503.02531v1,
Author        = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
Title         = {Distilling the Knowledge in a Neural Network},
Eprint        = {1503.02531v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {A very simple way to improve the performance of almost any machine learning
algorithm is to train many different models on the same data and then to
average their predictions. Unfortunately, making predictions using a whole
ensemble of models is cumbersome and may be too computationally expensive to
allow deployment to a large number of users, especially if the individual
models are large neural nets. Caruana and his collaborators have shown that it
is possible to compress the knowledge in an ensemble into a single model which
is much easier to deploy and we develop this approach further using a different
compression technique. We achieve some surprising results on MNIST and we show
that we can significantly improve the acoustic model of a heavily used
commercial system by distilling the knowledge in an ensemble of models into a
single model. We also introduce a new type of ensemble composed of one or more
full models and many specialist models which learn to distinguish fine-grained
classes that the full models confuse. Unlike a mixture of experts, these
specialist models can be trained rapidly and in parallel.},
Year          = {2015},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1503.02531v1},
File          = {1503.02531v1.pdf},
EprintNoVer   = {1503.02531}
}

@article{2112.01008v1,
Author        = {Shibani Santurkar and Dimitris Tsipras and Mahalaxmi Elango and David Bau and Antonio Torralba and Aleksander Madry},
Title         = {Editing a classifier by rewriting its prediction rules},
Eprint        = {2112.01008v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We present a methodology for modifying the behavior of a classifier by
directly rewriting its prediction rules. Our approach requires virtually no
additional data collection and can be applied to a variety of settings,
including adapting a model to new environments, and modifying it to ignore
spurious features. Our code is available at
https://github.com/MadryLab/EditingClassifiers .},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.01008v1},
File          = {2112.01008v1.pdf},
EprintNoVer   = {2112.01008}
}

@article{2202.06856v2,
Author        = {Elan Rosenfeld and Pradeep Ravikumar and Andrej Risteski},
Title         = {Domain-Adjusted Regression or: ERM May Already Learn Features Sufficient
  for Out-of-Distribution Generalization},
Eprint        = {2202.06856v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {A common explanation for the failure of deep networks to generalize
out-of-distribution is that they fail to recover the "correct" features. We
challenge this notion with a simple experiment which suggests that ERM already
learns sufficient features and that the current bottleneck is not feature
learning, but robust regression. Our findings also imply that given a small
amount of data from the target distribution, retraining only the last linear
layer will give excellent performance. We therefore argue that devising simpler
methods for learning predictors on existing features is a promising direction
for future research. Towards this end, we introduce Domain-Adjusted Regression
(DARE), a convex objective for learning a linear predictor that is provably
robust under a new model of distribution shift. Rather than learning one
function, DARE performs a domain-specific adjustment to unify the domains in a
canonical latent space and learns to predict in this space. Under a natural
model, we prove that the DARE solution is the minimax-optimal predictor for a
constrained set of test distributions. Further, we provide the first
finite-environment convergence guarantee to the minimax risk, improving over
existing analyses which only yield minimax predictors after an environment
threshold. Evaluated on finetuned features, we find that DARE compares
favorably to prior methods, consistently achieving equal or better performance.},
Year          = {2022},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2202.06856v2},
File          = {2202.06856v2.pdf},
EprintNoVer   = {2202.06856}
}

@article{1912.02292v1,
Author        = {Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
Title         = {Deep Double Descent: Where Bigger Models and More Data Hurt},
Eprint        = {1912.02292v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We show that a variety of modern deep learning tasks exhibit a
"double-descent" phenomenon where, as we increase model size, performance first
gets worse and then gets better. Moreover, we show that double descent occurs
not just as a function of model size, but also as a function of the number of
training epochs. We unify the above phenomena by defining a new complexity
measure we call the effective model complexity and conjecture a generalized
double descent with respect to this measure. Furthermore, our notion of model
complexity allows us to identify certain regimes where increasing (even
quadrupling) the number of train samples actually hurts test performance.},
Year          = {2019},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1912.02292v1},
File          = {1912.02292v1.pdf},
EprintNoVer   = {1912.02292}
}

@article{1812.11118v2,
Author        = {Mikhail Belkin and Daniel Hsu and Siyuan Ma and Soumik Mandal},
Title         = {Reconciling modern machine learning practice and the bias-variance
  trade-off},
Eprint        = {1812.11118v2},
DOI           = {10.1073/pnas.1903070116},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {Breakthroughs in machine learning are rapidly changing science and society,
yet our fundamental understanding of this technology has lagged far behind.
Indeed, one of the central tenets of the field, the bias-variance trade-off,
appears to be at odds with the observed behavior of methods used in the modern
machine learning practice. The bias-variance trade-off implies that a model
should balance under-fitting and over-fitting: rich enough to express
underlying structure in data, simple enough to avoid fitting spurious patterns.
However, in the modern practice, very rich models such as neural networks are
trained to exactly fit (i.e., interpolate) the data. Classically, such models
would be considered over-fit, and yet they often obtain high accuracy on test
data. This apparent contradiction has raised questions about the mathematical
foundations of machine learning and their relevance to practitioners.
  In this paper, we reconcile the classical understanding and the modern
practice within a unified performance curve. This "double descent" curve
subsumes the textbook U-shaped bias-variance trade-off curve by showing how
increasing model capacity beyond the point of interpolation results in improved
performance. We provide evidence for the existence and ubiquity of double
descent for a wide spectrum of models and datasets, and we posit a mechanism
for its emergence. This connection between the performance and the structure of
machine learning models delineates the limits of classical analyses, and has
implications for both the theory and practice of machine learning.},
Year          = {2018},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1812.11118v2},
File          = {1812.11118v2.pdf},
EprintNoVer   = {1812.11118}
}

@article{2305.14283v3,
Author        = {Xinbei Ma and Yeyun Gong and Pengcheng He and Hai Zhao and Nan Duan},
Title         = {Query Rewriting for Retrieval-Augmented Large Language Models},
Eprint        = {2305.14283v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) play powerful, black-box readers in the
retrieve-then-read pipeline, making remarkable progress in knowledge-intensive
tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of
the previous retrieve-then-read for the retrieval-augmented LLMs from the
perspective of the query rewriting. Unlike prior studies focusing on adapting
either the retriever or the reader, our approach pays attention to the
adaptation of the search query itself, for there is inevitably a gap between
the input text and the needed knowledge in retrieval. We first prompt an LLM to
generate the query, then use a web search engine to retrieve contexts.
Furthermore, to better align the query to the frozen modules, we propose a
trainable scheme for our pipeline. A small language model is adopted as a
trainable rewriter to cater to the black-box LLM reader. The rewriter is
trained using the feedback of the LLM reader by reinforcement learning.
Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice
QA. Experiments results show consistent performance improvement, indicating
that our framework is proven effective and scalable, and brings a new framework
for retrieval-augmented LLM.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.14283v3},
File          = {2305.14283v3.pdf},
EprintNoVer   = {2305.14283}
}

@article{2305.02437v3,
Author        = {Xin Cheng and Di Luo and Xiuying Chen and Lemao Liu and Dongyan Zhao and Rui Yan},
Title         = {Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory},
Eprint        = {2305.02437v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With direct access to human-written reference as memory, retrieval-augmented
generation has achieved much progress in a wide range of text generation tasks.
Since better memory would typically prompt better generation~(we define this as
primal problem). The traditional approach for memory retrieval involves
selecting memory that exhibits the highest similarity to the input. However,
this method is constrained by the quality of the fixed corpus from which memory
is retrieved. In this paper, by exploring the duality of the primal problem:
better generation also prompts better memory, we propose a novel framework,
selfmem, which addresses this limitation by iteratively employing a
retrieval-augmented generator to create an unbounded memory pool and using a
memory selector to choose one output as memory for the subsequent generation
round. This enables the model to leverage its own output, referred to as
self-memory, for improved generation. We evaluate the effectiveness of selfmem
on three distinct text generation tasks: neural machine translation,
abstractive text summarization, and dialogue generation, under two generation
paradigms: fine-tuned small model and few-shot LLM. Our approach achieves
state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1),
and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in
enhancing retrieval-augmented generation models. Furthermore, we conduct
thorough analyses of each component in the selfmem framework to identify
bottlenecks and provide insights for future research.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.02437v3},
File          = {2305.02437v3.pdf},
EprintNoVer   = {2305.02437}
}

@article{2312.06648v2,
Author        = {Tong Chen and Hongwei Wang and Sihao Chen and Wenhao Yu and Kaixin Ma and Xinran Zhao and Hongming Zhang and Dong Yu},
Title         = {Dense X Retrieval: What Retrieval Granularity Should We Use?},
Eprint        = {2312.06648v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Dense retrieval has become a prominent method to obtain relevant context or
world knowledge in open-domain NLP tasks. When we use a learned dense retriever
on a retrieval corpus at inference time, an often-overlooked design choice is
the retrieval unit in which the corpus is indexed, e.g. document, passage, or
sentence. We discover that the retrieval unit choice significantly impacts the
performance of both retrieval and downstream tasks. Distinct from the typical
approach of using passages or sentences, we introduce a novel retrieval unit,
proposition, for dense retrieval. Propositions are defined as atomic
expressions within text, each encapsulating a distinct factoid and presented in
a concise, self-contained natural language format. We conduct an empirical
comparison of different retrieval granularity. Our results reveal that
proposition-based retrieval significantly outperforms traditional passage or
sentence-based methods in dense retrieval. Moreover, retrieval by proposition
also enhances the performance of downstream QA tasks, since the retrieved texts
are more condensed with question-relevant information, reducing the need for
lengthy input tokens and minimizing the inclusion of extraneous, irrelevant
information.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.06648v2},
File          = {2312.06648v2.pdf},
EprintNoVer   = {2312.06648}
}

@article{2212.10496v1,
Author        = {Luyu Gao and Xueguang Ma and Jimmy Lin and Jamie Callan},
Title         = {Precise Zero-Shot Dense Retrieval without Relevance Labels},
Eprint        = {2212.10496v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {While dense retrieval has been shown effective and efficient across tasks and
languages, it remains difficult to create effective fully zero-shot dense
retrieval systems when no relevance label is available. In this paper, we
recognize the difficulty of zero-shot learning and encoding relevance. Instead,
we propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a
query, HyDE first zero-shot instructs an instruction-following language model
(e.g. InstructGPT) to generate a hypothetical document. The document captures
relevance patterns but is unreal and may contain false details. Then, an
unsupervised contrastively learned encoder~(e.g. Contriever) encodes the
document into an embedding vector. This vector identifies a neighborhood in the
corpus embedding space, where similar real documents are retrieved based on
vector similarity. This second step ground the generated document to the actual
corpus, with the encoder's dense bottleneck filtering out the incorrect
details. Our experiments show that HyDE significantly outperforms the
state-of-the-art unsupervised dense retriever Contriever and shows strong
performance comparable to fine-tuned retrievers, across various tasks (e.g. web
search, QA, fact verification) and languages~(e.g. sw, ko, ja).},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.10496v1},
File          = {2212.10496v1.pdf},
EprintNoVer   = {2212.10496}
}

@article{2307.03172v3,
Author        = {Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
Title         = {Lost in the Middle: How Language Models Use Long Contexts},
Eprint        = {2307.03172v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While recent language models have the ability to take long contexts as input,
relatively little is known about how well they use longer context. We analyze
the performance of language models on two tasks that require identifying
relevant information in their input contexts: multi-document question answering
and key-value retrieval. We find that performance can degrade significantly
when changing the position of relevant information, indicating that current
language models do not robustly make use of information in long input contexts.
In particular, we observe that performance is often highest when relevant
information occurs at the beginning or end of the input context, and
significantly degrades when models must access relevant information in the
middle of long contexts, even for explicitly long-context models. Our analysis
provides a better understanding of how language models use their input context
and provides new evaluation protocols for future long-context language models.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.03172v3},
File          = {2307.03172v3.pdf},
EprintNoVer   = {2307.03172}
}

@article{2303.08559v2,
Author        = {Yubo Ma and Yixin Cao and YongChing Hong and Aixin Sun},
Title         = {Large Language Model Is Not a Good Few-shot Information Extractor, but a
  Good Reranker for Hard Samples!},
Eprint        = {2303.08559v2},
DOI           = {10.18653/v1/2023.findings-emnlp.710},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have made remarkable strides in various tasks.
Whether LLMs are competitive few-shot solvers for information extraction (IE)
tasks, however, remains an open problem. In this work, we aim to provide a
thorough answer to this question. Through extensive experiments on nine
datasets across four IE tasks, we demonstrate that current advanced LLMs
consistently exhibit inferior performance, higher latency, and increased budget
requirements compared to fine-tuned SLMs under most settings. Therefore, we
conclude that LLMs are not effective few-shot information extractors in
general. Nonetheless, we illustrate that with appropriate prompting strategies,
LLMs can effectively complement SLMs and tackle challenging samples that SLMs
struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm
to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as
filters and LLMs serve as rerankers. By prompting LLMs to rerank a small
portion of difficult samples identified by SLMs, our preliminary system
consistently achieves promising improvements (2.4% F1-gain on average) on
various IE tasks, with an acceptable time and cost investment.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.08559v2},
File          = {2303.08559v2.pdf},
EprintNoVer   = {2303.08559}
}

@article{2310.13243v1,
Author        = {Shengyao Zhuang and Bing Liu and Bevan Koopman and Guido Zuccon},
Title         = {Open-source Large Language Models are Strong Zero-shot Query Likelihood
  Models for Document Ranking},
Eprint        = {2310.13243v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In the field of information retrieval, Query Likelihood Models (QLMs) rank
documents based on the probability of generating the query given the content of
a document. Recently, advanced large language models (LLMs) have emerged as
effective QLMs, showcasing promising ranking capabilities. This paper focuses
on investigating the genuine zero-shot ranking effectiveness of recent LLMs,
which are solely pre-trained on unstructured text data without supervised
instruction fine-tuning. Our findings reveal the robust zero-shot ranking
ability of such LLMs, highlighting that additional instruction fine-tuning may
hinder effectiveness unless a question generation task is present in the
fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking
system that integrates LLM-based QLMs with a hybrid zero-shot retriever,
demonstrating exceptional effectiveness in both zero-shot and few-shot
scenarios. We make our codebase publicly available at
https://github.com/ielab/llm-qlm.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.13243v1},
File          = {2310.13243v1.pdf},
EprintNoVer   = {2310.13243}
}

@article{2202.05144v1,
Author        = {Luiz Bonifacio and Hugo Abonizio and Marzieh Fadaee and Rodrigo Nogueira},
Title         = {InPars: Data Augmentation for Information Retrieval using Large Language
  Models},
Eprint        = {2202.05144v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The information retrieval community has recently witnessed a revolution due
to large pretrained transformer models. Another key ingredient for this
revolution was the MS MARCO dataset, whose scale and diversity has enabled
zero-shot transfer learning to various tasks. However, not all IR tasks and
domains can benefit from one single dataset equally. Extensive research in
various NLP tasks has shown that using domain-specific training data, as
opposed to a general-purpose one, improves the performance of neural models. In
this work, we harness the few-shot capabilities of large pretrained language
models as synthetic data generators for IR tasks. We show that models finetuned
solely on our unsupervised dataset outperform strong baselines such as BM25 as
well as recently proposed self-supervised dense retrieval methods. Furthermore,
retrievers finetuned on both supervised and our synthetic data achieve better
zero-shot transfer than models finetuned only on supervised data. Code, models,
and data are available at https://github.com/zetaalphavector/inpars .},
Year          = {2022},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2202.05144v1},
File          = {2202.05144v1.pdf},
EprintNoVer   = {2202.05144}
}

@article{2305.15294v2,
Author        = {Zhihong Shao and Yeyun Gong and Yelong Shen and Minlie Huang and Nan Duan and Weizhu Chen},
Title         = {Enhancing Retrieval-Augmented Large Language Models with Iterative
  Retrieval-Generation Synergy},
Eprint        = {2305.15294v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models are powerful text processors and reasoners, but are
still subject to limitations including outdated knowledge and hallucinations,
which necessitates connecting them to the world. Retrieval-augmented large
language models have raised extensive attention for grounding model generation
on external knowledge. However, retrievers struggle to capture relevance,
especially for queries with complex information needs. Recent work has proposed
to improve relevance modeling by having large language models actively involved
in retrieval, i.e., to improve retrieval with generation. In this paper, we
show that strong performance can be achieved by a method we call Iter-RetGen,
which synergizes retrieval and generation in an iterative manner. A model
output shows what might be needed to finish a task, and thus provides an
informative context for retrieving more relevant knowledge which in turn helps
generate a better output in the next iteration. Compared with recent work which
interleaves retrieval with generation when producing an output, Iter-RetGen
processes all retrieved knowledge as a whole and largely preserves the
flexibility in generation without structural constraints. We evaluate
Iter-RetGen on multi-hop question answering, fact verification, and commonsense
reasoning, and show that it can flexibly leverage parametric knowledge and
non-parametric knowledge, and is superior to or competitive with
state-of-the-art retrieval-augmented baselines while causing fewer overheads of
retrieval and generation. We can further improve performance via
generation-augmented retrieval adaptation.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.15294v2},
File          = {2305.15294v2.pdf},
EprintNoVer   = {2305.15294}
}

@article{2212.10509v2,
Author        = {Harsh Trivedi and Niranjan Balasubramanian and Tushar Khot and Ashish Sabharwal},
Title         = {Interleaving Retrieval with Chain-of-Thought Reasoning for
  Knowledge-Intensive Multi-Step Questions},
Eprint        = {2212.10509v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Prompting-based large language models (LLMs) are surprisingly powerful at
generating natural language reasoning steps or Chains-of-Thoughts (CoT) for
multi-step question answering (QA). They struggle, however, when the necessary
knowledge is either unavailable to the LLM or not up-to-date within its
parameters. While using the question to retrieve relevant text from an external
knowledge source helps LLMs, we observe that this one-step retrieve-and-read
approach is insufficient for multi-step QA. Here, \textit{what to retrieve}
depends on \textit{what has already been derived}, which in turn may depend on
\textit{what was previously retrieved}. To address this, we propose IRCoT, a
new approach for multi-step QA that interleaves retrieval with steps
(sentences) in a CoT, guiding the retrieval with CoT and in turn using
retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves
retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four
datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar
substantial gains in out-of-distribution (OOD) settings as well as with much
smaller models such as Flan-T5-large without additional training. IRCoT reduces
model hallucination, resulting in factually more accurate CoT reasoning. Code,
data, and prompts are available at \url{https://github.com/stonybrooknlp/ircot}},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.10509v2},
File          = {2212.10509v2.pdf},
EprintNoVer   = {2212.10509}
}

@article{2204.06092v2,
Author        = {Ivan Stelmakh and Yi Luan and Bhuwan Dhingra and Ming-Wei Chang},
Title         = {ASQA: Factoid Questions Meet Long-Form Answers},
Eprint        = {2204.06092v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {An abundance of datasets and availability of reliable evaluation metrics have
resulted in strong progress in factoid question answering (QA). This progress,
however, does not easily transfer to the task of long-form QA, where the goal
is to answer questions that require in-depth explanations. The hurdles include
(i) a lack of high-quality data, and (ii) the absence of a well-defined notion
of the answer's quality. In this work, we address these problems by (i)
releasing a novel dataset and a task that we call ASQA (Answer Summaries for
Questions which are Ambiguous); and (ii) proposing a reliable metric for
measuring performance on ASQA. Our task focuses on factoid questions that are
ambiguous, that is, have different correct answers depending on interpretation.
Answers to ambiguous questions should synthesize factual information from
multiple sources into a long-form summary that resolves the ambiguity. In
contrast to existing long-form QA tasks (such as ELI5), ASQA admits a clear
notion of correctness: a user faced with a good summary should be able to
answer different interpretations of the original ambiguous question. We use
this notion of correctness to define an automated metric of performance for
ASQA. Our analysis demonstrates an agreement between this metric and human
judgments, and reveals a considerable gap between human performance and strong
baselines.},
Year          = {2022},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2204.06092v2},
File          = {2204.06092v2.pdf},
EprintNoVer   = {2204.06092}
}

@article{2405.14333v1,
Author        = {Huajian Xin and Daya Guo and Zhihong Shao and Zhizhou Ren and Qihao Zhu and Bo Liu and Chong Ruan and Wenda Li and Xiaodan Liang},
Title         = {DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale
  Synthetic Data},
Eprint        = {2405.14333v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Proof assistants like Lean have revolutionized mathematical proof
verification, ensuring high accuracy and reliability. Although large language
models (LLMs) show promise in mathematical reasoning, their advancement in
formal theorem proving is hindered by a lack of training data. To address this
issue, we introduce an approach to generate extensive Lean 4 proof data derived
from high-school and undergraduate-level mathematical competition problems.
This approach involves translating natural language problems into formal
statements, filtering out low-quality statements, and generating proofs to
create synthetic data. After fine-tuning the DeepSeekMath 7B model on this
synthetic dataset, which comprises 8 million formal statements with proofs, our
model achieved whole-proof generation accuracies of 46.3% with 64 samples and
52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at
23.0% with 64 samples and a tree search reinforcement learning method at 41.0%.
Additionally, our model successfully proved 5 out of 148 problems in the Lean 4
Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4
failed to prove any. These results demonstrate the potential of leveraging
large-scale synthetic data to enhance theorem-proving capabilities in LLMs.
Both the synthetic dataset and the model will be made available to facilitate
further research in this promising field.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.14333v1},
File          = {2405.14333v1.pdf},
EprintNoVer   = {2405.14333}
}

@article{2209.10063v3,
Author        = {Wenhao Yu and Dan Iter and Shuohang Wang and Yichong Xu and Mingxuan Ju and Soumya Sanyal and Chenguang Zhu and Michael Zeng and Meng Jiang},
Title         = {Generate rather than Retrieve: Large Language Models are Strong Context
  Generators},
Eprint        = {2209.10063v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-intensive tasks, such as open-domain question answering (QA),
require access to a large amount of world or domain knowledge. A common
approach for knowledge-intensive tasks is to employ a retrieve-then-read
pipeline that first retrieves a handful of relevant contextual documents from
an external corpus such as Wikipedia and then predicts an answer conditioned on
the retrieved documents. In this paper, we present a novel perspective for
solving knowledge-intensive tasks by replacing document retrievers with large
language model generators. We call our method generate-then-read (GenRead),
which first prompts a large language model to generate contextutal documents
based on a given question, and then reads the generated documents to produce
the final answer. Furthermore, we propose a novel clustering-based prompting
method that selects distinct prompts, resulting in the generated documents that
cover different perspectives, leading to better recall over acceptable answers.
We conduct extensive experiments on three different knowledge-intensive tasks,
including open-domain QA, fact checking, and dialogue system. Notably, GenRead
achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly
outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0
and +3.9, without retrieving any documents from any external knowledge source.
Lastly, we demonstrate the model performance can be further improved by
combining retrieval and generation. Our code and generated documents can be
found at https://github.com/wyu97/GenRead.},
Year          = {2022},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2209.10063v3},
File          = {2209.10063v3.pdf},
EprintNoVer   = {2209.10063}
}

@article{2112.09118v4,
Author        = {Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
Title         = {Unsupervised Dense Information Retrieval with Contrastive Learning},
Eprint        = {2112.09118v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Recently, information retrieval has seen the emergence of dense retrievers,
using neural networks, as an alternative to classical sparse methods based on
term-frequency. These models have obtained state-of-the-art results on datasets
and tasks where large training sets are available. However, they do not
transfer well to new applications with no training data, and are outperformed
by unsupervised term-frequency methods such as BM25. In this work, we explore
the limits of contrastive learning as a way to train unsupervised dense
retrievers and show that it leads to strong performance in various retrieval
settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11
out of 15 datasets for the Recall@100. When used as pre-training before
fine-tuning, either on a few thousands in-domain examples or on the large
MS~MARCO dataset, our contrastive model leads to improvements on the BEIR
benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where
training data is even scarcer than for English, and show that our approach
leads to strong unsupervised performance. Our model also exhibits strong
cross-lingual transfer when fine-tuned on supervised English data only and
evaluated on low resources language such as Swahili. We show that our
unsupervised models can perform cross-lingual retrieval between different
scripts, such as retrieving English documents from Arabic queries, which would
not be possible with term matching methods.},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.09118v4},
File          = {2112.09118v4.pdf},
EprintNoVer   = {2112.09118}
}

@article{2403.09472v1,
Author        = {Zhiqing Sun and Longhui Yu and Yikang Shen and Weiyang Liu and Yiming Yang and Sean Welleck and Chuang Gan},
Title         = {Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision},
Eprint        = {2403.09472v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Current AI alignment methodologies rely on human-provided demonstrations or
judgments, and the learned capabilities of AI systems would be upper-bounded by
human capabilities as a result. This raises a challenging research question:
How can we keep improving the systems when their capabilities have surpassed
the levels of humans? This paper answers this question in the context of
tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from
human annotations on easier tasks (e.g., level 1-3 MATH problems), which we
term as \textit{easy-to-hard generalization}. Our key insight is that an
evaluator (reward model) trained on supervisions for easier tasks can be
effectively used for scoring candidate solutions of harder tasks and hence
facilitating easy-to-hard generalization over different levels of tasks. Based
on this insight, we propose a novel approach to scalable alignment, which
firstly trains the process-supervised reward models on easy problems (e.g.,
level 1-3), and then uses them to evaluate the performance of policy models on
hard problems. We show that such \textit{easy-to-hard generalization from
evaluators} can enable \textit{easy-to-hard generalizations in generators}
either through re-ranking or reinforcement learning (RL). Notably, our
process-supervised 7b RL model achieves an accuracy of 34.0\% on MATH500,
despite only using human supervision on easy problems. Our approach suggests a
promising path toward AI systems that advance beyond the frontier of human
supervision.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.09472v1},
File          = {2403.09472v1.pdf},
EprintNoVer   = {2403.09472}
}

@article{2409.12917v2,
Author        = {Aviral Kumar and Vincent Zhuang and Rishabh Agarwal and Yi Su and John D Co-Reyes and Avi Singh and Kate Baumli and Shariq Iqbal and Colton Bishop and Rebecca Roelofs and Lei M Zhang and Kay McKinney and Disha Shrivastava and Cosmin Paduraru and George Tucker and Doina Precup and Feryal Behbahani and Aleksandra Faust},
Title         = {Training Language Models to Self-Correct via Reinforcement Learning},
Eprint        = {2409.12917v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Self-correction is a highly desirable capability of large language models
(LLMs), yet it has consistently been found to be largely ineffective in modern
LLMs. Current methods for training self-correction typically depend on either
multiple models, a more advanced model, or additional forms of supervision. To
address these shortcomings, we develop a multi-turn online reinforcement
learning (RL) approach, SCoRe, that significantly improves an LLM's
self-correction ability using entirely self-generated data. To build SCoRe, we
first show that variants of supervised fine-tuning (SFT) on offline
model-generated correction traces are often insufficient for instilling
self-correction behavior. In particular, we observe that training via SFT falls
prey to either a distribution mismatch between mistakes made by the
data-collection policy and the model's own responses, or to behavior collapse,
where learning implicitly prefers only a certain mode of correction behavior
that is often not effective at self-correction on test problems. SCoRe
addresses these challenges by training under the model's own distribution of
self-generated correction traces and using appropriate regularization to steer
the learning process into learning a self-correction behavior that is effective
at test time as opposed to fitting high-reward responses for a given prompt.
This regularization process includes an initial phase of multi-turn RL on a
base model to generate a policy initialization that is less susceptible to
collapse, followed by using a reward bonus to amplify self-correction. With
Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves
state-of-the-art self-correction performance, improving the base models'
self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.12917v2},
File          = {2409.12917v2.pdf},
EprintNoVer   = {2409.12917}
}

@article{2309.15217v1,
Author        = {Shahul Es and Jithin James and Luis Espinosa-Anke and Steven Schockaert},
Title         = {RAGAS: Automated Evaluation of Retrieval Augmented Generation},
Eprint        = {2309.15217v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework
for reference-free evaluation of Retrieval Augmented Generation (RAG)
pipelines. RAG systems are composed of a retrieval and an LLM based generation
module, and provide LLMs with knowledge from a reference textual database,
which enables them to act as a natural language layer between a user and
textual databases, reducing the risk of hallucinations. Evaluating RAG
architectures is, however, challenging because there are several dimensions to
consider: the ability of the retrieval system to identify relevant and focused
context passages, the ability of the LLM to exploit such passages in a faithful
way, or the quality of the generation itself. With RAGAs, we put forward a
suite of metrics which can be used to evaluate these different dimensions
\textit{without having to rely on ground truth human annotations}. We posit
that such a framework can crucially contribute to faster evaluation cycles of
RAG architectures, which is especially important given the fast adoption of
LLMs.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.15217v1},
File          = {2309.15217v1.pdf},
EprintNoVer   = {2309.15217}
}
