---
---

@article{2209.03430v2,
Author        = {Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency},
Title         = {Foundations and Trends in Multimodal Machine Learning: Principles,
  Challenges, and Open Questions},
Eprint        = {2209.03430v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Multimodal machine learning is a vibrant multi-disciplinary research field
that aims to design computer agents with intelligent capabilities such as
understanding, reasoning, and learning through integrating multiple
communicative modalities, including linguistic, acoustic, visual, tactile, and
physiological messages. With the recent interest in video understanding,
embodied autonomous agents, text-to-image generation, and multisensor fusion in
application domains such as healthcare and robotics, multimodal machine
learning has brought unique computational and theoretical challenges to the
machine learning community given the heterogeneity of data sources and the
interconnections often found between modalities. However, the breadth of
progress in multimodal research has made it difficult to identify the common
themes and open questions in the field. By synthesizing a broad range of
application domains and theoretical frameworks from both historical and recent
perspectives, this paper is designed to provide an overview of the
computational and theoretical foundations of multimodal machine learning. We
start by defining three key principles of modality heterogeneity, connections,
and interactions that have driven subsequent innovations, and propose a
taxonomy of six core technical challenges: representation, alignment,
reasoning, generation, transference, and quantification covering historical and
recent trends. Recent technical achievements will be presented through the lens
of this taxonomy, allowing researchers to understand the similarities and
differences across new approaches. We end by motivating several open problems
for future research as identified by our taxonomy.},
Year          = {2022},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2209.03430v2},
File          = {2209.03430v2.pdf},
EprintNoVer   = {2209.03430}
}

@article{1802.05365v2,
Author        = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
Title         = {Deep contextualized word representations},
Eprint        = {1802.05365v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce a new type of deep contextualized word representation that
models both (1) complex characteristics of word use (e.g., syntax and
semantics), and (2) how these uses vary across linguistic contexts (i.e., to
model polysemy). Our word vectors are learned functions of the internal states
of a deep bidirectional language model (biLM), which is pre-trained on a large
text corpus. We show that these representations can be easily added to existing
models and significantly improve the state of the art across six challenging
NLP problems, including question answering, textual entailment and sentiment
analysis. We also present an analysis showing that exposing the deep internals
of the pre-trained network is crucial, allowing downstream models to mix
different types of semi-supervision signals.},
Year          = {2018},
Month         = {Feb},
Url           = {http://arxiv.org/abs/1802.05365v2},
File          = {1802.05365v2.pdf},
EprintNoVer   = {1802.05365}
}

@article{2201.11903v6,
Author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
Title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
Eprint        = {2201.11903v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.},
Year          = {2022},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2201.11903v6},
File          = {2201.11903v6.pdf},
EprintNoVer   = {2201.11903}
}

@article{2005.14165v4,
Author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
Title         = {Language Models are Few-Shot Learners},
Eprint        = {2005.14165v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3's few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general.},
Year          = {2020},
Month         = {May},
Url           = {http://arxiv.org/abs/2005.14165v4},
File          = {2005.14165v4.pdf},
EprintNoVer   = {2005.14165}
}