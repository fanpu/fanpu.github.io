---
title: "NTK"
layout: post
tags: [machine-learning, math]
cover: furano.webp
cover_preview: furano.webp
caption: Lavender Fields in Biei, Kamikawa Subprefecture, Hokkaido, Japan
class: post-template
author: fanpu
toc:
  sidebar: left
giscus_comments: true
description: >
  TODO
published: false
---

When do we have algorithmic results?
- optimizing a single layer (random features)
- local results (Polyak condition)


Functions of some form induces a RKHS...

if very wide:
sums become integrals
those kind of function can be approximated by a two-layer
network


something something only requires a finite number of basis elements?? such that it is close?

construction: sample uniformly from omega

Show: function classes learnable via SGD


# Gradient Descent Finds Global Minima of Deep Neural Networks

Tries to demystify two phenomenons:
1. First-order methods like GD can get zero training loss, even if labels are arbitrary
2. Deep NNs are harder to train (requires residual connections)

Draws on previous work:
Learning overparameterized neural networks via stochastic gradient descent on
structured data
- says that in over-parameterized NNs, every weight matrix is close
to its initialization

Gradient descent provably optimizes over-parameterized neural networks
- convergence is determined by the least eigenvalue of the Gram matrix
induced by the NN architecture. 
- So want to lower bound the least eigenvalue
- To do that, sufficient to bound distance of each weight matrix from initialization

Proof technique:
- Extending this to deep networks
- perturbation analysis: shows why for resnet there isn't the exponential
problem with how the perturbation grows, as opposed to normal deep networks


# On Exact Computation with an Infinitely Wide Neural Net

Easier to study objects in the infinite limit in math

Weakly-trained nets: layers receive random initialization, only top layer trained by GD

