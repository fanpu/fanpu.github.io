<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ML Reading List | Fan Pu Zeng </title> <meta name="author" content="Fan Pu Zeng"> <meta name="description" content="Curated list of papers that I have bookmarked to read, well, someday..."> <meta name="keywords" content="fanpu, fan pu, fanpu zeng, fan pu zeng, zengfanpu, fanpuzeng, fzeng, CMU, cmu, carnegie mellon, cmu courses, school of computer science, scs, machine learning, computer science, ml, theory, courses, course reviews, CS, Jane Street"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon_new.ico?426605099301e95aedae716cc398b951"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://fanpu.io/reading-list/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Fan Pu</span> Zeng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/courses/">CMU Course Reviews </a> </li> <li class="nav-item "> <a class="nav-link" href="/cmu-online/">CMU Online </a> </li> <li class="nav-item "> <a class="nav-link" href="/summaries/">ML Paper Summaries </a> </li> <li class="nav-item active"> <a class="nav-link" href="/reading-list/">ML Reading List <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">ML Reading List</h1> <p class="post-description">Curated list of papers that I have bookmarked to read, well, someday...</p> </header> <article> <p>This list is continously updated as I bookmark more papers to read. It’s also a good reflection of my current research interests.</p> <p>There’s a good chance that if I like the paper, I will write a summary about it.</p> <ul> <li><a href="https://arxiv.org/abs/2211.17192" rel="external nofollow noopener" target="_blank">Fast Inference from Transformers via Speculative Decoding</a></li> <li><a href="https://arxiv.org/abs/2312.08935" rel="external nofollow noopener" target="_blank">Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations</a></li> <li><a href="https://arxiv.org/abs/2305.16264" rel="external nofollow noopener" target="_blank">Scaling Data-Constrained Language Models</a></li> <li><a href="https://arxiv.org/abs/2311.03099" rel="external nofollow noopener" target="_blank">Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch</a></li> <li><a href="https://arxiv.org/abs/2306.01708" rel="external nofollow noopener" target="_blank">TIES-Merging: Resolving Interference When Merging Models</a></li> <li><a href="https://arxiv.org/abs/2212.04089" rel="external nofollow noopener" target="_blank">Editing Models with Task Arithmetic</a></li> <li><a href="https://arxiv.org/abs/2401.10774" rel="external nofollow noopener" target="_blank">Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a></li> <li><a href="https://arxiv.org/abs/1811.01727" rel="external nofollow noopener" target="_blank">AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification</a></li> <li><a href="https://arxiv.org/abs/2211.17192" rel="external nofollow noopener" target="_blank">Fast Inference from Transformers via Speculative Decoding</a></li> <li><a href="https://arxiv.org/abs/2412.19048" rel="external nofollow noopener" target="_blank">Jasper and Stella: distillation of SOTA embedding models</a></li> <li><a href="https://arxiv.org/abs/2402.03300" rel="external nofollow noopener" target="_blank">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a></li> <li><a href="https://arxiv.org/abs/2309.00071" rel="external nofollow noopener" target="_blank">YaRN: Efficient Context Window Extension of Large Language Models</a></li> <li><a href="https://arxiv.org/abs/2401.10241" rel="external nofollow noopener" target="_blank">Zero Bubble Pipeline Parallelism</a></li> <li><a href="https://arxiv.org/abs/2410.21676" rel="external nofollow noopener" target="_blank">How Does Critical Batch Size Scale in Pre-training?</a></li> <li><a href="https://arxiv.org/abs/2404.19737" rel="external nofollow noopener" target="_blank">Better &amp; Faster Large Language Models via Multi-token Prediction</a></li> <li><a href="https://arxiv.org/abs/1506.03134" rel="external nofollow noopener" target="_blank">Pointer Networks</a></li> <li><a href="https://arxiv.org/abs/2404.10719" rel="external nofollow noopener" target="_blank">Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</a></li> <li><a href="https://arxiv.org/abs/2407.18219" rel="external nofollow noopener" target="_blank">Recursive Introspection: Teaching Language Model Agents How to Self-Improve</a></li> <li><a href="https://arxiv.org/abs/2401.12187" rel="external nofollow noopener" target="_blank">WARM: On the Benefits of Weight Averaged Reward Models</a></li> <li><a href="https://arxiv.org/abs/2402.18819" rel="external nofollow noopener" target="_blank">Dual Operating Modes of In-Context Learning</a></li> <li><a href="https://arxiv.org/abs/2411.02272" rel="external nofollow noopener" target="_blank">Combining Induction and Transduction for Abstract Reasoning</a></li> <li><a href="https://arxiv.org/abs/2412.08905" rel="external nofollow noopener" target="_blank">Phi-4 Technical Report</a></li> <li><a href="https://arxiv.org/abs/2305.19466" rel="external nofollow noopener" target="_blank">The Impact of Positional Encoding on Length Generalization in Transformers</a></li> <li><a href="https://arxiv.org/abs/2410.02089" rel="external nofollow noopener" target="_blank">RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning</a></li> <li><a href="https://arxiv.org/abs/2410.08020" rel="external nofollow noopener" target="_blank">Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs</a></li> <li><a href="https://arxiv.org/abs/2410.21216v1" rel="external nofollow noopener" target="_blank">HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation</a></li> <li><a href="https://arxiv.org/abs/2404.03683" rel="external nofollow noopener" target="_blank">Stream of Search (SoS): Learning to Search in Language</a></li> <li><a href="https://arxiv.org/abs/2205.11787" rel="external nofollow noopener" target="_blank">Quadratic models for understanding catapult dynamics of neural networks</a></li> <li><a href="https://arxiv.org/abs/2306.04815" rel="external nofollow noopener" target="_blank">Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning</a></li> <li><a href="https://arxiv.org/abs/2406.00153" rel="external nofollow noopener" target="_blank">$μ$LO: Compute-Efficient Meta-Generalization of Learned Optimizers</a></li> <li><a href="https://arxiv.org/abs/2211.09760" rel="external nofollow noopener" target="_blank">VeLO: Training Versatile Learned Optimizers by Scaling Up</a></li> <li><a href="https://arxiv.org/abs/2406.11233" rel="external nofollow noopener" target="_blank">Probing the Decision Boundaries of In-context Learning in Large Language Models</a></li> <li><a href="https://arxiv.org/abs/2411.15370" rel="external nofollow noopener" target="_blank">Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers</a></li> <li><a href="https://arxiv.org/abs/2402.08147" rel="external nofollow noopener" target="_blank">VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search</a></li> <li><a href="https://arxiv.org/abs/2405.04517" rel="external nofollow noopener" target="_blank">xLSTM: Extended Long Short-Term Memory</a></li> <li><a href="https://arxiv.org/abs/2408.12857" rel="external nofollow noopener" target="_blank">Memory-Efficient LLM Training with Online Subspace Descent</a></li> <li><a href="https://arxiv.org/abs/2307.03997" rel="external nofollow noopener" target="_blank">Efficient Model-Free Exploration in Low-Rank MDPs</a></li> <li><a href="https://arxiv.org/abs/2405.15599" rel="external nofollow noopener" target="_blank">On the Computational Landscape of Replicable Learning</a></li> <li><a href="https://arxiv.org/abs/2311.02971" rel="external nofollow noopener" target="_blank">TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications</a></li> <li><a href="https://arxiv.org/abs/2207.01848" rel="external nofollow noopener" target="_blank">TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second</a></li> <li><a href="https://arxiv.org/abs/2412.05265" rel="external nofollow noopener" target="_blank">Reinforcement Learning: An Overview</a></li> <li><a href="https://arxiv.org/abs/2406.01583" rel="external nofollow noopener" target="_blank">Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP</a></li> <li><a href="https://arxiv.org/abs/2203.03466" rel="external nofollow noopener" target="_blank">Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer</a></li> <li><a href="https://arxiv.org/abs/2410.02117" rel="external nofollow noopener" target="_blank">Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices</a></li> <li><a href="https://arxiv.org/abs/2406.06248" rel="external nofollow noopener" target="_blank">Compute Better Spent: Replacing Dense Layers with Structured Matrices</a></li> <li><a href="https://arxiv.org/abs/2309.03060" rel="external nofollow noopener" target="_blank">CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra</a></li> <li><a href="https://arxiv.org/abs/2411.11824" rel="external nofollow noopener" target="_blank">Theoretical Foundations of Conformal Prediction</a></li> <li><a href="https://arxiv.org/abs/2406.11200" rel="external nofollow noopener" target="_blank">AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning</a></li> <li><a href="https://arxiv.org/abs/2309.00570" rel="external nofollow noopener" target="_blank">Mechanism of feature learning in convolutional neural networks</a></li> <li><a href="https://arxiv.org/abs/2402.13728" rel="external nofollow noopener" target="_blank">Average gradient outer product as a mechanism for deep neural collapse</a></li> <li><a href="https://arxiv.org/abs/1810.05369" rel="external nofollow noopener" target="_blank">Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel</a></li> <li><a href="https://arxiv.org/abs/2105.14368" rel="external nofollow noopener" target="_blank">Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation</a></li> <li><a href="https://arxiv.org/abs/1708.00523" rel="external nofollow noopener" target="_blank">The duality structure gradient descent algorithm: analysis and applications to neural networks</a></li> <li><a href="https://arxiv.org/abs/2310.17813" rel="external nofollow noopener" target="_blank">A Spectral Condition for Feature Learning</a></li> <li><a href="https://arxiv.org/abs/2410.21265" rel="external nofollow noopener" target="_blank">Modular Duality in Deep Learning</a></li> <li><a href="https://arxiv.org/abs/2409.20325" rel="external nofollow noopener" target="_blank">Old Optimizer, New Norm: An Anthology</a></li> <li><a href="https://arxiv.org/abs/2410.21265" rel="external nofollow noopener" target="_blank">Modular Duality in Deep Learning</a></li> <li><a href="https://arxiv.org/abs/1406.2572" rel="external nofollow noopener" target="_blank">Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</a></li> <li><a href="https://arxiv.org/abs/2102.13019" rel="external nofollow noopener" target="_blank">Investigating the Limitations of Transformers with Simple Arithmetic Tasks</a></li> <li><a href="https://arxiv.org/abs/2308.01825" rel="external nofollow noopener" target="_blank">Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</a></li> <li><a href="https://arxiv.org/abs/2306.01694" rel="external nofollow noopener" target="_blank">Evaluating Language Models for Mathematics through Interactions</a></li> <li><a href="https://arxiv.org/abs/2305.14201" rel="external nofollow noopener" target="_blank">Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</a></li> <li><a href="https://arxiv.org/abs/2308.04014" rel="external nofollow noopener" target="_blank">Continual Pre-Training of Large Language Models: How to (re)warm your model?</a></li> <li><a href="https://arxiv.org/abs/2312.05523" rel="external nofollow noopener" target="_blank">Functional Data Analysis: An Introduction and Recent Developments</a></li> <li><a href="https://arxiv.org/abs/1706.04599" rel="external nofollow noopener" target="_blank">On Calibration of Modern Neural Networks</a></li> <li><a href="https://arxiv.org/abs/2411.04105" rel="external nofollow noopener" target="_blank">How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis</a></li> <li><a href="https://arxiv.org/abs/2410.21228v1" rel="external nofollow noopener" target="_blank">LoRA vs Full Fine-tuning: An Illusion of Equivalence</a></li> <li><a href="https://arxiv.org/abs/1812.06162" rel="external nofollow noopener" target="_blank">An Empirical Model of Large-Batch Training</a></li> <li><a href="https://arxiv.org/abs/1802.07044" rel="external nofollow noopener" target="_blank">The Description Length of Deep Learning Models</a></li> <li><a href="https://arxiv.org/abs/2411.02853" rel="external nofollow noopener" target="_blank">ADOPT: Modified Adam Can Converge with Any $β_2$ with the Optimal Rate</a></li> <li><a href="https://arxiv.org/abs/2402.04384" rel="external nofollow noopener" target="_blank">Denoising Diffusion Probabilistic Models in Six Simple Steps</a></li> <li><a href="https://arxiv.org/abs/2410.24206" rel="external nofollow noopener" target="_blank">Understanding Optimization in Deep Learning with Central Flows</a></li> <li><a href="https://arxiv.org/abs/2410.21265" rel="external nofollow noopener" target="_blank">Modular Duality in Deep Learning</a></li> <li><a href="https://arxiv.org/abs/2405.14333" rel="external nofollow noopener" target="_blank">DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data</a></li> <li><a href="https://arxiv.org/abs/2406.13762" rel="external nofollow noopener" target="_blank">Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis</a></li> <li><a href="https://arxiv.org/abs/2410.05192" rel="external nofollow noopener" target="_blank">Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective</a></li> <li><a href="https://arxiv.org/abs/2410.12001v1" rel="external nofollow noopener" target="_blank">Impacts of Continued Legal Pre-Training and IFT on LLMs’ Latent Representations of Human-Defined Legal Concepts</a></li> <li><a href="https://arxiv.org/abs/2405.16359" rel="external nofollow noopener" target="_blank">A First Course in Monte Carlo Methods</a></li> <li><a href="https://arxiv.org/abs/2410.19034" rel="external nofollow noopener" target="_blank">Mixture of Parrots: Experts improve memorization more than reasoning</a></li> <li><a href="https://arxiv.org/abs/2408.13296v1" rel="external nofollow noopener" target="_blank">The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities</a></li> <li><a href="https://arxiv.org/abs/2410.09713" rel="external nofollow noopener" target="_blank">Agentic Information Retrieval</a></li> <li><a href="https://arxiv.org/abs/2410.01131" rel="external nofollow noopener" target="_blank">nGPT: Normalized Transformer with Representation Learning on the Hypersphere</a></li> <li><a href="https://arxiv.org/abs/2310.04415" rel="external nofollow noopener" target="_blank">Why Do We Need Weight Decay in Modern Deep Learning?</a></li> <li><a href="https://arxiv.org/abs/2002.05202" rel="external nofollow noopener" target="_blank">GLU Variants Improve Transformer</a></li> <li><a href="https://arxiv.org/abs/1702.03118" rel="external nofollow noopener" target="_blank">Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning</a></li> <li><a href="https://arxiv.org/abs/2410.08146" rel="external nofollow noopener" target="_blank">Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning</a></li> <li><a href="https://arxiv.org/abs/2410.06940" rel="external nofollow noopener" target="_blank">Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think</a></li> <li><a href="https://arxiv.org/abs/2408.15240" rel="external nofollow noopener" target="_blank">Generative Verifiers: Reward Modeling as Next-Token Prediction</a></li> <li><a href="https://arxiv.org/abs/2402.14740" rel="external nofollow noopener" target="_blank">Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs</a></li> <li><a href="https://arxiv.org/abs/2410.06205" rel="external nofollow noopener" target="_blank">Round and Round We Go! What makes Rotary Positional Encodings useful?</a></li> <li><a href="https://arxiv.org/abs/2410.02724" rel="external nofollow noopener" target="_blank">Large Language Models as Markov Chains</a></li> <li><a href="https://arxiv.org/abs/2407.01219" rel="external nofollow noopener" target="_blank">Searching for Best Practices in Retrieval-Augmented Generation</a></li> <li><a href="https://arxiv.org/abs/2402.01502" rel="external nofollow noopener" target="_blank">Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers</a></li> <li><a href="https://arxiv.org/abs/2308.08998" rel="external nofollow noopener" target="_blank">Reinforced Self-Training (ReST) for Language Modeling</a></li> <li><a href="https://arxiv.org/abs/2408.08294" rel="external nofollow noopener" target="_blank">eGAD! double descent is explained by Generalized Aliasing Decomposition</a></li> <li><a href="https://arxiv.org/abs/2310.18988" rel="external nofollow noopener" target="_blank">A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning</a></li> <li><a href="https://arxiv.org/abs/2409.18842" rel="external nofollow noopener" target="_blank">Classical Statistical (In-Sample) Intuitions Don’t Generalize Well: A Note on Bias-Variance Tradeoffs, Overfitting and Moving from Fixed to Random Designs</a></li> <li><a href="https://arxiv.org/abs/2410.02525" rel="external nofollow noopener" target="_blank">Contextual Document Embeddings</a></li> <li><a href="https://arxiv.org/abs/2410.01201" rel="external nofollow noopener" target="_blank">Were RNNs All We Needed?</a></li> <li><a href="https://arxiv.org/abs/2409.14254" rel="external nofollow noopener" target="_blank">Instruction Following without Instruction Tuning</a></li> <li><a href="https://arxiv.org/abs/2403.09472" rel="external nofollow noopener" target="_blank">Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision</a></li> <li><a href="https://arxiv.org/abs/2406.16838" rel="external nofollow noopener" target="_blank">From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models</a></li> <li><a href="https://arxiv.org/abs/2402.12875" rel="external nofollow noopener" target="_blank">Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</a></li> <li><a href="https://arxiv.org/abs/1711.05101" rel="external nofollow noopener" target="_blank">Decoupled Weight Decay Regularization</a></li> <li><a href="https://arxiv.org/abs/2408.03314" rel="external nofollow noopener" target="_blank">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a></li> <li><a href="https://arxiv.org/abs/2409.03384" rel="external nofollow noopener" target="_blank">Hardware Acceleration of LLMs: A comprehensive survey and comparison</a></li> <li><a href="https://arxiv.org/abs/2409.02426" rel="external nofollow noopener" target="_blank">Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering</a></li> <li><a href="https://arxiv.org/abs/2311.13110" rel="external nofollow noopener" target="_blank">White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?</a></li> <li><a href="https://arxiv.org/abs/2403.10853" rel="external nofollow noopener" target="_blank">Just Say the Name: Online Continual Learning with Category Names Only via Data Generation</a></li> <li><a href="https://arxiv.org/abs/2312.02120" rel="external nofollow noopener" target="_blank">Magicoder: Empowering Code Generation with OSS-Instruct</a></li> <li><a href="https://arxiv.org/abs/2308.12950" rel="external nofollow noopener" target="_blank">Code Llama: Open Foundation Models for Code</a></li> <li><a href="https://arxiv.org/abs/2308.07074" rel="external nofollow noopener" target="_blank">#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models</a></li> <li><a href="https://arxiv.org/abs/2408.00949" rel="external nofollow noopener" target="_blank">Equivariant neural networks and piecewise linear representation theory</a></li> <li><a href="https://arxiv.org/abs/2406.03476" rel="external nofollow noopener" target="_blank">Does your data spark joy? Performance gains from domain upsampling at the end of training</a></li> <li><a href="https://arxiv.org/abs/2408.03314" rel="external nofollow noopener" target="_blank">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a></li> <li><a href="https://arxiv.org/abs/1812.11118" rel="external nofollow noopener" target="_blank">Reconciling modern machine learning practice and the bias-variance trade-off</a></li> <li><a href="https://arxiv.org/abs/1812.01754" rel="external nofollow noopener" target="_blank">Moment Matching for Multi-Source Domain Adaptation</a></li> <li><a href="https://arxiv.org/abs/1907.02893" rel="external nofollow noopener" target="_blank">Invariant Risk Minimization</a></li> <li><a href="https://arxiv.org/abs/1808.04926" rel="external nofollow noopener" target="_blank">How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks</a></li> <li><a href="https://arxiv.org/abs/1902.10811" rel="external nofollow noopener" target="_blank">Do ImageNet Classifiers Generalize to ImageNet?</a></li> <li><a href="https://arxiv.org/abs/2004.14444" rel="external nofollow noopener" target="_blank">The Effect of Natural Distribution Shift on Question Answering Models</a></li> <li><a href="https://arxiv.org/abs/1501.01332" rel="external nofollow noopener" target="_blank">Causal inference using invariant prediction: identification and confidence intervals</a></li> <li><a href="https://arxiv.org/abs/1806.07572" rel="external nofollow noopener" target="_blank">Neural Tangent Kernel: Convergence and Generalization in Neural Networks</a></li> <li><a href="https://arxiv.org/abs/1804.08838" rel="external nofollow noopener" target="_blank">Measuring the Intrinsic Dimension of Objective Landscapes</a></li> <li><a href="https://arxiv.org/abs/1707.02968" rel="external nofollow noopener" target="_blank">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</a></li> <li><a href="https://arxiv.org/abs/2011.12945" rel="external nofollow noopener" target="_blank">No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems</a></li> <li><a href="https://arxiv.org/abs/2202.06856" rel="external nofollow noopener" target="_blank">Domain-Adjusted Regression or: ERM May Already Learn Features Sufficient for Out-of-Distribution Generalization</a></li> <li><a href="https://arxiv.org/abs/2307.12375v4" rel="external nofollow noopener" target="_blank">In-Context Learning Learns Label Relationships but Is Not Conventional Learning</a></li> <li><a href="https://arxiv.org/abs/2404.01413" rel="external nofollow noopener" target="_blank">Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</a></li> <li><a href="https://arxiv.org/abs/2207.05221" rel="external nofollow noopener" target="_blank">Language Models (Mostly) Know What They Know</a></li> <li><a href="https://arxiv.org/abs/1807.02811" rel="external nofollow noopener" target="_blank">A Tutorial on Bayesian Optimization</a></li> <li><a href="https://arxiv.org/abs/2204.02937" rel="external nofollow noopener" target="_blank">Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations</a></li> <li><a href="https://arxiv.org/abs/2210.17323" rel="external nofollow noopener" target="_blank">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a></li> <li><a href="https://arxiv.org/abs/2406.01506" rel="external nofollow noopener" target="_blank">The Geometry of Categorical and Hierarchical Concepts in Large Language Models</a></li> <li><a href="https://arxiv.org/abs/2402.09142" rel="external nofollow noopener" target="_blank">When Representations Align: Universality in Representation Learning Dynamics</a></li> <li><a href="https://arxiv.org/abs/2406.20094" rel="external nofollow noopener" target="_blank">Scaling Synthetic Data Creation with 1,000,000,000 Personas</a></li> <li><a href="https://arxiv.org/abs/2406.10529" rel="external nofollow noopener" target="_blank">A Theory of Interpretable Approximations</a></li> <li><a href="https://arxiv.org/abs/2406.12168" rel="external nofollow noopener" target="_blank">BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment</a></li> <li><a href="https://arxiv.org/abs/1707.02038" rel="external nofollow noopener" target="_blank">A Tutorial on Thompson Sampling</a></li> <li><a href="https://arxiv.org/abs/2402.03175" rel="external nofollow noopener" target="_blank">Beyond the Black Box: A Statistical Model for LLM Reasoning and Inference</a></li> <li><a href="https://arxiv.org/abs/2406.11741v1" rel="external nofollow noopener" target="_blank">Transcendence: Generative Models Can Outperform The Experts That Train Them</a></li> <li><a href="https://arxiv.org/abs/2406.08929" rel="external nofollow noopener" target="_blank">Step-by-Step Diffusion: An Elementary Tutorial</a></li> <li><a href="https://arxiv.org/abs/2310.06816" rel="external nofollow noopener" target="_blank">Text Embeddings Reveal (Almost) As Much As Text</a></li> <li><a href="https://arxiv.org/abs/1612.08083" rel="external nofollow noopener" target="_blank">Language Modeling with Gated Convolutional Networks</a></li> <li><a href="https://arxiv.org/abs/2312.08550" rel="external nofollow noopener" target="_blank">Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks</a></li> <li><a href="https://arxiv.org/abs/2405.05417" rel="external nofollow noopener" target="_blank">Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models</a></li> <li><a href="https://arxiv.org/abs/2405.01536" rel="external nofollow noopener" target="_blank">Customizing Text-to-Image Models with a Single Image Pair</a></li> <li><a href="https://arxiv.org/abs/2405.00675" rel="external nofollow noopener" target="_blank">Self-Play Preference Optimization for Language Model Alignment</a></li> <li><a href="https://arxiv.org/abs/2404.12272" rel="external nofollow noopener" target="_blank">Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences</a></li> <li><a href="https://arxiv.org/abs/2404.18444" rel="external nofollow noopener" target="_blank">U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models</a></li> <li><a href="https://arxiv.org/abs/2006.05205" rel="external nofollow noopener" target="_blank">On the Bottleneck of Graph Neural Networks and its Practical Implications</a></li> <li><a href="https://arxiv.org/abs/1911.12543" rel="external nofollow noopener" target="_blank">How Can We Know What Language Models Know?</a></li> <li><a href="https://arxiv.org/abs/1712.05877" rel="external nofollow noopener" target="_blank">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a></li> <li><a href="https://arxiv.org/abs/2207.08815" rel="external nofollow noopener" target="_blank">Why do tree-based models still outperform deep learning on tabular data?</a></li> <li><a href="https://arxiv.org/abs/1407.4443" rel="external nofollow noopener" target="_blank">On the Complexity of Best Arm Identification in Multi-Armed Bandit Models</a></li> <li><a href="https://arxiv.org/abs/1211.2476" rel="external nofollow noopener" target="_blank">Random Utility Theory for Social Choice</a></li> <li><a href="https://arxiv.org/abs/2209.06015" rel="external nofollow noopener" target="_blank">Black-box Dataset Ownership Verification via Backdoor Watermarking</a></li> <li><a href="https://arxiv.org/abs/1711.05101" rel="external nofollow noopener" target="_blank">Decoupled Weight Decay Regularization</a></li> <li><a href="https://arxiv.org/abs/2207.14255" rel="external nofollow noopener" target="_blank">Efficient Training of Language Models to Fill in the Middle</a></li> <li><a href="https://arxiv.org/abs/2404.12358" rel="external nofollow noopener" target="_blank">From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function</a></li> <li><a href="https://arxiv.org/abs/2210.11610" rel="external nofollow noopener" target="_blank">Large Language Models Can Self-Improve</a></li> <li><a href="https://arxiv.org/abs/2402.09668" rel="external nofollow noopener" target="_blank">How to Train Data-Efficient LLMs</a></li> <li><a href="https://arxiv.org/abs/2202.00828" rel="external nofollow noopener" target="_blank">Co-training Improves Prompt-based Learning for Large Language Models</a></li> <li><a href="https://arxiv.org/abs/2306.04751" rel="external nofollow noopener" target="_blank">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</a></li> <li><a href="https://arxiv.org/abs/2306.01128" rel="external nofollow noopener" target="_blank">Learning Transformer Programs</a></li> <li><a href="https://arxiv.org/abs/1804.04235" rel="external nofollow noopener" target="_blank">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</a></li> <li><a href="https://arxiv.org/abs/1808.01204" rel="external nofollow noopener" target="_blank">Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data</a></li> <li><a href="https://arxiv.org/abs/1811.03962" rel="external nofollow noopener" target="_blank">A Convergence Theory for Deep Learning via Over-Parameterization</a></li> <li><a href="https://arxiv.org/abs/1906.05392" rel="external nofollow noopener" target="_blank">Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian</a></li> <li><a href="https://arxiv.org/abs/2012.13255" rel="external nofollow noopener" target="_blank">Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</a></li> <li><a href="https://arxiv.org/abs/2203.14465" rel="external nofollow noopener" target="_blank">STaR: Bootstrapping Reasoning With Reasoning</a></li> <li><a href="https://arxiv.org/abs/2403.09636" rel="external nofollow noopener" target="_blank">Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</a></li> <li><a href="https://arxiv.org/abs/2309.17002" rel="external nofollow noopener" target="_blank">Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks</a></li> <li><a href="https://arxiv.org/abs/1906.08039" rel="external nofollow noopener" target="_blank">The Barron Space and the Flow-induced Function Spaces for Neural Network Models</a></li> <li><a href="https://arxiv.org/abs/2312.00234" rel="external nofollow noopener" target="_blank">Deep Equilibrium Based Neural Operators for Steady-State PDEs</a></li> <li><a href="https://arxiv.org/abs/2103.02138" rel="external nofollow noopener" target="_blank">Parametric Complexity Bounds for Approximating PDEs with Neural Networks</a></li> <li><a href="https://arxiv.org/abs/2402.18668" rel="external nofollow noopener" target="_blank">Simple linear attention language models balance the recall-throughput tradeoff</a></li> <li><a href="https://arxiv.org/abs/2402.10200" rel="external nofollow noopener" target="_blank">Chain-of-Thought Reasoning Without Prompting</a></li> <li><a href="https://arxiv.org/abs/2403.09636" rel="external nofollow noopener" target="_blank">Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</a></li> <li><a href="https://arxiv.org/abs/2403.13187" rel="external nofollow noopener" target="_blank">Evolutionary Optimization of Model Merging Recipes</a></li> <li><a href="https://arxiv.org/abs/2312.08365" rel="external nofollow noopener" target="_blank">An Invitation to Deep Reinforcement Learning</a></li> <li><a href="https://arxiv.org/abs/2310.00873" rel="external nofollow noopener" target="_blank">Deep Neural Networks Tend To Extrapolate Predictably</a></li> <li><a href="https://arxiv.org/abs/2403.03206" rel="external nofollow noopener" target="_blank">Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</a></li> <li><a href="https://arxiv.org/abs/2403.05440" rel="external nofollow noopener" target="_blank">Is Cosine-Similarity of Embeddings Really About Similarity?</a></li> <li><a href="https://arxiv.org/abs/1612.08083" rel="external nofollow noopener" target="_blank">Language Modeling with Gated Convolutional Networks</a></li> <li><a href="https://arxiv.org/abs/2002.05202v1" rel="external nofollow noopener" target="_blank">GLU Variants Improve Transformer</a></li> <li><a href="https://arxiv.org/abs/1911.01547" rel="external nofollow noopener" target="_blank">On the Measure of Intelligence</a></li> <li><a href="https://arxiv.org/abs/2402.16842" rel="external nofollow noopener" target="_blank">Asymmetry in Low-Rank Adapters of Foundation Models</a></li> <li><a href="https://arxiv.org/abs/2212.09748" rel="external nofollow noopener" target="_blank">Scalable Diffusion Models with Transformers</a></li> <li><a href="https://arxiv.org/abs/2401.12178" rel="external nofollow noopener" target="_blank">In-Context Learning for Extreme Multi-Label Classification</a></li> <li><a href="https://arxiv.org/abs/2205.13147" rel="external nofollow noopener" target="_blank">Matryoshka Representation Learning</a></li> <li><a href="https://arxiv.org/abs/2311.04163" rel="external nofollow noopener" target="_blank">Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization</a></li> <li><a href="https://arxiv.org/abs/2312.14977" rel="external nofollow noopener" target="_blank">Diffusion Models for Generative Artificial Intelligence: An Introduction for Applied Mathematicians</a></li> <li><a href="https://arxiv.org/abs/2209.11895" rel="external nofollow noopener" target="_blank">In-context Learning and Induction Heads</a></li> <li><a href="https://arxiv.org/abs/1802.08246" rel="external nofollow noopener" target="_blank">Characterizing Implicit Bias in Terms of Optimization Geometry</a></li> <li><a href="https://arxiv.org/abs/2210.11416" rel="external nofollow noopener" target="_blank">Scaling Instruction-Finetuned Language Models</a></li> <li><a href="https://arxiv.org/abs/2309.05463" rel="external nofollow noopener" target="_blank">Textbooks Are All You Need II: phi-1.5 technical report</a></li> <li><a href="https://arxiv.org/abs/2302.03169" rel="external nofollow noopener" target="_blank">Data Selection for Language Models via Importance Resampling</a></li> <li><a href="https://arxiv.org/abs/2305.18290" rel="external nofollow noopener" target="_blank">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></li> <li><a href="https://arxiv.org/abs/2007.00072" rel="external nofollow noopener" target="_blank">Data Movement Is All You Need: A Case Study on Optimizing Transformers</a></li> <li><a href="https://arxiv.org/abs/2312.01429" rel="external nofollow noopener" target="_blank">Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars</a></li> <li><a href="https://arxiv.org/abs/2308.04623" rel="external nofollow noopener" target="_blank">Accelerating LLM Inference with Staged Speculative Decoding</a></li> <li><a href="https://arxiv.org/abs/2302.01318" rel="external nofollow noopener" target="_blank">Accelerating Large Language Model Decoding with Speculative Sampling</a></li> <li><a href="https://arxiv.org/abs/2211.17192" rel="external nofollow noopener" target="_blank">Fast Inference from Transformers via Speculative Decoding</a></li> <li><a href="https://arxiv.org/abs/2208.07339" rel="external nofollow noopener" target="_blank">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a></li> <li><a href="https://arxiv.org/abs/2111.06377" rel="external nofollow noopener" target="_blank">Masked Autoencoders Are Scalable Vision Learners</a></li> <li><a href="https://arxiv.org/abs/2309.01826" rel="external nofollow noopener" target="_blank">One Wide Feedforward is All You Need</a></li> <li><a href="https://arxiv.org/abs/1804.06561" rel="external nofollow noopener" target="_blank">A Mean Field View of the Landscape of Two-Layers Neural Networks</a></li> <li><a href="https://arxiv.org/abs/2010.01412" rel="external nofollow noopener" target="_blank">Sharpness-Aware Minimization for Efficiently Improving Generalization</a></li> <li><a href="https://arxiv.org/abs/2103.00065" rel="external nofollow noopener" target="_blank">Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability</a></li> <li><a href="https://arxiv.org/abs/2003.02218" rel="external nofollow noopener" target="_blank">The large learning rate phase of deep learning: the catapult mechanism</a></li> <li><a href="https://arxiv.org/abs/2106.06530" rel="external nofollow noopener" target="_blank">Label Noise SGD Provably Prefers Flat Global Minimizers</a></li> <li><a href="https://arxiv.org/abs/1906.05890" rel="external nofollow noopener" target="_blank">Gradient Descent Maximizes the Margin of Homogeneous Neural Networks</a></li> <li><a href="https://arxiv.org/abs/1912.02292" rel="external nofollow noopener" target="_blank">Deep Double Descent: Where Bigger Models and More Data Hurt</a></li> <li><a href="https://arxiv.org/abs/1908.05355" rel="external nofollow noopener" target="_blank">The generalization error of random features regression: Precise asymptotics and double descent curve</a></li> <li><a href="https://arxiv.org/abs/1706.08947" rel="external nofollow noopener" target="_blank">Exploring Generalization in Deep Learning</a></li> <li><a href="https://arxiv.org/abs/1611.03530" rel="external nofollow noopener" target="_blank">Understanding deep learning requires rethinking generalization</a></li> <li><a href="https://arxiv.org/abs/1904.11955" rel="external nofollow noopener" target="_blank">On Exact Computation with an Infinitely Wide Neural Net</a></li> <li><a href="https://arxiv.org/abs/1806.07572" rel="external nofollow noopener" target="_blank">Neural Tangent Kernel: Convergence and Generalization in Neural Networks</a></li> <li><a href="https://arxiv.org/abs/1811.04918" rel="external nofollow noopener" target="_blank">Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers</a></li> <li><a href="https://arxiv.org/abs/1803.03635" rel="external nofollow noopener" target="_blank">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a></li> <li><a href="https://arxiv.org/abs/2002.05709" rel="external nofollow noopener" target="_blank">A Simple Framework for Contrastive Learning of Visual Representations</a></li> <li><a href="https://arxiv.org/abs/1912.11370" rel="external nofollow noopener" target="_blank">Big Transfer (BiT): General Visual Representation Learning</a></li> <li><a href="https://arxiv.org/abs/1906.08988" rel="external nofollow noopener" target="_blank">A Fourier Perspective on Model Robustness in Computer Vision</a></li> <li><a href="https://arxiv.org/abs/1902.02918" rel="external nofollow noopener" target="_blank">Certified Adversarial Robustness via Randomized Smoothing</a></li> <li><a href="https://arxiv.org/abs/2010.11929" rel="external nofollow noopener" target="_blank">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></li> <li><a href="https://arxiv.org/abs/2105.07581" rel="external nofollow noopener" target="_blank">Vision Transformers are Robust Learners</a></li> <li><a href="https://arxiv.org/abs/2103.15670" rel="external nofollow noopener" target="_blank">On the Adversarial Robustness of Vision Transformers</a></li> <li><a href="https://arxiv.org/abs/2012.07805" rel="external nofollow noopener" target="_blank">Extracting Training Data from Large Language Models</a></li> <li><a href="https://arxiv.org/abs/2004.11362" rel="external nofollow noopener" target="_blank">Supervised Contrastive Learning</a></li> <li><a href="https://arxiv.org/abs/2111.00396v3" rel="external nofollow noopener" target="_blank">Efficiently Modeling Long Sequences with Structured State Spaces</a></li> </ul> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'fanpu/fanpu.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Fan Pu Zeng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: January 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?5fdb75708acb79bd79b0cacb0ed0237d"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-S3VHEYH05S"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-S3VHEYH05S');
  </script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>